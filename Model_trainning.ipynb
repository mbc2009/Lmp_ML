{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Lmp_ML/blob/main/Model_trainning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Enviornment Initialization"
      ],
      "metadata": {
        "id": "5LSSxtHN4uqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "\n",
        "# remove unnecessary\n",
        "rm -rf *\n",
        "\n",
        "# update pip\n",
        "python -m pip install --upgrade pip\n",
        "\n",
        "# install package\n",
        "pip install opencv-python pillow\n",
        "pip install segmentation_models_pytorch\n",
        "pip install -q kaggle\n",
        "pip install dropbox\n",
        "pip install scikit-image\n",
        "pip install pandas openpyxl"
      ],
      "metadata": {
        "id": "VkjBZjG7RWAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic import\n",
        "import  os, sys, time, math, random, math, psutil, h5py, re, pickle\n",
        "from    datetime                import datetime\n",
        "from    concurrent.futures      import ThreadPoolExecutor\n",
        "from    typing                  import  List, Tuple\n",
        "from    dropbox                 import  Dropbox\n",
        "from    tqdm                    import  tqdm\n",
        "from    mpl_toolkits.mplot3d    import  Axes3D\n",
        "from    matplotlib              import  pyplot      as plt\n",
        "import  numpy                                       as np\n",
        "import  pandas                                      as pd\n",
        "import  zipfile\n",
        "import  warnings\n",
        "import  shutil\n",
        "import  joblib\n",
        "\n",
        "from    sklearn.preprocessing   import MinMaxScaler\n",
        "from    sklearn.metrics         import r2_score\n",
        "from    skimage                 import  io\n",
        "from    scipy                   import  interpolate\n",
        "from    scipy.interpolate       import  RegularGridInterpolator\n",
        "from    scipy.ndimage           import  generic_filter, rotate\n",
        "from    PIL                     import  Image\n",
        "\n",
        "import  torch\n",
        "from    torch                   import  nn\n",
        "from    torch.nn                import  functional  as F\n",
        "import  torch.optim                                 as optim\n",
        "import  torchvision.transforms.functional           as TF\n",
        "from    torch.utils.data        import  Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "from    torchvision             import  transforms, models\n",
        "from    torchvision.transforms  import  *\n",
        "from    sklearn.model_selection import KFold\n",
        "import  kagglehub"
      ],
      "metadata": {
        "id": "ERcbfqGSXFT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check hardware\n",
        "print(f\"CPU core #:\\t{os.cpu_count()}\")\n",
        "print(f\"CPU threads #:\\t{psutil.cpu_count(logical=True)}\")\n",
        "print(f\"Total memory:\\t\\t{psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"available GPU #:\\t{gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i+1}:\\t\\t{gpu_name}\")\n",
        "else:\n",
        "    print(\"No available GPU\")"
      ],
      "metadata": {
        "id": "AnJ7zHLJxbro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configuration"
      ],
      "metadata": {
        "id": "GBCVteWiX60Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Confuration():\n",
        "  # define paths to directory\n",
        "  home_dir                = os.path.expanduser(\"~\")\n",
        "  working_dir             = os.path.join(os.getcwd(),'Lmp_ML')\n",
        "  DataBase_dir            = os.path.join(working_dir,'DataBase')\n",
        "  DB_version              = 5\n",
        "  DB_3D_Grids_density     = 128\n",
        "  DB_3D_Grids_path        = os.path.join(DataBase_dir, f'{DB_version}', f'3D_Grids_{DB_3D_Grids_density}.h5') # TODO: choose database version, here ver=4\n",
        "  DB_Excel_path           = os.path.join(working_dir,  f'LmpGP.xlsx')                                         # TODO: choose database version, here ver=4\n",
        "\n",
        "  # dataset\n",
        "  DB_items                = ['temp\\n(k)',\n",
        "                             #'len\\n(A)',\n",
        "                             'density\\n(g/cm^3)',\n",
        "                             'pore_radius\\n(A)',\n",
        "                             'porosity\\n(unitless)',\n",
        "                             #'bond_density\\n(unitless)',\n",
        "                             'specific_surface_area\\n(m^2/g)',\n",
        "                             #'tortuosity\\n(unitless)'\n",
        "                             'thermal_conductivity\\n(W/(mÂ·K))'\n",
        "                             ]\n",
        "\n",
        "  # debug\n",
        "  dev_mode                = False\n",
        "\n",
        "conf = Confuration()"
      ],
      "metadata": {
        "id": "FW2lJM8WXWUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import code from git hub\n",
        "!git clone https://github.com/mbc2009/Lmp_ML"
      ],
      "metadata": {
        "id": "nla41gbQQuJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make directory\n",
        "os.makedirs(conf.DataBase_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "pjO4Z6FqrZgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download\n",
        "original_path = kagglehub.dataset_download(\"mbc2009/heat-and-mass-transfer\",force_download=True)\n",
        "shutil.move(original_path, conf.DataBase_dir)"
      ],
      "metadata": {
        "id": "z_1Gn-B1mOa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Functions and Classes"
      ],
      "metadata": {
        "id": "cJevHupf53MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_hdf5_content(file_path:str,PrintStrcut=False)->int:\n",
        "  '''\n",
        "  check the content (name, quantity) of hdf5 file\n",
        "  input:\n",
        "    file_path: the path of hdf5 file\n",
        "    PrintStrcut: print the structure of hdf5 file\n",
        "  return:\n",
        "    the name and quantity of variables in hdf5 file\n",
        "  '''\n",
        "  # åˆå§‹åŒ–æ•°æ®é›†è®¡æ•°å™¨\n",
        "  dataset_count = 0\n",
        "\n",
        "  # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°ç”¨äºéå† HDF5 æ–‡ä»¶å†…éƒ¨\n",
        "  def count_datasets(name, obj):\n",
        "        nonlocal dataset_count\n",
        "        if isinstance(obj, h5py.Dataset):  # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®é›†\n",
        "            dataset_count += 1\n",
        "        elif isinstance(obj, h5py.Group):  # åˆ¤æ–­æ˜¯å¦ä¸ºç»„\n",
        "            pass  # å¦‚æœæ˜¯ç»„ï¼Œä¸è®¡æ•°\n",
        "\n",
        "  with h5py.File(file_path, \"r\") as h5f:\n",
        "\n",
        "        # éå†æ–‡ä»¶å†…å®¹ä»¥è®¡æ•°\n",
        "        h5f.visititems(count_datasets)\n",
        "\n",
        "        # æ‰“å°æ‰€æœ‰å†…å˜é‡åå­—\n",
        "        if PrintStrcut:\n",
        "          print(f\"æ–‡ä»¶ç»“æ„:\")\n",
        "          h5f.visit(print)\n",
        "\n",
        "  print(f'æ–‡ä»¶æ•°: {dataset_count}')\n",
        "\n",
        "  return dataset_count # æ–‡ä»¶æ•°"
      ],
      "metadata": {
        "id": "YcLDL7kX52K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Prepare Data Base"
      ],
      "metadata": {
        "id": "XNATVGXi3qJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Prepare Excel Data Base"
      ],
      "metadata": {
        "id": "xdsVuq64sDWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Excel data into pandas data frame\n",
        "df = pd.read_excel(conf.DB_Excel_path, engine=\"openpyxl\")"
      ],
      "metadata": {
        "id": "8G5we5wnsDCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # æ‰“å°å½¢çŠ¶\n",
        "  print(df.shape)\n",
        "\n",
        "  # è¯»å–åˆ—å\n",
        "  print(df.columns)"
      ],
      "metadata": {
        "id": "mjSiqXTlhZYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # ç­›é€‰åˆ—åå¯¹åº”åˆ—\n",
        "  df = df.loc[:, ['len\\n(A)', 'sigma\\n(A)', 'temp\\n(k)', 'flux\\n(L/m^2/h)', 'density\\n(g/cm^3)']]"
      ],
      "metadata": {
        "id": "lAes1TjFg59q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_from_pandas(len_i: int, sigma_i: int, temp_i: int, items: List[str], df: pd.DataFrame) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    æ ¹æ® len_i, sigma_i, temp_i åœ¨ pandas DataFrame ä¸­æŸ¥æ‰¾æŒ‡å®š itemsï¼Œå¹¶è¿”å›ä¸€ä¸ª PyTorch Tensorã€‚\n",
        "\n",
        "    Args:\n",
        "        len_i (int):        ç›®æ ‡ len å€¼\n",
        "        sigma_i (int):      ç›®æ ‡ sigma å€¼\n",
        "        temp_i (int):       ç›®æ ‡ temp å€¼\n",
        "        items (List[str]):  éœ€è¦æå–çš„åˆ—ååˆ—è¡¨\n",
        "        df (pd.DataFrame):  æ•°æ®æº Pandas DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Optional[torch.Tensor]: è‹¥æ‰¾åˆ°æ•°æ®ï¼Œåˆ™è¿”å›ä¸€ä¸ª Float32 ç±»å‹çš„ PyTorch Tensorï¼Œå¦åˆ™è¿”å› Noneã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„è¡Œï¼Œå¹¶æå–å¤šä¸ªåˆ—\n",
        "        item_values = df.loc[\n",
        "            (df[\"len\\n(A)\"] == len_i) &\n",
        "            (df[\"sigma\\n(A)\"] == sigma_i) &\n",
        "            (df[\"temp\\n(k)\"] == temp_i),\n",
        "            items\n",
        "        ]\n",
        "\n",
        "        # ç¡®ä¿åªæœ‰ä¸€è¡Œæ•°æ®ï¼Œè½¬æ¢ä¸º Tensor\n",
        "        if not item_values.empty:\n",
        "            tensor_values = torch.tensor(item_values.values.flatten(), dtype=torch.float32)\n",
        "            return tensor_values\n",
        "        else:\n",
        "            return None  # æ²¡æœ‰åŒ¹é…æ•°æ®æ—¶è¿”å› None\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"åˆ—åé”™è¯¯: {e}\")\n",
        "        return None\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"æ•°æ®è½¬æ¢é”™è¯¯: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ZXqnsln3g0nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # ä½¿ç”¨ç¤ºä¾‹\n",
        "  items = fetch_from_pandas(len_i= 2, sigma_i=8, temp_i=373, items=['density\\n(g/cm^3)'], df=df)\n",
        "  print(items[0].item())"
      ],
      "metadata": {
        "id": "3GTSxD6GlDtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.Check grid data base"
      ],
      "metadata": {
        "id": "DJ1Jn8Oo4qm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # æŸ¥çœ‹hdf5æ–‡ä»¶å†…å®¹\n",
        "  num_grids = check_hdf5_content(conf.DB_3D_Grids_path,PrintStrcut=False)"
      ],
      "metadata": {
        "id": "OinYQCkb46xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # æŸ¥çœ‹å•ä¸ªæ•°æ®ç‚¹\n",
        "  DB_3D_Grids = h5py.File(conf.DB_3D_Grids_path, \"r\")\n",
        "  GA          = DB_3D_Grids[f\"len_{2}_sigma_{18}_{343}\"][:]\n",
        "  print(f'çŸ©é˜µå½¢çŠ¶:   {GA.shape}')\n",
        "  print(f'çŸ©é˜µæœ€å¤§å€¼: {np.max(GA)}')\n",
        "  print(f'çŸ©é˜µæœ€å°å€¼: {np.min(GA)}')\n",
        "  print(f\"çŸ©é˜µ GA ä¸­ {np.max(GA)} çš„æ•°é‡ï¼š{np.count_nonzero(GA == np.max(GA)) }\")"
      ],
      "metadata": {
        "id": "VD-kiwaI6UGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å¯è§†åŒ–å‡½æ•°\n",
        "def plot_3D_Grid(matrix_3d):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶ä¸‰ç»´ç‚¹é˜µçš„ä¸‰è§†å›¾ï¼ˆæ­£è§†å›¾ã€ä¾§è§†å›¾ã€ä¿¯è§†å›¾ï¼‰ã€‚\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): ä¸‰ç»´ç‚¹é˜µæ•°æ®ã€‚\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # åˆ›å»ºå›¾å½¢å’Œåæ ‡è½´\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # ä¿¯è§†å›¾ (X-Y å¹³é¢)\n",
        "    axes[0].imshow(np.sum(matrix_3d, axis=0), cmap='viridis')  # æ²¿ Z è½´æ±‚å’Œ\n",
        "    axes[0].set_title('Top View (X-Y)')\n",
        "    axes[0].set_xlabel('X')\n",
        "    axes[0].set_ylabel('Y')\n",
        "\n",
        "    # æ­£è§†å›¾ (Z-X å¹³é¢)\n",
        "    axes[1].imshow(np.sum(matrix_3d, axis=1), cmap='viridis')  # æ²¿ Y è½´æ±‚å’Œ\n",
        "    axes[1].set_title('Front View (Z-X)')\n",
        "    axes[1].set_xlabel('Z')\n",
        "    axes[1].set_ylabel('X')\n",
        "\n",
        "    # ä¾§è§†å›¾ (Z-Y å¹³é¢)\n",
        "    axes[2].imshow(np.sum(matrix_3d, axis=2), cmap='viridis')  # æ²¿ X è½´æ±‚å’Œ\n",
        "    axes[2].set_title('Side View (Z-Y)')\n",
        "    axes[2].set_xlabel('Z')\n",
        "    axes[2].set_ylabel('Y')\n",
        "\n",
        "    # æ˜¾ç¤ºå›¾å½¢\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X0k9E-fD9Rv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  plot_3D_Grid(GA)  # å°† GA æ›¿æ¢ä¸ºä½ çš„ä¸‰ç»´ç‚¹é˜µæ•°æ®"
      ],
      "metadata": {
        "id": "Idw6WbDn_8JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.Home-brewing dataset"
      ],
      "metadata": {
        "id": "sh83ztK7I5gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myDataBase(Dataset):\n",
        "    def __init__(self,\n",
        "                 hdf5_3D_Grids_file_path:str,\n",
        "                 excel_Performance_and_Properties_file_path:str):\n",
        "        '''\n",
        "        name:  'len_{i}_sigma_{j}_temp_{k}'\n",
        "        label: [len_i, sigma_i, temp_i], type: pytorch tensor\n",
        "        structure: 3D grid, type: pytorch tensor\n",
        "        '''\n",
        "        ## GRID\n",
        "        # 3D grids (h5py file)\n",
        "        self.grids_hdf5     = h5py.File(hdf5_3D_Grids_file_path, 'r')\n",
        "        # Sorted name list of all GA structures (h5py file)\n",
        "        self.data_NameList  = self.sort_NameList((self.grids_hdf5.keys()))\n",
        "\n",
        "        ## EXCEL\n",
        "        # Performance & Properties (Excel => PD)\n",
        "        self.PnP_pd = pd.read_excel(excel_Performance_and_Properties_file_path, engine=\"openpyxl\")\n",
        "        self.PnP_pd = self.PnP_pd\n",
        "\n",
        "        # Normalize PD file\n",
        "        self.scaler           = MinMaxScaler()\n",
        "        self.PnP_pd_selected  = self.PnP_pd[conf.DB_items].copy()                # Select desired columns from the original DataFrame\n",
        "        self.PnP_pd_scaled    = self.scaler.fit_transform(self.PnP_pd_selected)  # Scale the selected data\n",
        "        self.PnP_pd_scaled    = pd.DataFrame(self.PnP_pd_scaled,                 # Convert to DataFrame\n",
        "                                          columns=self.PnP_pd_selected.columns)\n",
        "\n",
        "\n",
        "        joblib.dump(self.scaler, \"scaler.pkl\")  # ä¿å­˜ scalerï¼ˆç”¨äºæ¨ç†æ—¶ inverse_transform åå½’ä¸€åŒ–ï¼‰\n",
        "        print('Scaler Saved')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_NameList)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## 1. Index ##\n",
        "        # è§£æ: index -> str\n",
        "        name_i      = self.data_NameList[index]\n",
        "\n",
        "        # è½¬æ¢: str -> list [len_i, sigma_i, temp_i] -> tensor\n",
        "        label_i     = self.extract_label_from_key(name_i)\n",
        "\n",
        "        ## 2. GRIDS ##\n",
        "        # æå– (hdf5 => np.ndarray)\n",
        "        grid_i      = self.grids_hdf5[name_i][:]\n",
        "        # è½¬æ¢: Grids (np.ndarray -> PyTorch tensor)\n",
        "        grid_i      = torch.from_numpy(grid_i)\n",
        "\n",
        "        ## 3. EXCEL ##\n",
        "        # find row index\n",
        "        pd_idx   = self.seek_idx_from_pandas(len_i=label_i[0],\n",
        "                                            sigma_i=label_i[1],\n",
        "                                            temp_i=label_i[2],\n",
        "                                            items=conf.DB_items,\n",
        "                                            df=self.PnP_pd)\n",
        "        # Pandas => tensor ([item1,item2,item3...]\n",
        "        items_i  = self.PnP_pd_scaled.iloc[pd_idx]\n",
        "        items_i  = torch.tensor(items_i.values, dtype=torch.float32)\n",
        "\n",
        "        ## 4. è¿”å›\n",
        "        return torch.tensor(label_i), grid_i, items_i\n",
        "\n",
        "    def close(self):\n",
        "         # å…³é—­h5æ–‡ä»¶ï¼Œé˜²æ­¢æŸå\n",
        "        self.grids_hdf5.close()\n",
        "\n",
        "    def extract_label_from_key(self, name:str):\n",
        "        # è§£ææ•°æ®ç‚¹åç§°ä¸ºä¸‰ç»´å¼ é‡\n",
        "        len_val   = int(name.split('_')[1])\n",
        "        sigma_val = int(name.split('_')[3])\n",
        "        temp_val  = int(name.split('_')[-1])\n",
        "        label_i   = [len_val, sigma_val, temp_val]\n",
        "        return label_i\n",
        "\n",
        "    def seek_idx_from_pandas(self, len_i:int, sigma_i:int, temp_i:int, items:List[str], df: pd.DataFrame) -> torch.Tensor:\n",
        "        # ... (other parts of the function remain the same) ...\n",
        "\n",
        "        try:\n",
        "            item_idx = df.loc[\n",
        "                (df[\"len\\n(A)\"]   == len_i) &\n",
        "                (df[\"sigma\\n(A)\"] == sigma_i) &\n",
        "                (df[\"temp\\n(k)\"]  == temp_i),\n",
        "                items\n",
        "            ].index\n",
        "\n",
        "            # Check if item_idx is empty before accessing element 0\n",
        "            if len(item_idx) > 0:\n",
        "                return item_idx[0]\n",
        "            else:\n",
        "                # Handle the case where no matching rows are found\n",
        "                print(f\"Warning: No matching rows found for len={len_i}, sigma={sigma_i}, temp={temp_i}: {item_idx}\")\n",
        "                return -1 # or raise ValueError(\"No matching rows found\")\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"åˆ—åé”™è¯¯: {e}\")\n",
        "            return None\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"æ•°æ®è½¬æ¢é”™è¯¯: {e}\")\n",
        "            return None\n",
        "\n",
        "    def sort_NameList(self, strings:str):\n",
        "        # æ•°æ®ç‚¹åˆ—è¡¨åç§°æ’åº\n",
        "        def key_func(s):\n",
        "            match = re.match(r\"len_(\\d+)_sigma_(\\d+)_(\\d+)\", s)  # æå–æ•°å­—\n",
        "            if match:\n",
        "                len_val, sigma_val, temp_val = map(int, match.groups())\n",
        "                return (len_val, sigma_val, -temp_val)  # ç¬¬ä¸‰ä¸ªæ•°å­—å–åï¼Œå®ç°é™åº\n",
        "            else:\n",
        "                return (float('inf'), float('inf'), float('-inf'))  # å¤„ç†ä¸åŒ¹é…çš„æƒ…å†µ\n",
        "        return sorted(strings, key=key_func)  # æ’åº\n"
      ],
      "metadata": {
        "id": "LoTTY1WaJE6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # åˆ›å»º\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path,conf.DB_Excel_path)\n",
        "\n",
        "  # è¯»å–\n",
        "  label_i, grid_i, items_i  = dataset[0]\n",
        "  dataset.close() # å…³é—­è¯»å–\n",
        "\n",
        "  # æ‰“å°\n",
        "  print(f'{label_i}\\t\\t{type(label_i)}\\n{grid_i.shape}\\t{type(grid_i)}\\n{items_i}\\t{type(items_i)}')"
      ],
      "metadata": {
        "id": "ZjIttRqEnFhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  print(dataset.PnP_pd)#.to_string())"
      ],
      "metadata": {
        "id": "VF-TltFg9bP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  print(dataset.PnP_pd_scaled.to_string())"
      ],
      "metadata": {
        "id": "yKDUJtuZxx8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # æŸ¥çœ‹æ–‡ä»¶ååˆ—è¡¨\n",
        "  # é•¿åº¦\n",
        "  print(len(dataset.data_NameList))\n",
        "\n",
        "  # å†…å®¹\n",
        "  for i in dataset.data_NameList:\n",
        "      print(i)\n",
        "  dataset.close() # å…³é—­è¯»å–"
      ],
      "metadata": {
        "id": "LKuL5MuG7XVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Data augmentation"
      ],
      "metadata": {
        "id": "w7SXUvb6Bkb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_3D_Grid(matrix_3d: np.ndarray, angle_degrees: float):\n",
        "    \"\"\"\n",
        "    ç»•ç©¿è¿‡ x-y å¹³é¢çš„ä¸­å¿ƒç‚¹ä¸”å¹³è¡Œäº z è½´çš„è½´æ—‹è½¬ä¸‰ç»´çŸ©é˜µã€‚\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): è¦æ—‹è½¬çš„ä¸‰ç»´çŸ©é˜µã€‚\n",
        "        angle_degrees (float): æ—‹è½¬è§’åº¦ï¼ˆä»¥åº¦ä¸ºå•ä½ï¼Œé»˜è®¤é€†æ—¶é’ˆï¼‰ã€‚\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: æ—‹è½¬åçš„ä¸‰ç»´çŸ©é˜µã€‚\n",
        "    \"\"\"\n",
        "    rotated_matrix = rotate(matrix_3d,\n",
        "                            angle=-angle_degrees,  # é¡ºæ—¶é’ˆæ—‹è½¬\n",
        "                            axes=(1, 2),  # æ—‹è½¬ x-y å¹³é¢ï¼Œå³ç»• z è½´æ—‹è½¬\n",
        "                            reshape=False,\n",
        "                            order=0,\n",
        "                            mode='constant',\n",
        "                            cval=0)\n",
        "    return rotated_matrix\n",
        "\n",
        "def multi_threaded_rotation(matrix_3d: np.ndarray):\n",
        "    \"\"\"\n",
        "    å¹¶è¡Œè®¡ç®— 90Â°, 180Â°, 270Â° ä¸‰ç§æ—‹è½¬åçš„ 3D çŸ©é˜µã€‚\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): è¦æ—‹è½¬çš„ä¸‰ç»´çŸ©é˜µã€‚\n",
        "\n",
        "    Returns:\n",
        "        dict: åŒ…å« 90Â°, 180Â°, 270Â° æ—‹è½¬åçš„çŸ©é˜µã€‚\n",
        "    \"\"\"\n",
        "    angles          = [90, 180, 270]\n",
        "    rotated_results = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        results = executor.map(rotate_3D_Grid, [matrix_3d]*3, angles)\n",
        "\n",
        "    # å­˜å‚¨æ—‹è½¬åçš„çŸ©é˜µ\n",
        "    for angle, rotated_matrix in zip(angles, results):\n",
        "        rotated_results[f\"rotate_{angle}\"] = rotated_matrix\n",
        "\n",
        "    return rotated_results"
      ],
      "metadata": {
        "id": "7un57V8nBiPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # simple case\n",
        "  GA = np.asarray([\n",
        "                  [[2,3,4],  # z = 3\n",
        "                  [1,2,3],\n",
        "                  [0,1,2]],\n",
        "                  [[2,2,2],   # z = 3\n",
        "                  [2,2,2],\n",
        "                  [2,2,2]],\n",
        "                  [[1,1,1],   # z = 2\n",
        "                  [1,1,1],\n",
        "                  [1,1,1]],\n",
        "                  [[0,0,0],   # z = 1\n",
        "                  [0,0,0],\n",
        "                  [0,0,0]],\n",
        "                  [[0,0,0],   # z = 0\n",
        "                  [0,0,0],\n",
        "                  [0,0,0]]]\n",
        "                  )\n",
        "\n",
        "  # ç¤ºä¾‹æµ‹è¯•\n",
        "  rotated_matrices = multi_threaded_rotation(GA)\n",
        "\n",
        "  # è·å–æ—‹è½¬åçš„ç»“æœ\n",
        "  rotate_90  = rotated_matrices[\"rotate_90\"]\n",
        "  rotate_180 = rotated_matrices[\"rotate_180\"]\n",
        "  rotate_270 = rotated_matrices[\"rotate_270\"]\n",
        "\n",
        "  # è¾“å‡ºå½¢çŠ¶æ£€æŸ¥\n",
        "  print(rotate_90.shape, rotate_180.shape, rotate_270.shape)\n",
        "\n",
        "  # plot\n",
        "  plot_3D_Grid(GA)\n",
        "  plot_3D_Grid(rotate_90)\n",
        "  plot_3D_Grid(rotate_180)\n",
        "  plot_3D_Grid(rotate_270)"
      ],
      "metadata": {
        "id": "F60zIc1FCkJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  DB_3D_Grids = h5py.File(conf.DB_3D_Grids_path, \"r\")\n",
        "  GA          = DB_3D_Grids[f\"len_{2}_sigma_{18}_{343}\"][:]\n",
        "\n",
        "\n",
        "  # ç¤ºä¾‹æµ‹è¯•\n",
        "  rotated_matrices = multi_threaded_rotation(GA)\n",
        "\n",
        "  # è·å–æ—‹è½¬åçš„ç»“æœ\n",
        "  rotate_90  = rotated_matrices[\"rotate_90\"]\n",
        "  rotate_180 = rotated_matrices[\"rotate_180\"]\n",
        "  rotate_270 = rotated_matrices[\"rotate_270\"]\n",
        "\n",
        "  # è¾“å‡ºå½¢çŠ¶æ£€æŸ¥\n",
        "  print(rotate_90.shape, rotate_180.shape, rotate_270.shape)\n",
        "\n",
        "  # plot\n",
        "  plot_3D_Grid(GA)\n",
        "  plot_3D_Grid(rotate_90)\n",
        "  plot_3D_Grid(rotate_180)\n",
        "  plot_3D_Grid(rotate_270)"
      ],
      "metadata": {
        "id": "jAmVF9uaQqzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotatedDataBase(myDataBase):\n",
        "    \"\"\"\n",
        "    3D çŸ©é˜µæ—‹è½¬æ•°æ®é›†ï¼Œè¿”å› 90Â°, 180Â°, 270Â° æ—‹è½¬åçš„çŸ©é˜µã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ•°æ®é›†ã€‚\n",
        "\n",
        "        Args:\n",
        "            data_list (list of np.ndarray): åŸå§‹ 3D çŸ©é˜µåˆ—è¡¨ï¼Œæ¯ä¸ªçŸ©é˜µ shape=(400,400,400)ã€‚\n",
        "        \"\"\"\n",
        "        self.base_dataset     = base_dataset                # åŸå§‹æ•°æ®é›†\n",
        "        self.rotation_angles  = [0, 90, 180, 270]           # æ—‹è½¬è§’åº¦\n",
        "        self.num_rotations    = len(self.rotation_angles)   # æ—‹è½¬æ¬¡æ•°\n",
        "        #self.data_NameList    = base_dataset.data_NameList  # æ•°æ®ç‚¹åˆ—è¡¨åç§°\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        æ•°æ®é›†å¤§å°\n",
        "        \"\"\"\n",
        "        return int(len(self.base_dataset)*(self.num_rotations))\n",
        "\n",
        "    def __getitem__(self, new_idx):\n",
        "        \"\"\"\n",
        "        è·å–æ•°æ®å¹¶è¿”å›æ—‹è½¬åçš„4ä¸ªç‰ˆæœ¬\n",
        "        Returns:\n",
        "            dict: {\"original\":   åŸå§‹ 3D çŸ©é˜µ,\n",
        "                   \"rotate_90\":  é¡ºæ—¶æ—‹è½¬ 90Â°,\n",
        "                   \"rotate_180\": é¡ºæ—¶æ—‹è½¬ 180Â°,\n",
        "                   \"rotate_270\": é¡ºæ—¶æ—‹è½¬ 270Â°}\n",
        "        \"\"\"\n",
        "        # è®¡ç®—index\n",
        "        rotation_idx    = new_idx %  (self.num_rotations)  # index within the rotation group\n",
        "        original_idx    = new_idx // (self.num_rotations)  # index in the oringal base dataset\n",
        "\n",
        "        # åŸå§‹çŸ©é˜µ\n",
        "        label_i, grid_i, items_i  = self.base_dataset[original_idx]\n",
        "\n",
        "        # æ—‹è½¬\n",
        "        rotated_grid_i = rotate_3D_Grid(matrix_3d=grid_i, angle_degrees=self.rotation_angles[rotation_idx])\n",
        "\n",
        "        # ç¡®ä¿è½¬æ¢ä¸º float32 çš„ PyTorch Tensor\n",
        "        rotated_grid_i = torch.from_numpy(rotated_grid_i).to(torch.float32)\n",
        "\n",
        "        # æ·»åŠ  channel ç»´åº¦ (channels, depth, height, width) for 3D CNN\n",
        "        #rotated_grid_i = rotated_grid_i.unsqueeze(0)\n",
        "\n",
        "        # è¿”å›\n",
        "        return  label_i, rotated_grid_i, items_i"
      ],
      "metadata": {
        "id": "V9gz6eb3XudQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # åˆ›å»º\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path,conf.DB_Excel_path)\n",
        "\n",
        "  # Augmentation\n",
        "  rotated_dataset = RotatedDataBase(dataset)\n",
        "\n",
        "  for idx in [0,1,2,3]:\n",
        "    idx += (11*4)\n",
        "    print(idx)\n",
        "    label_i, grid_i, items_i = rotated_dataset[idx]\n",
        "\n",
        "    print(f'check-{dataset.data_NameList[idx//4]}')\n",
        "\n",
        "    print(label_i, grid_i.shape, items_i)\n",
        "    print(type(grid_i))"
      ],
      "metadata": {
        "id": "kdEF5NDjhXTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader speed test\n",
        "if False:\n",
        "\n",
        "  def load_data_item(dataset, index):\n",
        "      \"\"\"\n",
        "      åŠ è½½å•ä¸ªæ•°æ®é¡¹\n",
        "      \"\"\"\n",
        "      _, _, _ = dataset[index]\n",
        "\n",
        "\n",
        "  def calculate_loading_time_multithreaded(dataset, num_threads=4):\n",
        "      \"\"\"ä½¿ç”¨å¤šçº¿ç¨‹è®¡ç®—åŠ è½½æ•°æ®é›†æ‰€éœ€çš„æ—¶é—´ã€‚\"\"\"\n",
        "      start_time = time.time()\n",
        "\n",
        "      with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "          # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŠ è½½ä¸€ä¸ªæ•°æ®é¡¹\n",
        "          tasks = [executor.submit(load_data_item, dataset, i) for i in range(len(dataset))]\n",
        "\n",
        "          # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¹¶ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
        "          for _ in tqdm(tasks, total=len(tasks), desc=\"å¤šçº¿ç¨‹åŠ è½½æ•°æ®é›†...\"):\n",
        "              _.result()  # è·å–ä»»åŠ¡ç»“æœï¼Œä»¥ç¡®ä¿ä»»åŠ¡å·²å®Œæˆ\n",
        "\n",
        "      end_time = time.time()\n",
        "      total_time = end_time - start_time\n",
        "\n",
        "      return total_time\n",
        "\n",
        "\n",
        "  # åˆ›å»ºæ•°æ®é›†\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path, conf.DB_Excel_path)\n",
        "  rotated_dataset = RotatedDataBase(dataset)\n",
        "\n",
        "  # è®¡ç®—å¹¶æ‰“å°åŠ è½½æ—¶é—´\n",
        "  loading_time = calculate_loading_time_multithreaded(rotated_dataset, num_threads=int(psutil.cpu_count(logical=True)))\n",
        "  print(f\"ä½¿ç”¨å¤šçº¿ç¨‹åŠ è½½æ•´ä¸ª rotated_dataset æ‰€éœ€æ—¶é—´ï¼š{loading_time:.2f} ç§’\")"
      ],
      "metadata": {
        "id": "-SYAs_dqq1RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Model Preparation"
      ],
      "metadata": {
        "id": "G_pgJxC031wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pure3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pure3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool  = nn.MaxPool3d(2, 2)        # 2x2x2 æ± åŒ–ï¼Œå°ºå¯¸å‡åŠ\n",
        "\n",
        "        # ç”±äºå»æ‰ Transformerï¼Œfc1 ä¹‹åç›´æ¥è¿›å…¥ fc2\n",
        "        self.fc1 = nn.Linear(32 * 32 * 32 * 32, 128)  # CNN æå–çš„ç‰¹å¾\n",
        "        self.fc2 = nn.Linear(128 + 1, 64)  # é¢å¤–åŠ å…¥ temp_iï¼ˆ1 ç»´ï¼‰\n",
        "        self.output_dim = len(conf.DB_items) - 1  # è®¡ç®—è¾“å‡ºç»´åº¦\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)  # æœ€ç»ˆè¾“å‡ºå±‚\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = self.pool(F.relu(self.conv1(x.float())))  # 128Â³ -> 64Â³\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64Â³ -> 32Â³\n",
        "        x = x.view(-1, 32 * 32 * 32 * 32)  # å±•å¹³\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # fc1 out: CNN ç‰¹å¾æå–\n",
        "\n",
        "        temp_i  = tensor_i.unsqueeze(1)  # åªå– temp_iï¼Œå½¢çŠ¶å˜ä¸º (batch_size, 1)\n",
        "        x       = torch.cat((x, temp_i), dim=1)  # æ‹¼æ¥ temp_i\n",
        "\n",
        "        x = F.relu(self.fc2(x))  # fc2 out: relu\n",
        "        x = self.fc3(x)  # fc3 out: è¾“å‡ºæœ€ç»ˆç»“æœ\n",
        "        return x"
      ],
      "metadata": {
        "id": "1Kj_vy7k39wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT3D(nn.Module):\n",
        "    def __init__(self, input_dim=32 * 32 * 32, d_model=128, nhead=4, num_layers=2):\n",
        "        super(ViT3D, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc2 = nn.Linear(d_model + 1, 64)  # é¢å¤–åŠ å…¥ temp_i\n",
        "        self.output_dim = len(conf.DB_items) - 1\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = x.view(x.size(0), -1)  # å±•å¹³\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(0)  # Transformer è¾“å…¥éœ€è¦ (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # æ¢å¤ batch ç»´åº¦\n",
        "        temp_i = tensor_i.unsqueeze(1)\n",
        "        x = torch.cat((x, temp_i), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Swin3D(nn.Module):\n",
        "    def __init__(self, input_dim=32 * 32 * 32, d_model=128, nhead=4, num_layers=2):\n",
        "        super(Swin3D, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc2 = nn.Linear(d_model + 1, 64)  # é¢å¤–åŠ å…¥ temp_i\n",
        "        self.output_dim = len(conf.DB_items) - 1\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = x.view(x.size(0), -1)  # å±•å¹³\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(0)  # Transformer è¾“å…¥éœ€è¦ (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # æ¢å¤ batch ç»´åº¦\n",
        "        temp_i = tensor_i.unsqueeze(1)\n",
        "        x = torch.cat((x, temp_i), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M16-BZHJot1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Improved3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Improved3DCNN, self).__init__()\n",
        "        self.conv1  = nn.Conv3d(1,  16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2  = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool   = nn.MaxPool3d(2, 2)        # 2x2x2 æ± åŒ–ï¼Œå°ºå¯¸å‡åŠ\n",
        "\n",
        "        self.fc1          = nn.Linear(32 * 32 * 32 * 32, 128)               # CNN æå–çš„ç‰¹å¾\n",
        "        self.transformer  = TransformerEncoderLayer(d_model=128, nhead=4)   # Transformer å¤„ç†ç‰¹å¾\n",
        "        self.fc2          = nn.Linear(128 + 1, 64)          # åªåŠ å…¥ temp_iï¼ˆ1 ç»´ï¼‰\n",
        "        self.output_dim   = len(conf.DB_items) - 1          # Calculate output dimension\n",
        "        self.fc3          = nn.Linear(64, self.output_dim)  # Output dimension based on conf.DB_items\n",
        "\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = self.pool(F.relu(self.conv1(x.float())))  # 128Â³ -> 64Â³\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64Â³ -> 32Â³\n",
        "        x = x.view(-1, 32 * 32 * 32 * 32)     # å±•å¹³\n",
        "\n",
        "        x = F.relu(self.fc1(x))             # fc1 out: CNN ç‰¹å¾æå–\n",
        "        x = x.unsqueeze(0)                  # Transformer è¾“å…¥éœ€è¦ (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)             # é€šè¿‡ Transformer å¤„ç†\n",
        "        x = x.squeeze(0)                    # æ¢å¤ batch ç»´åº¦\n",
        "\n",
        "        temp_i  = tensor_i.unsqueeze(1)            # åªå– temp_iï¼Œå½¢çŠ¶å˜ä¸º (batch_size, 1)\n",
        "        x       = torch.cat((x, temp_i), dim=1)    # æ‹¼æ¥ temp_i\n",
        "\n",
        "        x = F.relu(self.fc2(x))             # fc2 out: relu\n",
        "        x = self.fc3(x)                     # fc3 out: è¾“å‡ºæœ€ç»ˆç»“æœ\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, dim_feedforward=256, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer   = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)"
      ],
      "metadata": {
        "id": "oBaiZYQWg-vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Load Data"
      ],
      "metadata": {
        "id": "AtNcOFfAj9sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è®¾å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¯”ä¾‹\n",
        "train_ratio = 0.8  # 80% è®­ç»ƒé›†, 20% æµ‹è¯•é›†\n",
        "\n",
        "# è¯»å–åŸå§‹æ•°æ®\n",
        "dataset = myDataBase(conf.DB_3D_Grids_path, conf.DB_Excel_path)\n",
        "\n",
        "# è®¡ç®—åˆ’åˆ†æ•°é‡\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size  = len(dataset) - train_size\n",
        "\n",
        "# å…ˆåœ¨ åŸå§‹æ•°æ® ä¸Šè¿›è¡Œåˆ’åˆ†\n",
        "indices = list(range(len(dataset)))  # åŸå§‹æ•°æ®ç´¢å¼•\n",
        "train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
        "\n",
        "# åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ Subset\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "test_dataset  = Subset(dataset, test_indices)\n",
        "\n",
        "# å¢å¼ºè®­ç»ƒé›†\n",
        "train_dataset_augmented = RotatedDataBase(train_dataset)\n",
        "test_dataset_augmented  = test_dataset # testset no need to be augmented"
      ],
      "metadata": {
        "id": "ax_wFWLmkA9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ç”Ÿæˆ DataLoader\n",
        "train_loader = DataLoader(train_dataset_augmented, batch_size=32, shuffle=True, num_workers=psutil.cpu_count(logical=True))\n",
        "test_loader  = DataLoader(test_dataset_augmented,  batch_size=32, shuffle=False, num_workers=psutil.cpu_count(logical=True))\n",
        "\n",
        "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset_augmented)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "qhnW64cxnpSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Model Trainning"
      ],
      "metadata": {
        "id": "QMAxND_E7Rb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization\n",
        "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model           = Improved3DCNN().to(device)\n",
        "criterion       = nn.MSELoss()\n",
        "train_loss_history = []\n",
        "\n",
        "torch.cuda.empty_cache() # æ¸…ç†ç¼“å­˜\n",
        "\n",
        "def model_train(num_epochs:int, learning_rate:float):\n",
        "  model.train() # è®­ç»ƒæ¨¡å¼\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  for epoch in range(num_epochs):\n",
        "      running_loss = 0.0\n",
        "      for label, grid, items in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "          label, grid, items = label.to(device), grid.to(device), items.to(device)\n",
        "          grid  = grid.unsqueeze(1)  # æ·»åŠ  channel ç»´åº¦ (channels, depth, height, width) for 3D CNN\n",
        "          optimizer.zero_grad()      # æ¸…é›¶ æ¢¯åº¦\n",
        "          output = model(grid, items[:, 0])  # TODO: ç¡®ä¿ è¾“å…¥grid + temp_i, å‰å‘ä¼ æ’­\n",
        "          loss   = criterion(output, items[:, 1:]) # TODO: ç¡®ä¿ è¾“å…¥ temp_i ä»¥åçš„\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "      # record\n",
        "      train_loss_history.append(running_loss / len(train_loader))\n",
        "      print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.6f}\")"
      ],
      "metadata": {
        "id": "BJAUxE_xlVeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainning: stage-1\n",
        "train_start_time = time.time() # è®°å½•æ—¶é—´\n",
        "model_train(num_epochs=10,learning_rate=0.0002) # è®­ç»ƒ\n",
        "\n",
        "torch.save(model.state_dict(), \"3d_cnn_model.pth\") # save\n",
        "\n",
        "print(\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ï¼\")"
      ],
      "metadata": {
        "id": "cEgeh2w4cCEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\\\n",
        "As TC added, 4$e^{-4}$ shows gradient disapper (loss:Nan)\\\n",
        "$\\therefore$ 4$e^{-4}$ $\\rightarrow$ 2$e^{-4}$"
      ],
      "metadata": {
        "id": "feyZshXnJPb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Trainning: stage-2\n",
        "model.load_state_dict(torch.load(\"3d_cnn_model.pth\")) # load model\n",
        "model_train(num_epochs=20,learning_rate=0.0001) # train\n",
        "\n",
        "torch.save(model.state_dict(), \"3d_cnn_model_v2.pth\") # å†æ¬¡ä¿å­˜æ¨¡å‹\n",
        "train_end_time = time.time() # è®°å½•æ—¶é—´\n",
        "trainning_time = train_end_time - train_start_time # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
        "\n",
        "print(\"âœ… ç»§ç»­è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ä¸º `3d_cnn_model_v2.pth`\")\n",
        "print(f\"ğŸ•› è®­ç»ƒè€—æ—¶: {trainning_time:.3f} ç§’\")"
      ],
      "metadata": {
        "id": "2QCH02r77Q14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# record\n",
        "fig, ax1 = plt.subplots()\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(train_loss_history, color=color)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BOPjNxMFH9D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Model Testing"
      ],
      "metadata": {
        "id": "YvNzmgG3nLFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åŠ è½½æ¨¡å‹\n",
        "model = Improved3DCNN().to(device)                        # Create an instance of your model\n",
        "model.load_state_dict(torch.load('3d_cnn_model_v2.pth'))  # Load the saved state_dict\n",
        "model.eval()                                              # Set the model to evaluation mode\n",
        "\n",
        "# ç”¨äºå­˜å‚¨é¢„æµ‹å€¼å’ŒçœŸå®å€¼\n",
        "predictions_scaled   = []\n",
        "actual_values_scaled = []\n",
        "\n",
        "\n",
        "# ç”¨äºå­˜å‚¨æ€»çš„æŸå¤±\n",
        "total_loss    = 0.0\n",
        "criterion     = nn.MSELoss()\n",
        "test_loss_history    = []\n",
        "\n",
        "# æ¨ç†\n",
        "reasoning_start_time = time.time() # è®°å½•æ—¶é—´\n",
        "with torch.no_grad():\n",
        "    for label, grid, items in tqdm(test_loader, desc=f\"Evaluating...\"): # Changed description\n",
        "        label, grid, items = label.to(device), grid.to(device), items.to(device)\n",
        "        grid    = grid.unsqueeze(1)               # æ·»åŠ  channel ç»´åº¦ (channels, depth, height, width) for 3D CNN\n",
        "        output  = model(grid, items[:, 0])        # TODO: ç¡®ä¿ è¾“å…¥grid + temp_i, å‰å‘ä¼ æ’­\n",
        "        loss    = criterion(output, items[:, 1:]) # TODO: ç¡®ä¿ è¾“å…¥ temp_i ä»¥åçš„\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append predictions and actual values to the lists\n",
        "        predictions_scaled.extend(output.cpu())         # Move predictions to CPU\n",
        "        actual_values_scaled.extend(items[:, 1:].cpu()) # Move actual values to CPU\n",
        "\n",
        "        # record\n",
        "        test_loss_history.append(total_loss / len(test_loader))\n",
        "\n",
        "reasoning_end_time = time.time() # è®°å½•æ—¶é—´\n",
        "reasoning_time = reasoning_end_time - reasoning_start_time # è®¡ç®—æ¨ç†æ—¶é—´\n",
        "print(f\"ğŸ•› æ¨ç†è€—æ—¶: {reasoning_time:.3f} ç§’\")\n",
        "print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")"
      ],
      "metadata": {
        "id": "5zn0vY-6nIZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Model Evaluation"
      ],
      "metadata": {
        "id": "iBFgqcg7i5No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import scipy.stats"
      ],
      "metadata": {
        "id": "4paTRQg0Al3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.1 ç»å¯¹è¯¯å·® (MAE - Mean Absolute Error)**\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum |y_{true} - y_{pred}|\n",
        "$$\n",
        "\n",
        "- è¡¡é‡æ¨¡å‹é¢„æµ‹è¯¯å·®çš„å¹³å‡ç»å¯¹å€¼ã€‚\n",
        "- ä¸ MSE ç›¸æ¯”ï¼ŒMAE ä¸ä¼šæ”¾å¤§è¾ƒå¤§çš„è¯¯å·®ï¼Œå¯¹å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰æ›´é²æ£’ã€‚"
      ],
      "metadata": {
        "id": "CkP2Oi2b2gse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAE = mean_absolute_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"ğŸ“‰ MAE: {MAE:.4f}\")"
      ],
      "metadata": {
        "id": "hcv9NmmB2blf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.2 å‡æ–¹è¯¯å·®(MSE - Mean Squared Error)**\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum \\left(y_{true} - y_{pred}\\right)^2\n",
        "$$\n",
        "\n",
        "- è¡¡é‡æ¨¡å‹é¢„æµ‹è¯¯å·®çš„å¹³æ–¹å¹³å‡å€¼ï¼Œæ”¾å¤§è¾ƒå¤§è¯¯å·®ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿã€‚\n"
      ],
      "metadata": {
        "id": "ibbHFlLA3V4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"ğŸ“‰ MSE: {MSE:.4f}\")"
      ],
      "metadata": {
        "id": "6nF8q3oyjqGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.3. å‡æ–¹æ ¹è¯¯å·®(RMSE - Root Mean Squared Error)**\n",
        "$$\n",
        "RMSE = \\sqrt{MSE}\n",
        "$$\n",
        "\n",
        "- RMSE = MSE å¼€å¹³æ–¹ï¼Œå•ä½ä¸åŸå˜é‡ä¸€è‡´ï¼Œæ›´æ˜“è§£é‡Šã€‚"
      ],
      "metadata": {
        "id": "6WnY_at14kFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = np.sqrt(MSE)\n",
        "print(f\"ğŸ“‰ RMSE: {RMSE:.4f}\")"
      ],
      "metadata": {
        "id": "9wk9Yb6_4p1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.4 å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®(MAPE - Mean Absolute Percentage Error)**\n",
        "$$\n",
        "MAPE = \\frac{1}{n} \\sum \\left| \\frac{y_{true} - y_{pred}}{y_{true}} \\right| x 100 \\text{%}\n",
        "$$\n",
        "\n",
        "- è¡¡é‡é¢„æµ‹è¯¯å·®ç›¸å¯¹äºçœŸå®å€¼çš„ç™¾åˆ†æ¯”ï¼Œé€‚ç”¨äºä¸åŒå°ºåº¦çš„æ•°æ®ã€‚\n",
        "- âš ï¸ é€‚ç”¨äºéè´Ÿæ•°æ®ï¼Œå¦åˆ™å¯èƒ½ä¼šæŠ¥é”™ã€‚\n"
      ],
      "metadata": {
        "id": "qSsg02pc5E-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "MAPE = mean_absolute_percentage_error(np.array(actual_values_scaled), np.array(predictions_scaled))\n",
        "print(f\"ğŸ“‰ MAPE: {MAPE:.2f}%\")"
      ],
      "metadata": {
        "id": "y6-OoYho5EUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.5 è§£é‡Šæ–¹å·®å¾—åˆ†(Explained Variance Score)**\n",
        "$$\n",
        "EVS = 1 - \\frac{Var(y_{true}-y_{pred})}{Var(y_{true})}\n",
        "$$\n",
        "\n",
        "- è¡¡é‡æ¨¡å‹å¯¹æ•°æ®æ–¹å·®çš„è§£é‡Šèƒ½åŠ›ï¼Œç±»ä¼¼ RÂ²ï¼Œä½†ä¸å—æ•°æ®ç¼©æ”¾å½±å“ã€‚"
      ],
      "metadata": {
        "id": "3QvF_FdO6v1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVS = explained_variance_score(actual_values_scaled, predictions_scaled)\n",
        "print(f\"ğŸ“Š Explained Variance Score: {EVS:.4f}\")"
      ],
      "metadata": {
        "id": "nB_Dn7jM6jih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.6 å¹³å‡å¯¹æ•°è¯¯å·®(MSLE - Mean Squared Logarithmic Error)**\n",
        "$$\n",
        "\\text{MSLE} = \\frac{1}{n} \\sum \\left( \\log(1 + y_{\\text{true}}) - \\log(1 + y_{\\text{pred}}) \\right)^2\n",
        "$$\n",
        "\n",
        "- é€‚ç”¨äºé¢„æµ‹å€¼å˜åŒ–èŒƒå›´è¾ƒå¤§çš„æƒ…å†µï¼ˆå¦‚é¢„æµ‹æ”¶å…¥ã€äººå£ç­‰ï¼‰ã€‚\n",
        "- é¿å…æç«¯å€¼å½±å“ï¼Œæ›´å…³æ³¨ç›¸å¯¹è¯¯å·®ã€‚\n",
        "- âš ï¸ é€‚ç”¨äºéè´Ÿæ•°æ®ï¼Œå¦åˆ™å¯èƒ½ä¼šæŠ¥é”™ã€‚\n"
      ],
      "metadata": {
        "id": "6bY0onRP8PXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSLE = mean_squared_log_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"ğŸ“‰ MSLE: {MSLE:.4f}\")"
      ],
      "metadata": {
        "id": "Y-EDhu8O8Zn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.7 çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPearson Correlation Coefficientï¼‰**\n",
        "$$\n",
        "\\text{PCC} = \\frac{\\text{Cov}(y_{\\text{true}}, y_{\\text{pred}})}{\\sigma_{y_{\\text{true}}} \\cdot \\sigma_{y_{\\text{pred}}}}\n",
        "$$\n",
        "\n",
        "- è¡¡é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§ï¼ŒèŒƒå›´åœ¨ \\([-1,1]\\) ä¹‹é—´ï¼š\n",
        "  - **+1** è¡¨ç¤ºå®Œå…¨æ­£ç›¸å…³\n",
        "  - **0** è¡¨ç¤ºæ— ç›¸å…³æ€§\n",
        "  - **-1** è¡¨ç¤ºå®Œå…¨è´Ÿç›¸å…³\n"
      ],
      "metadata": {
        "id": "4-hhL7qJ994k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCC, _ = scipy.stats.pearsonr(np.array(actual_values_scaled).flatten(), np.array(predictions_scaled).flatten())\n",
        "print(f\"ğŸ“ˆ Pearson Correlation Coefficient: {PCC:.4f}\")"
      ],
      "metadata": {
        "id": "S81plJVH-BPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.7 æ–¹è¯¯å·®($R^{2}$ )**\n",
        "**RÂ² å…¬å¼**\n",
        "\n",
        "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "\n",
        "- $SS$ = Sum of Squares\n",
        "- $SS_{res} = \\sum (y_{\\text{true}} - y_{\\text{pred}})^2$ â€”â€” æ®‹å·®å¹³æ–¹å’Œ\n",
        "- $SS_{tot} = \\sum (y_{\\text{true}} - \\bar{y}_{\\text{true}})^2$ â€”â€” æ€»å¹³æ–¹å’Œ\n",
        "- $\\bar{y}_{\\text{true}}$ æ˜¯çœŸå®å€¼çš„å‡å€¼ã€‚\n",
        "\n",
        "**RÂ² çš„è§£é‡Š**\n",
        "\n",
        "- **\\( R^2 = 1.0 \\)**ï¼šæ¨¡å‹å®Œç¾æ‹Ÿåˆæ•°æ®ã€‚\n",
        "- **\\( 0 < R^2 < 1 \\)**ï¼šæ¨¡å‹æœ‰ä¸€å®šçš„è§£é‡Šèƒ½åŠ›ï¼Œä½†ä»æœ‰è¯¯å·®ã€‚\n",
        "- **\\( R^2 $\\leq$ 0 \\)**ï¼šæ¨¡å‹ç”šè‡³æ¯”ç®€å•çš„å‡å€¼é¢„æµ‹è¿˜å·®ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "0WesUyXZ4P-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è®¡ç®— RÂ² Score\n",
        "R2 = r2_score(actual_values_scaled, predictions_scaled)\n",
        "print(f\"ğŸ“Š RÂ² Score: {R2:.4f}\")"
      ],
      "metadata": {
        "id": "nUt_1bVX4Mq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ“Œ 8.8 è®­ç»ƒä¸æ¨ç†è€—æ—¶ (Time consumed)**"
      ],
      "metadata": {
        "id": "6CBb7mdJ_qd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è¾“å‡ºè¯„ä¼°ç»“æœ\n",
        "print(f\"ğŸ•› è®­ç»ƒè€—æ—¶ï¼š{trainning_time:3f} ç§’\")\n",
        "print(f\"ğŸ•› æ¨ç†è€—æ—¶: {reasoning_time:.3f} ç§’\")"
      ],
      "metadata": {
        "id": "1i-7pkgonrqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Œ 8.9 å¯¹æ¯”æ€»ç»“\n",
        "\n",
        "| æŒ‡æ ‡  | ä½œç”¨ | é€‚ç”¨åœºæ™¯ |\n",
        "|------|------|--------|\n",
        "| **MSE**  | è¯¯å·®å¹³æ–¹å¹³å‡ï¼Œæ”¾å¤§å¼‚å¸¸å€¼ | å…³æ³¨å¤§è¯¯å·® |\n",
        "| **RMSE** | MSE å¼€å¹³æ–¹ï¼Œä¸çœŸå®å€¼å•ä½ä¸€è‡´ | ç›´è§‚è§£é‡Šè¯¯å·® |\n",
        "| **MAE**  | è¯¯å·®ç»å¯¹å€¼å¹³å‡ | æŠ—å¼‚å¸¸å€¼èƒ½åŠ›å¼º |\n",
        "| **MAPE** | è¯¯å·®çš„ç›¸å¯¹ç™¾åˆ†æ¯” | é€‚ç”¨äºä¸åŒå°ºåº¦æ•°æ® |\n",
        "| **RÂ²**   | è§£é‡Šæ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ | è¡¡é‡æ•´ä½“æ•ˆæœ |\n",
        "| **EVS**  | è§£é‡Šæ•°æ®æ–¹å·®çš„èƒ½åŠ› | ç±»ä¼¼ RÂ²ï¼Œä½†ä¸å—ç¼©æ”¾å½±å“ |\n",
        "| **MSLE** | è®¡ç®—å¯¹æ•°è¯¯å·® | é€‚ç”¨äºæŒ‡æ•°å¢é•¿é—®é¢˜ |\n",
        "| **PCC**  | ç›¸å…³æ€§åˆ†æ | åˆ¤æ–­é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„çº¿æ€§ç›¸å…³æ€§ |\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“Œ 8.10 ç»“è®º\n",
        "\n",
        "âœ… **å¦‚æœä½ å…³æ³¨è¯¯å·®çš„å®é™…å¤§å°**ï¼šä½¿ç”¨ **MSE / RMSE / MAE**  \n",
        "âœ… **å¦‚æœä½ éœ€è¦æ ‡å‡†åŒ–çš„æ¨¡å‹è¯„ä»·**ï¼šä½¿ç”¨ **RÂ² / EVS**  \n",
        "âœ… **å¦‚æœæ•°æ®å…·æœ‰ä¸åŒå°ºåº¦**ï¼šä½¿ç”¨ **MAPE**  \n",
        "âœ… **å¦‚æœæ•°æ®åˆ†å¸ƒæœ‰é•¿å°¾æ•ˆåº”ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰**ï¼šä½¿ç”¨ **MSLE**  \n",
        "âœ… **å¦‚æœä½ æƒ³åˆ†æé¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„ç›¸å…³æ€§**ï¼šä½¿ç”¨ **PCC**  \n"
      ],
      "metadata": {
        "id": "yvf9QztH_5BY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 9.Vissualization"
      ],
      "metadata": {
        "id": "d3Rnv159jSWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check å½’ä¸€åŒ–å‚æ•°\n",
        "def scaler_checker(idx,scaler=joblib.load(\"scaler.pkl\")):\n",
        "  print(f'item:\\t{conf.DB_items[idx]}\\n'\n",
        "      f'min:\\t{scaler.data_min_[idx]:.2f} \\t\\t# è®­ç»ƒæ•°æ®çš„æœ€å°å€¼ (æ¯åˆ—) \\n'\n",
        "      f'max:\\t{scaler.data_max_[idx]:.2f} \\t\\t# è®­ç»ƒæ•°æ®çš„æœ€å¤§å€¼ (æ¯åˆ—) \\n'\n",
        "      f'range:\\t{scaler.data_range_[idx]:.2f} \\t\\t# max - min (æ¯åˆ—) \\n'\n",
        "      f'offset:\\t{scaler.min_[idx]:.2f} \\t\\t# å½’ä¸€åŒ–åç§»é‡ï¼Œè®¡ç®—å…¬å¼ï¼š`-min * scaler`\\n'\n",
        "      f'scaler:\\t{scaler.scale_[idx]:.2f} \\t\\t# å½’ä¸€åŒ–æ¯”ä¾‹å› å­ï¼Œè®¡ç®—å…¬å¼ï¼š`1 / data_range_`\\n')"
      ],
      "metadata": {
        "id": "JtNyOwjYA_Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(1,len(conf.DB_items)):\n",
        "  scaler_checker(idx)"
      ],
      "metadata": {
        "id": "ODRWZcZ7eJvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å½’ä¸€åŒ–å…¬å¼ï¼š\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "åå½’ä¸€åŒ–ï¼š\n",
        "$$\n",
        "X_{\\text{original}} = X_{\\text{scaled}} \\times (X_{\\max} - X_{\\min}) + X_{\\min}\n",
        "$$\n",
        "\n",
        "Range:\n",
        "$$\n",
        "X_{\\text{range}} = X_{\\max} - X_{\\min}\n",
        "$$\n",
        "\n",
        "Offset:\n",
        "$$\n",
        "X_{\\text{offset}} = - X_{\\min} \\times X_{\\text{scaler}}\n",
        "$$\n",
        "\n",
        "Scaler:\n",
        "$$\n",
        "X_{\\text{scaler}} = \\frac{1}{X_{\\text{range}}}\n",
        "$$"
      ],
      "metadata": {
        "id": "OYP4L4qVgzur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è½¬æ¢ä¸º numpy æ•°ç»„\n",
        "predictions_scaled   = np.array(predictions_scaled).copy()\n",
        "actual_values_scaled = np.array(actual_values_scaled).copy()\n",
        "\n",
        "# åå½’ä¸€åŒ–\n",
        "reverse_nromalization = True\n",
        "if reverse_nromalization:\n",
        "  scaler          = joblib.load(\"scaler.pkl\")\n",
        "  scaler.min_     = scaler.min_[1:] # åªé€‰æ‹© scaler çš„å‰ 2 ç»´\n",
        "  scaler.scale_   = scaler.scale_[1:]\n",
        "  predictions     = scaler.inverse_transform(predictions_scaled)\n",
        "  actual_values   = scaler.inverse_transform(actual_values_scaled)\n",
        "# ä¸åšåå½’ä¸€åŒ–\n",
        "else:\n",
        "  predictions     = predictions_scaled\n",
        "  actual_values   = actual_values_scaled"
      ],
      "metadata": {
        "id": "bTXW7cwKjzGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å®šä¹‰ç»˜å›¾å‚æ•°\n",
        "def plot_prediction_results(actual_values, predictions, target_idx):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶é¢„æµ‹ç»“æœåˆ†æå›¾ï¼ˆé¢„æµ‹ vs çœŸå®å€¼ã€è¯¯å·®åˆ†å¸ƒã€è¯¯å·® vs çœŸå®å€¼ï¼‰\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "    - actual_values: çœŸå®å€¼çš„ NumPy æ•°ç»„ (N, 2)\n",
        "    - predictions: é¢„æµ‹å€¼çš„ NumPy æ•°ç»„ (N, 2)\n",
        "    - target_idx: ç›®æ ‡å˜é‡ç´¢å¼•ï¼ˆ0 æˆ– 1ï¼‰\n",
        "    - target_name: ç›®æ ‡å˜é‡çš„åç§° (str)\n",
        "    \"\"\"\n",
        "    target_name = conf.DB_items[target_idx + 1].replace('\\n', ' ')  # skip temp\n",
        "    errors = predictions[:, target_idx] - actual_values[:, target_idx]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # 1ï¸âƒ£ é¢„æµ‹å€¼ vs çœŸå®å€¼\n",
        "    axes[0].scatter(actual_values[:, target_idx], predictions[:, target_idx], c='blue', label='Predictions')\n",
        "    axes[0].plot([min(actual_values[:, target_idx]), max(actual_values[:, target_idx])],\n",
        "                 [min(actual_values[:, target_idx]), max(actual_values[:, target_idx])], 'r--', label='Ideal fit')\n",
        "    axes[0].set_xlabel(f'Actual Values ({target_name})')\n",
        "    axes[0].set_ylabel(f'Predicted Values ({target_name})')\n",
        "    axes[0].set_title(f'Predicted vs Actual Values ({target_name})')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # 2ï¸âƒ£ è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾\n",
        "    axes[1].hist(errors, bins=25, color='purple', edgecolor='black')\n",
        "    axes[1].set_xlabel(f'Prediction Error ({target_name})')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Distribution of Prediction Errors ({target_name})')\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # 3ï¸âƒ£ è¯¯å·® vs çœŸå®å€¼\n",
        "    axes[2].scatter(actual_values[:, target_idx], errors, c='green')\n",
        "    axes[2].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[2].set_xlabel(f'Actual Values ({target_name})')\n",
        "    axes[2].set_ylabel(f'Prediction Error ({target_name})')\n",
        "    axes[2].set_title(f'Prediction Error vs Actual Values ({target_name})')\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'Prediction_Results.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Tnu_agM1nu8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è°ƒç”¨å‡½æ•°ç»˜åˆ¶ç›®æ ‡å€¼ 1 (Target 1)\n",
        "for idx in range(0,predictions.shape[1]):\n",
        "  plot_prediction_results(actual_values, predictions, target_idx=idx)"
      ],
      "metadata": {
        "id": "zyXRYNe6eQCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Save"
      ],
      "metadata": {
        "id": "jyC7ZYN4F2ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save file ##\n",
        "\n",
        "# è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šYYYYMMDD_HHMMSSï¼‰\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# è·å–æ¨¡å‹ç±»å‹\n",
        "model_type = type(model).__name__  # æå–æ¨¡å‹ç±»åï¼Œå¦‚ \"Improved3DCNN\"\n",
        "\n",
        "# æ„é€ æ–‡ä»¶å\n",
        "file_name = f\"model_results_{model_type}_{timestamp}.pkl\"\n",
        "file_path = f\"/content/{file_name}\"  # Colab é»˜è®¤å­˜å‚¨è·¯å¾„\n",
        "\n",
        "# åˆ›å»ºä¿å­˜å­—å…¸\n",
        "results_dict = {\n",
        "    \"predictions\": predictions,\n",
        "    \"actual_values\": actual_values,\n",
        "    \"scaler\": scaler,\n",
        "    \"train_loss_history\": train_loss_history,\n",
        "    \"test_loss_history\": test_loss_history,\n",
        "    \"trainning_time\": trainning_time,\n",
        "    \"reasoning_time\": reasoning_time,\n",
        "    \"R2\": R2,\n",
        "    \"MSE\": MSE,\n",
        "    \"RMSE\": RMSE,\n",
        "    \"MAE\": MAE,\n",
        "    \"EVS\": EVS,\n",
        "    \"MSLE\": MSLE,\n",
        "    \"PCC\": PCC\n",
        "}\n",
        "\n",
        "# è®°å½•å¼€å§‹æ—¶é—´\n",
        "save_start_time = time.time()\n",
        "\n",
        "# ä¿å­˜åˆ°æ–‡ä»¶\n",
        "with open(file_path, \"wb\") as f:\n",
        "    pickle.dump(results_dict, f)\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# è®°å½•ç»“æŸæ—¶é—´\n",
        "save_end_time = time.time()\n",
        "save_time = save_end_time - save_start_time\n",
        "\n",
        "# è·å–æ–‡ä»¶å¤§å°\n",
        "file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "\n",
        "# æ‰“å°æ–‡ä»¶ä¿¡æ¯\n",
        "print(f\"âœ… ç»“æœå·²ä¿å­˜åˆ° {file_path}\")\n",
        "print(f\"ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size_mb:.3f} MB\")\n",
        "print(f\"â³ ä¿å­˜è€—æ—¶: {save_time:.4f} ç§’\")\n",
        "print(f\"ğŸ“‚ ä¸‹è½½æ–‡ä»¶å: {file_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "30vMXExaF7E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Post Check ##\n",
        "\n",
        "# 1ï¸âƒ£ è¯»å–ä¿å­˜çš„æ–‡ä»¶\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        loaded_results = pickle.load(f)\n",
        "    print(\"âœ… æˆåŠŸåŠ è½½ model_results.pkl\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}\")\n",
        "\n",
        "# 2ï¸âƒ£ è¯»å–å˜é‡\n",
        "expected_keys = [\n",
        "    \"predictions\",\n",
        "    \"actual_values\",\n",
        "    \"scaler\",\n",
        "    \"train_loss_history\",\n",
        "    \"test_loss_history\",\n",
        "    \"trainning_time\",\n",
        "    \"reasoning_time\",\n",
        "    \"R2\",\n",
        "    \"MSE\",\n",
        "    \"RMSE\",\n",
        "    \"MAE\",\n",
        "    \"EVS\",\n",
        "    \"MSLE\",\n",
        "    \"PCC\"\n",
        "]\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å˜é‡éƒ½æ­£ç¡®å­˜å‚¨\n",
        "missing_keys = [key for key in expected_keys if key not in loaded_results]\n",
        "if missing_keys:\n",
        "    print(f\"âš ï¸ ç¼ºå°‘å˜é‡: {missing_keys}\")\n",
        "else:\n",
        "    print(\"âœ… æ‰€æœ‰å˜é‡å‡å·²æ­£ç¡®å­˜å‚¨ï¼\")\n",
        "\n",
        "# 3ï¸âƒ£ æµ‹è¯•å˜é‡çš„é•¿åº¦å’Œå†…å®¹\n",
        "print(\"\\nğŸ“Š å˜é‡æ£€æŸ¥æŠ¥å‘Š\")\n",
        "\n",
        "for key, value in loaded_results.items():\n",
        "    if isinstance(value, (list, tuple, set)):\n",
        "        print(f\"ğŸ”¹ {key}: é•¿åº¦ {len(value)}\")\n",
        "    elif isinstance(value, dict):\n",
        "        print(f\"ğŸ”¹ {key}: åŒ…å« {len(value.keys())} ä¸ªé”®\")\n",
        "    elif isinstance(value, (int, float, str)):\n",
        "        print(f\"ğŸ”¹ {key}: å€¼ {value}\")\n",
        "    else:\n",
        "        print(f\"ğŸ”¹ {key}: ç±»å‹ {type(value)}\")\n",
        "\n",
        "print(\"\\nâœ… æ ¸éªŒå®Œæˆï¼\")"
      ],
      "metadata": {
        "id": "6iQ7Jj64KOBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¸‹è½½æ–‡ä»¶ï¼ˆä»…é™ Google Colabï¼‰\n",
        "from google.colab import files  # ä»…åœ¨ Google Colab è¿è¡Œ\n",
        "files.download(file_path)\n",
        "\n",
        "# ç­‰å¾…10åˆ†é’Ÿç¡®ä¿ä¸‹è½½å®Œæˆ\n",
        "def pause_execution(minutes=10):\n",
        "    \"\"\"\n",
        "    æš‚åœä»£ç æ‰§è¡ŒæŒ‡å®šçš„åˆ†é’Ÿæ•°ã€‚\n",
        "\n",
        "    Args:\n",
        "        minutes (int): æš‚åœæ‰§è¡Œçš„åˆ†é’Ÿæ•°ï¼Œé»˜è®¤ä¸º 10ã€‚\n",
        "    \"\"\"\n",
        "    seconds = minutes * 60  # å°†åˆ†é’Ÿè½¬æ¢ä¸ºç§’\n",
        "    time.sleep(seconds)  # ä½¿ç”¨ time.sleep() å‡½æ•°æš‚åœæ‰§è¡Œ\n",
        "    print(f\"âœ… å·²æš‚åœ {minutes} åˆ†é’Ÿï¼Œç»§ç»­æ‰§è¡Œ...\")\n",
        "\n",
        "print(\"å¼€å§‹æ‰§è¡Œ...\")\n",
        "pause_execution()  # æš‚åœæ‰§è¡Œ 10 åˆ†é’Ÿ\n",
        "print(\"ç»§ç»­æ‰§è¡Œ...\")"
      ],
      "metadata": {
        "id": "H1c-M0W5LUIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "YogN458yHH2Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}