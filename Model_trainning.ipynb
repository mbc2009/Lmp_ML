{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Lmp_ML/blob/main/Model_trainning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Enviornment Initialization"
      ],
      "metadata": {
        "id": "5LSSxtHN4uqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "\n",
        "# remove unnecessary\n",
        "rm -rf *\n",
        "\n",
        "# update pip\n",
        "python -m pip install --upgrade pip\n",
        "\n",
        "# install package\n",
        "pip install opencv-python pillow\n",
        "pip install segmentation_models_pytorch\n",
        "pip install -q kaggle\n",
        "pip install dropbox\n",
        "pip install scikit-image\n",
        "pip install pandas openpyxl"
      ],
      "metadata": {
        "id": "VkjBZjG7RWAz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic import\n",
        "import  os, sys, time, math, random, math, psutil, h5py, re, pickle\n",
        "from    datetime                import datetime\n",
        "from    concurrent.futures      import ThreadPoolExecutor\n",
        "from    typing                  import  List, Tuple\n",
        "from    dropbox                 import  Dropbox\n",
        "from    tqdm                    import  tqdm\n",
        "from    mpl_toolkits.mplot3d    import  Axes3D\n",
        "from    matplotlib              import  pyplot      as plt\n",
        "import  numpy                                       as np\n",
        "import  pandas                                      as pd\n",
        "import  zipfile\n",
        "import  warnings\n",
        "import  shutil\n",
        "import  joblib\n",
        "\n",
        "from    sklearn.preprocessing   import MinMaxScaler\n",
        "from    sklearn.metrics         import r2_score, mean_absolute_error,mean_squared_error,explained_variance_score,mean_squared_log_error\n",
        "from    skimage                 import  io\n",
        "from    sklearn.model_selection import KFold\n",
        "\n",
        "import  scipy\n",
        "from    scipy                   import  interpolate,stats\n",
        "from    scipy.interpolate       import  RegularGridInterpolator\n",
        "from    scipy.ndimage           import  generic_filter, rotate\n",
        "\n",
        "import  torch\n",
        "from    torch                   import  nn\n",
        "from    torch.nn                import  functional  as F\n",
        "import  torch.optim                                 as optim\n",
        "import  torchvision.transforms.functional           as TF\n",
        "from    torch.utils.data        import  Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "from    torchvision             import  transforms, models\n",
        "from    torchvision.transforms  import  *\n",
        "from    PIL                     import  Image\n",
        "import  kagglehub"
      ],
      "metadata": {
        "id": "ERcbfqGSXFT6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check hardware\n",
        "print(f\"CPU core #:\\t{os.cpu_count()}\")\n",
        "print(f\"CPU threads #:\\t{psutil.cpu_count(logical=True)}\")\n",
        "print(f\"Total memory:\\t\\t{psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"available GPU #:\\t{gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i+1}:\\t\\t{gpu_name}\")\n",
        "else:\n",
        "    print(\"No available GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnJ7zHLJxbro",
        "outputId": "1ec1c480-d05a-4531-818e-1d7c54e94424"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU core #:\t12\n",
            "CPU threads #:\t12\n",
            "Total memory:\t\t83.48 GB\n",
            "available GPU #:\t1\n",
            "GPU 1:\t\tNVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Model Designing"
      ],
      "metadata": {
        "id": "GBCVteWiX60Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT3D(nn.Module):\n",
        "    def __init__(self, input_dim=32 * 32 * 32, d_model=128, nhead=4, num_layers=2):\n",
        "        super(ViT3D, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc2 = nn.Linear(d_model + 1, 64)  # 额外加入 temp_i\n",
        "        self.output_dim = len(conf.DB_items) - 1\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = x.view(x.size(0), -1)  # 展平\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(0)  # Transformer 输入需要 (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # 恢复 batch 维度\n",
        "        temp_i = tensor_i.unsqueeze(1)\n",
        "        x = torch.cat((x, temp_i), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Swin3D(nn.Module):\n",
        "    def __init__(self, input_dim=32 * 32 * 32, d_model=128, nhead=4, num_layers=2):\n",
        "        super(Swin3D, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc2 = nn.Linear(d_model + 1, 64)  # 额外加入 temp_i\n",
        "        self.output_dim = len(conf.DB_items) - 1\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = x.view(x.size(0), -1)  # 展平\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(0)  # Transformer 输入需要 (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(0)  # 恢复 batch 维度\n",
        "        temp_i = tensor_i.unsqueeze(1)\n",
        "        x = torch.cat((x, temp_i), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ieA5bNNfjgJW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pure3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pure3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool  = nn.MaxPool3d(2, 2)        # 2x2x2 池化，尺寸减半\n",
        "\n",
        "        # 由于去掉 Transformer，fc1 之后直接进入 fc2\n",
        "        self.fc1 = nn.Linear(32 * 32 * 32 * 32, 128)  # CNN 提取的特征\n",
        "        self.fc2 = nn.Linear(128 + 1, 64)  # 额外加入 temp_i（1 维）\n",
        "        self.output_dim = len(conf.DB_items) - 1  # 计算输出维度\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)  # 最终输出层\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = self.pool(F.relu(self.conv1(x.float())))  # 128³ -> 64³\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64³ -> 32³\n",
        "        x = x.view(-1, 32 * 32 * 32 * 32)  # 展平\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # fc1 out: CNN 特征提取\n",
        "\n",
        "        temp_i  = tensor_i.unsqueeze(1)  # 只取 temp_i，形状变为 (batch_size, 1)\n",
        "        x       = torch.cat((x, temp_i), dim=1)  # 拼接 temp_i\n",
        "\n",
        "        x = F.relu(self.fc2(x))  # fc2 out: relu\n",
        "        x = self.fc3(x)  # fc3 out: 输出最终结果\n",
        "        return x"
      ],
      "metadata": {
        "id": "09WzyqJHL3pn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Improved3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Improved3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "        self.conv_res1 = nn.Conv3d(16, 32, kernel_size=1, stride=1)\n",
        "        self.conv_res2 = nn.Conv3d(32, 64, kernel_size=1, stride=1)\n",
        "\n",
        "        self.subregion_fc1 = nn.Conv3d(32, 31, kernel_size=4, stride=2, padding=1)\n",
        "        self.subregion_fc2 = nn.Conv3d(64, 31, kernel_size=4, stride=4)\n",
        "\n",
        "        self.pos_encoder1 = nn.Parameter(torch.zeros(16 * 16 * 16, 1, 32))  # (4096, 1, 32)\n",
        "        self.pos_encoder2 = nn.Parameter(torch.zeros(8 * 8 * 8, 1, 32))     # (512, 1, 32)\n",
        "\n",
        "        self.transformer1 = TransformerEncoderLayer(d_model=32, nhead=8, dim_feedforward=128, dropout=0.1, window_size=64)\n",
        "        self.transformer2 = TransformerEncoderLayer(d_model=32, nhead=8, dim_feedforward=128, dropout=0.1, window_size=32)\n",
        "\n",
        "        self.fc_transformer1 = nn.Linear((16 * 16 * 16) * 32, 256)\n",
        "        self.fc_transformer2 = nn.Linear((8 * 8 * 8) * 32, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(256 + 128, 64)\n",
        "        self.output_dim = len(conf.DB_items) - 1\n",
        "        self.fc3 = nn.Linear(64, self.output_dim)\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        print(f\"x shape: {x.shape}\")\n",
        "        print(f\"tensor_i shape: {tensor_i.shape}\")\n",
        "\n",
        "        if tensor_i.dim() > 1:\n",
        "            tensor_i = tensor_i.squeeze()\n",
        "            if tensor_i.dim() > 1:\n",
        "                tensor_i = tensor_i[:, 0]\n",
        "        assert tensor_i.dim() == 1, f\"Expected tensor_i to be 1D, got shape {tensor_i.shape}\"\n",
        "\n",
        "        x1 = F.relu(self.conv1(x.float()))\n",
        "        x1 = self.pool(x1)\n",
        "        print(f\"x1 shape after conv1 and pool: {x1.shape}\")\n",
        "\n",
        "        x2 = F.relu(self.conv2(x1))\n",
        "        x1_res = self.conv_res1(x1)\n",
        "        x2 = x2 + x1_res\n",
        "        x2 = self.pool(x2)\n",
        "        print(f\"x2 shape after conv2 and pool: {x2.shape}\")\n",
        "\n",
        "        x3 = F.relu(self.conv3(x2))\n",
        "        x2_res = self.conv_res2(x2)\n",
        "        x3 = x3 + x2_res\n",
        "        print(f\"x3 shape after conv3: {x3.shape}\")\n",
        "\n",
        "        x1 = self.subregion_fc1(x2)\n",
        "        print(f\"x1 shape after subregion_fc1: {x1.shape}\")\n",
        "        x2 = self.subregion_fc2(x3)\n",
        "        print(f\"x2 shape after subregion_fc2: {x2.shape}\")\n",
        "\n",
        "        temp_i = tensor_i.view(-1, 1, 1)\n",
        "        temp_1 = temp_i.repeat(1, 1, 16 * 16 * 16)\n",
        "        temp_2 = temp_i.repeat(1, 1, 8 * 8 * 8)\n",
        "        print(f\"temp_1 shape: {temp_1.shape}\")\n",
        "        print(f\"temp_2 shape: {temp_2.shape}\")\n",
        "\n",
        "        x1 = x1.view(-1, 31, 16 * 16 * 16)\n",
        "        print(f\"x1 shape after view: {x1.shape}\")\n",
        "        x1 = torch.cat((x1, temp_1), dim=1)\n",
        "        x1 = x1.permute(2, 0, 1)\n",
        "        x1 = x1 + self.pos_encoder1\n",
        "        x1 = self.transformer1(x1)\n",
        "        x1 = x1.permute(1, 0, 2)\n",
        "        x1 = x1.reshape(-1, (16 * 16 * 16) * 32)\n",
        "        x1 = F.relu(self.fc_transformer1(x1))\n",
        "\n",
        "        x2 = x2.view(-1, 31, 8 * 8 * 8)\n",
        "        x2 = torch.cat((x2, temp_2), dim=1)\n",
        "        x2 = x2.permute(2, 0, 1)\n",
        "        x2 = x2 + self.pos_encoder2\n",
        "        x2 = self.transformer2(x2)\n",
        "        x2 = x2.permute(1, 0, 2)\n",
        "        x2 = x2.reshape(-1, (8 * 8 * 8) * 32)\n",
        "        x2 = F.relu(self.fc_transformer2(x2))\n",
        "\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=32, nhead=8, dim_feedforward=128, dropout=0.1, window_size=64):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len, batch_size, d_model = x.shape\n",
        "        num_windows = (seq_len + self.window_size - 1) // self.window_size\n",
        "        for i in range(num_windows):\n",
        "            start = i * self.window_size\n",
        "            end = min((i + 1) * self.window_size, seq_len)\n",
        "            x[start:end] = self.transformer(x[start:end])\n",
        "        return x"
      ],
      "metadata": {
        "id": "AbM1VpuAfoir"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# lagacy-2: Trans/CNN with subregions\n",
        "\n",
        "class Improved3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Improved3DCNN, self).__init__()\n",
        "        self.conv1  = nn.Conv3d(1,  16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2  = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool   = nn.MaxPool3d(2, 2)        # 2x2x2 池化，尺寸减半\n",
        "\n",
        "        self.fc1          = nn.Linear(32 * 32 * 32 * 32, 128)               # CNN 提取的特征\n",
        "        self.transformer  = TransformerEncoderLayer(d_model=128, nhead=4)   # Transformer 处理特征\n",
        "        self.fc2          = nn.Linear(128 + 1, 64)          # 只加入 temp_i（1 维）\n",
        "        self.output_dim   = len(conf.DB_items) - 1          # Calculate output dimension\n",
        "        self.fc3          = nn.Linear(64, self.output_dim)  # Output dimension based on conf.DB_items\n",
        "\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = self.pool(F.relu(self.conv1(x.float())))  # 128³ -> 64³\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64³ -> 32³\n",
        "        x = x.view(-1, 32 * 32 * 32 * 32)     # 展平\n",
        "\n",
        "        x = F.relu(self.fc1(x))             # fc1 out: CNN 特征提取\n",
        "        x = x.unsqueeze(0)                  # Transformer 输入需要 (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)             # 通过 Transformer 处理\n",
        "        x = x.squeeze(0)                    # 恢复 batch 维度\n",
        "\n",
        "        temp_i  = tensor_i.unsqueeze(1)            # 只取 temp_i，形状变为 (batch_size, 1)\n",
        "        x       = torch.cat((x, temp_i), dim=1)    # 拼接 temp_i\n",
        "\n",
        "        x = F.relu(self.fc2(x))             # fc2 out: relu\n",
        "        x = self.fc3(x)                     # fc3 out: 输出最终结果\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, dim_feedforward=256, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer   = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)\n",
        "'''"
      ],
      "metadata": {
        "id": "F3VGGp5PLa8G",
        "outputId": "ff52525f-68dd-45bc-8d4e-8b368a37bc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# lagacy-2: Trans/CNN with subregions\\n\\nclass Improved3DCNN(nn.Module):\\n    def __init__(self):\\n        super(Improved3DCNN, self).__init__()\\n        self.conv1  = nn.Conv3d(1,  16, kernel_size=3, stride=1, padding=1)\\n        self.conv2  = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\\n        self.pool   = nn.MaxPool3d(2, 2)        # 2x2x2 池化，尺寸减半\\n\\n        self.fc1          = nn.Linear(32 * 32 * 32 * 32, 128)               # CNN 提取的特征\\n        self.transformer  = TransformerEncoderLayer(d_model=128, nhead=4)   # Transformer 处理特征\\n        self.fc2          = nn.Linear(128 + 1, 64)          # 只加入 temp_i（1 维）\\n        self.output_dim   = len(conf.DB_items) - 1          # Calculate output dimension\\n        self.fc3          = nn.Linear(64, self.output_dim)  # Output dimension based on conf.DB_items\\n\\n\\n    def forward(self, x, tensor_i):\\n        x = self.pool(F.relu(self.conv1(x.float())))  # 128³ -> 64³\\n        x = self.pool(F.relu(self.conv2(x)))  # 64³ -> 32³\\n        x = x.view(-1, 32 * 32 * 32 * 32)     # 展平\\n\\n        x = F.relu(self.fc1(x))             # fc1 out: CNN 特征提取\\n        x = x.unsqueeze(0)                  # Transformer 输入需要 (seq_len, batch, feature_dim)\\n        x = self.transformer(x)             # 通过 Transformer 处理\\n        x = x.squeeze(0)                    # 恢复 batch 维度\\n\\n        temp_i  = tensor_i.unsqueeze(1)            # 只取 temp_i，形状变为 (batch_size, 1)\\n        x       = torch.cat((x, temp_i), dim=1)    # 拼接 temp_i\\n\\n        x = F.relu(self.fc2(x))             # fc2 out: relu\\n        x = self.fc3(x)                     # fc3 out: 输出最终结果\\n        return x\\n\\nclass TransformerEncoderLayer(nn.Module):\\n    def __init__(self, d_model=128, nhead=4, dim_feedforward=256, dropout=0.1):\\n        super(TransformerEncoderLayer, self).__init__()\\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\\n        self.transformer   = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\\n\\n    def forward(self, x):\\n        return self.transformer(x)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# lagacy-1: Trans/CNN\n",
        "\n",
        "\n",
        "class Improved3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Improved3DCNN, self).__init__()\n",
        "        self.conv1  = nn.Conv3d(1,  16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2  = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool   = nn.MaxPool3d(2, 2)        # 2x2x2 池化，尺寸减半\n",
        "\n",
        "        self.fc1          = nn.Linear(32 * 32 * 32 * 32, 128)               # CNN 提取的特征\n",
        "        self.transformer  = TransformerEncoderLayer(d_model=128, nhead=4)   # Transformer 处理特征\n",
        "        self.fc2          = nn.Linear(128 + 1, 64)          # 只加入 temp_i（1 维）\n",
        "        self.output_dim   = len(conf.DB_items) - 1          # Calculate output dimension\n",
        "        self.fc3          = nn.Linear(64, self.output_dim)  # Output dimension based on conf.DB_items\n",
        "\n",
        "\n",
        "    def forward(self, x, tensor_i):\n",
        "        x = self.pool(F.relu(self.conv1(x.float())))  # 128³ -> 64³\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64³ -> 32³\n",
        "        x = x.view(-1, 32 * 32 * 32 * 32)     # 展平\n",
        "\n",
        "        x = F.relu(self.fc1(x))             # fc1 out: CNN 特征提取\n",
        "        x = x.unsqueeze(0)                  # Transformer 输入需要 (seq_len, batch, feature_dim)\n",
        "        x = self.transformer(x)             # 通过 Transformer 处理\n",
        "        x = x.squeeze(0)                    # 恢复 batch 维度\n",
        "\n",
        "        temp_i  = tensor_i.unsqueeze(1)            # 只取 temp_i，形状变为 (batch_size, 1)\n",
        "        x       = torch.cat((x, temp_i), dim=1)    # 拼接 temp_i\n",
        "\n",
        "        x = F.relu(self.fc2(x))             # fc2 out: relu\n",
        "        x = self.fc3(x)                     # fc3 out: 输出最终结果\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, dim_feedforward=256, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer   = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)\n",
        "'''"
      ],
      "metadata": {
        "id": "I7GablMcLiKQ",
        "outputId": "0985f1d7-ecc1-40ad-bdd9-7ec61ff00783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# lagacy-1: Trans/CNN\\n\\n\\nclass Improved3DCNN(nn.Module):\\n    def __init__(self):\\n        super(Improved3DCNN, self).__init__()\\n        self.conv1  = nn.Conv3d(1,  16, kernel_size=3, stride=1, padding=1)\\n        self.conv2  = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\\n        self.pool   = nn.MaxPool3d(2, 2)        # 2x2x2 池化，尺寸减半\\n\\n        self.fc1          = nn.Linear(32 * 32 * 32 * 32, 128)               # CNN 提取的特征\\n        self.transformer  = TransformerEncoderLayer(d_model=128, nhead=4)   # Transformer 处理特征\\n        self.fc2          = nn.Linear(128 + 1, 64)          # 只加入 temp_i（1 维）\\n        self.output_dim   = len(conf.DB_items) - 1          # Calculate output dimension\\n        self.fc3          = nn.Linear(64, self.output_dim)  # Output dimension based on conf.DB_items\\n\\n\\n    def forward(self, x, tensor_i):\\n        x = self.pool(F.relu(self.conv1(x.float())))  # 128³ -> 64³\\n        x = self.pool(F.relu(self.conv2(x)))  # 64³ -> 32³\\n        x = x.view(-1, 32 * 32 * 32 * 32)     # 展平\\n\\n        x = F.relu(self.fc1(x))             # fc1 out: CNN 特征提取\\n        x = x.unsqueeze(0)                  # Transformer 输入需要 (seq_len, batch, feature_dim)\\n        x = self.transformer(x)             # 通过 Transformer 处理\\n        x = x.squeeze(0)                    # 恢复 batch 维度\\n\\n        temp_i  = tensor_i.unsqueeze(1)            # 只取 temp_i，形状变为 (batch_size, 1)\\n        x       = torch.cat((x, temp_i), dim=1)    # 拼接 temp_i\\n\\n        x = F.relu(self.fc2(x))             # fc2 out: relu\\n        x = self.fc3(x)                     # fc3 out: 输出最终结果\\n        return x\\n\\nclass TransformerEncoderLayer(nn.Module):\\n    def __init__(self, d_model=128, nhead=4, dim_feedforward=256, dropout=0.1):\\n        super(TransformerEncoderLayer, self).__init__()\\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\\n        self.transformer   = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\\n\\n    def forward(self, x):\\n        return self.transformer(x)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Configuration"
      ],
      "metadata": {
        "id": "yl_ViHPRjoYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Confuration():\n",
        "  # define paths to directory\n",
        "  home_dir                = os.path.expanduser(\"~\")\n",
        "  working_dir             = os.path.join(os.getcwd(),'Lmp_ML')\n",
        "  DataBase_dir            = os.path.join(working_dir,'DataBase')\n",
        "  DB_version              = ''\n",
        "  DB_3D_Grids_density     = 128\n",
        "  DB_3D_Grids_path        = os.path.join(DataBase_dir, f'{DB_version}', f'3D_Grids_{DB_3D_Grids_density}.h5') # TODO: choose database version, here ver=4\n",
        "  DB_Excel_path           = os.path.join(working_dir,  f'LmpGP.xlsx')                                         # TODO: choose database version, here ver=4\n",
        "\n",
        "  # dataset\n",
        "  DB_items                = ['temp\\n(k)',\n",
        "                             'len\\n(A)',\n",
        "                             'density\\n(g/cm^3)',\n",
        "                             'pore_radius\\n(A)',\n",
        "                             'porosity\\n(unitless)',\n",
        "                             #'bond_density\\n(unitless)',\n",
        "                             'specific_surface_area\\n(m^2/g)',\n",
        "                             'tortuosity\\n(unitless)(smoothed_by_Surface area)',\n",
        "                             #'tortuosity\\n(unitless)',\n",
        "                             'flux\\n(L/m^2/h)(smoothed_by_Pore radius)(smoothed_by_Tortuosity)(smoothed_by_Porosity)',\n",
        "                             #'flux\\n(L/m^2/h)',,\n",
        "                             'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)(smoothed_by_Surface area)(smoothed_by_Pore radius)',\n",
        "                             #'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)',\n",
        "                             'thermal_conductivity\\n(W/(m·K))'\n",
        "                             ]\n",
        "\n",
        "  # training hyperpatameters\n",
        "  device              = None\n",
        "  model               = None\n",
        "  criterion           = nn.MSELoss()\n",
        "  optimizer           = optim.Adam\n",
        "  train_mode          = 'k-fold' #'simple'\n",
        "\n",
        "\n",
        "  # debug\n",
        "  dev_mode                = False\n",
        "\n",
        "conf = Confuration()\n",
        "conf.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "conf.model  = Improved3DCNN().to(conf.device) # TODO: select model"
      ],
      "metadata": {
        "id": "FW2lJM8WXWUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d01cb3-ddb2-435f-f6cc-c8d81df55400"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import code from git hub\n",
        "!git clone https://github.com/mbc2009/Lmp_ML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nla41gbQQuJ5",
        "outputId": "5ca77b63-1139-4043-8996-c30588a3296c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Lmp_ML'...\n",
            "remote: Enumerating objects: 755, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 755 (delta 52), reused 38 (delta 38), pack-reused 694 (from 3)\u001b[K\n",
            "Receiving objects: 100% (755/755), 163.99 MiB | 16.91 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make directory\n",
        "os.makedirs(conf.DataBase_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "pjO4Z6FqrZgE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download\n",
        "original_path = kagglehub.dataset_download(\"mbc2009/heat-and-mass-transfer\",force_download=True)\n",
        "# copy\n",
        "shutil.copytree(original_path, conf.DataBase_dir, dirs_exist_ok=True)\n",
        "# Remove\n",
        "#!rm -rf {original_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z_1Gn-B1mOa9",
        "outputId": "79a92265-87e7-4c6c-9cb4-4e529bdedd12"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Lmp_ML/DataBase'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Preparing Database"
      ],
      "metadata": {
        "id": "XNATVGXi3qJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Prepare Excel Data Base"
      ],
      "metadata": {
        "id": "xdsVuq64sDWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_hdf5_content(file_path:str,PrintStrcut=False)->int:\n",
        "  '''\n",
        "  check the content (name, quantity) of hdf5 file\n",
        "  input:\n",
        "    file_path: the path of hdf5 file\n",
        "    PrintStrcut: print the structure of hdf5 file\n",
        "  return:\n",
        "    the name and quantity of variables in hdf5 file\n",
        "  '''\n",
        "  # 初始化数据集计数器\n",
        "  dataset_count = 0\n",
        "\n",
        "  # 定义一个内部函数用于遍历 HDF5 文件内部\n",
        "  def count_datasets(name, obj):\n",
        "        nonlocal dataset_count\n",
        "        if isinstance(obj, h5py.Dataset):  # 判断是否为数据集\n",
        "            dataset_count += 1\n",
        "        elif isinstance(obj, h5py.Group):  # 判断是否为组\n",
        "            pass  # 如果是组，不计数\n",
        "\n",
        "  with h5py.File(file_path, \"r\") as h5f:\n",
        "\n",
        "        # 遍历文件内容以计数\n",
        "        h5f.visititems(count_datasets)\n",
        "\n",
        "        # 打印所有内变量名字\n",
        "        if PrintStrcut:\n",
        "          print(f\"文件结构:\")\n",
        "          h5f.visit(print)\n",
        "\n",
        "  print(f'文件数: {dataset_count}')\n",
        "\n",
        "  return dataset_count # 文件数\n"
      ],
      "metadata": {
        "id": "T0PhC4SjmZtM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Excel data into pandas data frame\n",
        "df = pd.read_excel(conf.DB_Excel_path, engine=\"openpyxl\")"
      ],
      "metadata": {
        "id": "8G5we5wnsDCL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 打印形状\n",
        "  print(df.shape)\n",
        "\n",
        "  # 读取列名\n",
        "  print(df.columns)"
      ],
      "metadata": {
        "id": "mjSiqXTlhZYT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 筛选列名对应列\n",
        "  df = df.loc[:, ['len\\n(A)', 'sigma\\n(A)', 'temp\\n(k)', 'flux\\n(L/m^2/h)', 'density\\n(g/cm^3)']]"
      ],
      "metadata": {
        "id": "lAes1TjFg59q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_from_pandas(len_i: int, sigma_i: int, temp_i: int, items: List[str], df: pd.DataFrame) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    根据 len_i, sigma_i, temp_i 在 pandas DataFrame 中查找指定 items，并返回一个 PyTorch Tensor。\n",
        "\n",
        "    Args:\n",
        "        len_i (int):        目标 len 值\n",
        "        sigma_i (int):      目标 sigma 值\n",
        "        temp_i (int):       目标 temp 值\n",
        "        items (List[str]):  需要提取的列名列表\n",
        "        df (pd.DataFrame):  数据源 Pandas DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Optional[torch.Tensor]: 若找到数据，则返回一个 Float32 类型的 PyTorch Tensor，否则返回 None。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 筛选符合条件的行，并提取多个列\n",
        "        item_values = df.loc[\n",
        "            (df[\"len\\n(A)\"] == len_i) &\n",
        "            (df[\"sigma\\n(A)\"] == sigma_i) &\n",
        "            (df[\"temp\\n(k)\"] == temp_i),\n",
        "            items\n",
        "        ]\n",
        "\n",
        "        # 确保只有一行数据，转换为 Tensor\n",
        "        if not item_values.empty:\n",
        "            tensor_values = torch.tensor(item_values.values.flatten(), dtype=torch.float32)\n",
        "            return tensor_values\n",
        "        else:\n",
        "            return None  # 没有匹配数据时返回 None\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"列名错误: {e}\")\n",
        "        return None\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"数据转换错误: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ZXqnsln3g0nG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 使用示例\n",
        "  items = fetch_from_pandas(len_i= 2, sigma_i=8, temp_i=373, items=['density\\n(g/cm^3)'], df=df)\n",
        "  print(items[0].item())"
      ],
      "metadata": {
        "id": "3GTSxD6GlDtF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.Check grid data base"
      ],
      "metadata": {
        "id": "DJ1Jn8Oo4qm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 查看hdf5文件内容\n",
        "  num_grids = check_hdf5_content(conf.DB_3D_Grids_path,PrintStrcut=False)"
      ],
      "metadata": {
        "id": "OinYQCkb46xc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 查看单个数据点\n",
        "  DB_3D_Grids = h5py.File(conf.DB_3D_Grids_path, \"r\")\n",
        "  GA          = DB_3D_Grids[f\"len_{2}_sigma_{18}_{343}\"][:]\n",
        "  print(f'矩阵形状:   {GA.shape}')\n",
        "  print(f'矩阵最大值: {np.max(GA)}')\n",
        "  print(f'矩阵最小值: {np.min(GA)}')\n",
        "  print(f\"矩阵 GA 中 {np.max(GA)} 的数量：{np.count_nonzero(GA == np.max(GA)) }\")"
      ],
      "metadata": {
        "id": "VD-kiwaI6UGt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化函数\n",
        "def plot_3D_Grid(matrix_3d):\n",
        "    \"\"\"\n",
        "    绘制三维点阵的三视图（正视图、侧视图、俯视图）。\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): 三维点阵数据。\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # 创建图形和坐标轴\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # 俯视图 (X-Y 平面)\n",
        "    axes[0].imshow(np.sum(matrix_3d, axis=0), cmap='viridis')  # 沿 Z 轴求和\n",
        "    axes[0].set_title('Top View (X-Y)')\n",
        "    axes[0].set_xlabel('X')\n",
        "    axes[0].set_ylabel('Y')\n",
        "\n",
        "    # 正视图 (Z-X 平面)\n",
        "    axes[1].imshow(np.sum(matrix_3d, axis=1), cmap='viridis')  # 沿 Y 轴求和\n",
        "    axes[1].set_title('Front View (Z-X)')\n",
        "    axes[1].set_xlabel('Z')\n",
        "    axes[1].set_ylabel('X')\n",
        "\n",
        "    # 侧视图 (Z-Y 平面)\n",
        "    axes[2].imshow(np.sum(matrix_3d, axis=2), cmap='viridis')  # 沿 X 轴求和\n",
        "    axes[2].set_title('Side View (Z-Y)')\n",
        "    axes[2].set_xlabel('Z')\n",
        "    axes[2].set_ylabel('Y')\n",
        "\n",
        "    # 显示图形\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "X0k9E-fD9Rv2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  plot_3D_Grid(GA)  # 将 GA 替换为你的三维点阵数据"
      ],
      "metadata": {
        "id": "Idw6WbDn_8JK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3.Home-brewing dataset"
      ],
      "metadata": {
        "id": "sh83ztK7I5gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myDataBase(Dataset):\n",
        "    def __init__(self,\n",
        "                 hdf5_3D_Grids_file_path:str,\n",
        "                 excel_Performance_and_Properties_file_path:str):\n",
        "        '''\n",
        "        name:  'len_{i}_sigma_{j}_temp_{k}'\n",
        "        label: [len_i, sigma_i, temp_i], type: pytorch tensor\n",
        "        structure: 3D grid, type: pytorch tensor\n",
        "        '''\n",
        "        ## GRID\n",
        "        # 3D grids (h5py file)\n",
        "        self.grids_hdf5     = h5py.File(hdf5_3D_Grids_file_path, 'r')\n",
        "        # Sorted name list of all GA structures (h5py file)\n",
        "        self.data_NameList  = self.sort_NameList((self.grids_hdf5.keys()))\n",
        "\n",
        "        ## EXCEL\n",
        "        # Performance & Properties (Excel => PD)\n",
        "        self.PnP_pd = pd.read_excel(excel_Performance_and_Properties_file_path, engine=\"openpyxl\")\n",
        "        self.PnP_pd = self.PnP_pd\n",
        "\n",
        "        # Normalize PD file\n",
        "        self.scaler           = MinMaxScaler()\n",
        "        self.PnP_pd_selected  = self.PnP_pd[conf.DB_items].copy()                # Select desired columns from the original DataFrame\n",
        "        self.PnP_pd_scaled    = self.scaler.fit_transform(self.PnP_pd_selected)  # Scale the selected data\n",
        "        self.PnP_pd_scaled    = pd.DataFrame(self.PnP_pd_scaled,                 # Convert to DataFrame\n",
        "                                          columns=self.PnP_pd_selected.columns)\n",
        "\n",
        "\n",
        "        joblib.dump(self.scaler, \"scaler.pkl\")  # 保存 scaler（用于推理时 inverse_transform 反归一化）\n",
        "        print('Scaler Saved')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_NameList)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## 1. Index ##\n",
        "        # 解析: index -> str\n",
        "        name_i      = self.data_NameList[index]\n",
        "\n",
        "        # 转换: str -> list [len_i, sigma_i, temp_i] -> tensor\n",
        "        label_i     = self.extract_label_from_key(name_i)\n",
        "\n",
        "        ## 2. GRIDS ##\n",
        "        # 提取 (hdf5 => np.ndarray)\n",
        "        grid_i      = self.grids_hdf5[name_i][:]\n",
        "        # 转换: Grids (np.ndarray -> PyTorch tensor)\n",
        "        grid_i      = torch.from_numpy(grid_i)\n",
        "\n",
        "        ## 3. EXCEL ##\n",
        "        # find row index\n",
        "        pd_idx   = self.seek_idx_from_pandas(len_i=label_i[0],\n",
        "                                            sigma_i=label_i[1],\n",
        "                                            temp_i=label_i[2],\n",
        "                                            items=conf.DB_items,\n",
        "                                            df=self.PnP_pd)\n",
        "        # Pandas => tensor ([item1,item2,item3...]\n",
        "        items_i  = self.PnP_pd_scaled.iloc[pd_idx]\n",
        "        items_i  = torch.tensor(items_i.values, dtype=torch.float32)\n",
        "\n",
        "        ## 4. 返回\n",
        "        return torch.tensor(label_i), grid_i, items_i\n",
        "\n",
        "    def close(self):\n",
        "         # 关闭h5文件，防止损坏\n",
        "        self.grids_hdf5.close()\n",
        "\n",
        "    def extract_label_from_key(self, name:str):\n",
        "        # 解析数据点名称为三维张量\n",
        "        len_val   = int(name.split('_')[1])\n",
        "        sigma_val = int(name.split('_')[3])\n",
        "        temp_val  = int(name.split('_')[-1])\n",
        "        label_i   = [len_val, sigma_val, temp_val]\n",
        "        return label_i\n",
        "\n",
        "    def seek_idx_from_pandas(self, len_i:int, sigma_i:int, temp_i:int, items:List[str], df: pd.DataFrame) -> torch.Tensor:\n",
        "        # ... (other parts of the function remain the same) ...\n",
        "\n",
        "        try:\n",
        "            item_idx = df.loc[\n",
        "                (df[\"len\\n(A)\"]   == len_i) &\n",
        "                (df[\"sigma\\n(A)\"] == sigma_i) &\n",
        "                (df[\"temp\\n(k)\"]  == temp_i),\n",
        "                items\n",
        "            ].index\n",
        "\n",
        "            # Check if item_idx is empty before accessing element 0\n",
        "            if len(item_idx) > 0:\n",
        "                return item_idx[0]\n",
        "            else:\n",
        "                # Handle the case where no matching rows are found\n",
        "                print(f\"Warning: No matching rows found for len={len_i}, sigma={sigma_i}, temp={temp_i}: {item_idx}\")\n",
        "                return -1 # or raise ValueError(\"No matching rows found\")\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"列名错误: {e}\")\n",
        "            return None\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"数据转换错误: {e}\")\n",
        "            return None\n",
        "\n",
        "    def sort_NameList(self, strings:str):\n",
        "        # 数据点列表名称排序\n",
        "        def key_func(s):\n",
        "            match = re.match(r\"len_(\\d+)_sigma_(\\d+)_(\\d+)\", s)  # 提取数字\n",
        "            if match:\n",
        "                len_val, sigma_val, temp_val = map(int, match.groups())\n",
        "                return (len_val, sigma_val, -temp_val)  # 第三个数字取反，实现降序\n",
        "            else:\n",
        "                return (float('inf'), float('inf'), float('-inf'))  # 处理不匹配的情况\n",
        "        return sorted(strings, key=key_func)  # 排序"
      ],
      "metadata": {
        "id": "LoTTY1WaJE6V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 创建\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path,conf.DB_Excel_path)\n",
        "\n",
        "  # 读取\n",
        "  label_i, grid_i, items_i  = dataset[0]\n",
        "  dataset.close() # 关闭读取\n",
        "\n",
        "  # 打印\n",
        "  print(f'{label_i}\\t\\t{type(label_i)}\\n{grid_i.shape}\\t{type(grid_i)}\\n{items_i}\\t{type(items_i)}')"
      ],
      "metadata": {
        "id": "ZjIttRqEnFhM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  print(dataset.PnP_pd)#.to_string())"
      ],
      "metadata": {
        "id": "VF-TltFg9bP3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  print(dataset.PnP_pd_scaled.to_string())"
      ],
      "metadata": {
        "id": "yKDUJtuZxx8B"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 查看文件名列表\n",
        "  # 长度\n",
        "  print(len(dataset.data_NameList))\n",
        "\n",
        "  # 内容\n",
        "  for i in dataset.data_NameList:\n",
        "      print(i)\n",
        "  dataset.close() # 关闭读取"
      ],
      "metadata": {
        "id": "LKuL5MuG7XVR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Data augmentation"
      ],
      "metadata": {
        "id": "w7SXUvb6Bkb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_3D_Grid(matrix_3d: np.ndarray, angle_degrees: float):\n",
        "    \"\"\"\n",
        "    绕穿过 x-y 平面的中心点且平行于 z 轴的轴旋转三维矩阵。\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): 要旋转的三维矩阵。\n",
        "        angle_degrees (float): 旋转角度（以度为单位，默认逆时针）。\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 旋转后的三维矩阵。\n",
        "    \"\"\"\n",
        "    rotated_matrix = rotate(matrix_3d,\n",
        "                            angle=-angle_degrees,  # 顺时针旋转\n",
        "                            axes=(1, 2),  # 旋转 x-y 平面，即绕 z 轴旋转\n",
        "                            reshape=False,\n",
        "                            order=0,\n",
        "                            mode='constant',\n",
        "                            cval=0)\n",
        "    return rotated_matrix\n",
        "\n",
        "def multi_threaded_rotation(matrix_3d: np.ndarray):\n",
        "    \"\"\"\n",
        "    并行计算 90°, 180°, 270° 三种旋转后的 3D 矩阵。\n",
        "\n",
        "    Args:\n",
        "        matrix_3d (numpy.ndarray): 要旋转的三维矩阵。\n",
        "\n",
        "    Returns:\n",
        "        dict: 包含 90°, 180°, 270° 旋转后的矩阵。\n",
        "    \"\"\"\n",
        "    angles          = [90, 180, 270]\n",
        "    rotated_results = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        results = executor.map(rotate_3D_Grid, [matrix_3d]*3, angles)\n",
        "\n",
        "    # 存储旋转后的矩阵\n",
        "    for angle, rotated_matrix in zip(angles, results):\n",
        "        rotated_results[f\"rotate_{angle}\"] = rotated_matrix\n",
        "\n",
        "    return rotated_results"
      ],
      "metadata": {
        "id": "7un57V8nBiPn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # simple case\n",
        "  GA = np.asarray([\n",
        "                  [[2,3,4],  # z = 3\n",
        "                  [1,2,3],\n",
        "                  [0,1,2]],\n",
        "                  [[2,2,2],   # z = 3\n",
        "                  [2,2,2],\n",
        "                  [2,2,2]],\n",
        "                  [[1,1,1],   # z = 2\n",
        "                  [1,1,1],\n",
        "                  [1,1,1]],\n",
        "                  [[0,0,0],   # z = 1\n",
        "                  [0,0,0],\n",
        "                  [0,0,0]],\n",
        "                  [[0,0,0],   # z = 0\n",
        "                  [0,0,0],\n",
        "                  [0,0,0]]]\n",
        "                  )\n",
        "\n",
        "  # 示例测试\n",
        "  rotated_matrices = multi_threaded_rotation(GA)\n",
        "\n",
        "  # 获取旋转后的结果\n",
        "  rotate_90  = rotated_matrices[\"rotate_90\"]\n",
        "  rotate_180 = rotated_matrices[\"rotate_180\"]\n",
        "  rotate_270 = rotated_matrices[\"rotate_270\"]\n",
        "\n",
        "  # 输出形状检查\n",
        "  print(rotate_90.shape, rotate_180.shape, rotate_270.shape)\n",
        "\n",
        "  # plot\n",
        "  plot_3D_Grid(GA)\n",
        "  plot_3D_Grid(rotate_90)\n",
        "  plot_3D_Grid(rotate_180)\n",
        "  plot_3D_Grid(rotate_270)"
      ],
      "metadata": {
        "id": "F60zIc1FCkJk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  DB_3D_Grids = h5py.File(conf.DB_3D_Grids_path, \"r\")\n",
        "  GA          = DB_3D_Grids[f\"len_{2}_sigma_{18}_{343}\"][:]\n",
        "\n",
        "\n",
        "  # 示例测试\n",
        "  rotated_matrices = multi_threaded_rotation(GA)\n",
        "\n",
        "  # 获取旋转后的结果\n",
        "  rotate_90  = rotated_matrices[\"rotate_90\"]\n",
        "  rotate_180 = rotated_matrices[\"rotate_180\"]\n",
        "  rotate_270 = rotated_matrices[\"rotate_270\"]\n",
        "\n",
        "  # 输出形状检查\n",
        "  print(rotate_90.shape, rotate_180.shape, rotate_270.shape)\n",
        "\n",
        "  # plot\n",
        "  plot_3D_Grid(GA)\n",
        "  plot_3D_Grid(rotate_90)\n",
        "  plot_3D_Grid(rotate_180)\n",
        "  plot_3D_Grid(rotate_270)"
      ],
      "metadata": {
        "id": "jAmVF9uaQqzH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotatedDataBase(myDataBase):\n",
        "    \"\"\"\n",
        "    3D 矩阵旋转数据集，返回 90°, 180°, 270° 旋转后的矩阵。\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset):\n",
        "        \"\"\"\n",
        "        初始化数据集。\n",
        "\n",
        "        Args:\n",
        "            data_list (list of np.ndarray): 原始 3D 矩阵列表，每个矩阵 shape=(400,400,400)。\n",
        "        \"\"\"\n",
        "        self.base_dataset     = base_dataset                # 原始数据集\n",
        "        self.rotation_angles  = [0, 90, 180, 270]           # 旋转角度\n",
        "        self.num_rotations    = len(self.rotation_angles)   # 旋转次数\n",
        "        #self.data_NameList    = base_dataset.data_NameList  # 数据点列表名称\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        数据集大小\n",
        "        \"\"\"\n",
        "        return int(len(self.base_dataset)*(self.num_rotations))\n",
        "\n",
        "    def __getitem__(self, new_idx):\n",
        "        \"\"\"\n",
        "        获取数据并返回旋转后的4个版本\n",
        "        Returns:\n",
        "            dict: {\"original\":   原始 3D 矩阵,\n",
        "                   \"rotate_90\":  顺时旋转 90°,\n",
        "                   \"rotate_180\": 顺时旋转 180°,\n",
        "                   \"rotate_270\": 顺时旋转 270°}\n",
        "        \"\"\"\n",
        "        # 计算index\n",
        "        rotation_idx    = new_idx %  (self.num_rotations)  # index within the rotation group\n",
        "        original_idx    = new_idx // (self.num_rotations)  # index in the oringal base dataset\n",
        "\n",
        "        # 原始矩阵\n",
        "        label_i, grid_i, items_i  = self.base_dataset[original_idx]\n",
        "\n",
        "        # 旋转\n",
        "        rotated_grid_i = rotate_3D_Grid(matrix_3d=grid_i, angle_degrees=self.rotation_angles[rotation_idx])\n",
        "\n",
        "        # 确保转换为 float32 的 PyTorch Tensor\n",
        "        rotated_grid_i = torch.from_numpy(rotated_grid_i).to(torch.float32)\n",
        "\n",
        "        # 添加 channel 维度 (channels, depth, height, width) for 3D CNN\n",
        "        #rotated_grid_i = rotated_grid_i.unsqueeze(0)\n",
        "\n",
        "        # 返回\n",
        "        return  label_i, rotated_grid_i, items_i"
      ],
      "metadata": {
        "id": "V9gz6eb3XudQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.dev_mode:\n",
        "  # 创建\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path,conf.DB_Excel_path)\n",
        "\n",
        "  # Augmentation\n",
        "  rotated_dataset = RotatedDataBase(dataset)\n",
        "\n",
        "  for idx in [0,1,2,3]:\n",
        "    idx += (11*4)\n",
        "    print(idx)\n",
        "    label_i, grid_i, items_i = rotated_dataset[idx]\n",
        "\n",
        "    print(f'check-{dataset.data_NameList[idx//4]}')\n",
        "\n",
        "    print(label_i, grid_i.shape, items_i)\n",
        "    print(type(grid_i))"
      ],
      "metadata": {
        "id": "kdEF5NDjhXTh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader speed test\n",
        "if False:\n",
        "\n",
        "  def load_data_item(dataset, index):\n",
        "      \"\"\"\n",
        "      加载单个数据项\n",
        "      \"\"\"\n",
        "      _, _, _ = dataset[index]\n",
        "\n",
        "\n",
        "  def calculate_loading_time_multithreaded(dataset, num_threads=4):\n",
        "      \"\"\"使用多线程计算加载数据集所需的时间。\"\"\"\n",
        "      start_time = time.time()\n",
        "\n",
        "      with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "          # 创建任务列表，每个任务加载一个数据项\n",
        "          tasks = [executor.submit(load_data_item, dataset, i) for i in range(len(dataset))]\n",
        "\n",
        "          # 使用 tqdm 显示进度条，并等待所有任务完成\n",
        "          for _ in tqdm(tasks, total=len(tasks), desc=\"多线程加载数据集...\"):\n",
        "              _.result()  # 获取任务结果，以确保任务已完成\n",
        "\n",
        "      end_time = time.time()\n",
        "      total_time = end_time - start_time\n",
        "\n",
        "      return total_time\n",
        "\n",
        "\n",
        "  # 创建数据集\n",
        "  dataset = myDataBase(conf.DB_3D_Grids_path, conf.DB_Excel_path)\n",
        "  rotated_dataset = RotatedDataBase(dataset)\n",
        "\n",
        "  # 计算并打印加载时间\n",
        "  loading_time = calculate_loading_time_multithreaded(rotated_dataset, num_threads=int(psutil.cpu_count(logical=True)))\n",
        "  print(f\"使用多线程加载整个 rotated_dataset 所需时间：{loading_time:.2f} 秒\")"
      ],
      "metadata": {
        "id": "-SYAs_dqq1RS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Loading Data"
      ],
      "metadata": {
        "id": "AtNcOFfAj9sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设定训练集和测试集比例\n",
        "train_ratio = 0.8  # 80% 训练集, 20% 测试集\n",
        "\n",
        "# 读取原始数据\n",
        "dataset = myDataBase(conf.DB_3D_Grids_path, conf.DB_Excel_path)\n",
        "\n",
        "# 计算划分数量\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size  = len(dataset) - train_size\n",
        "\n",
        "# 先在 原始数据 上进行划分\n",
        "indices = list(range(len(dataset)))  # 原始数据索引\n",
        "train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
        "\n",
        "# 创建训练集和测试集的 Subset\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "test_dataset  = Subset(dataset, test_indices)\n",
        "\n",
        "# 增强训练集\n",
        "train_dataset_augmented = RotatedDataBase(train_dataset)\n",
        "test_dataset_augmented  = test_dataset # testset no need to be augmented"
      ],
      "metadata": {
        "id": "ax_wFWLmkA9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69caaffb-bc6f-4acd-cc83-4888d9e55ad0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成 DataLoader\n",
        "train_loader = DataLoader(train_dataset_augmented, batch_size=32, shuffle=True, num_workers=psutil.cpu_count(logical=True))\n",
        "test_loader  = DataLoader(test_dataset_augmented,  batch_size=32, shuffle=False, num_workers=psutil.cpu_count(logical=True))\n",
        "\n",
        "print(f\"训练集大小: {len(train_dataset_augmented)}, 测试集大小: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "qhnW64cxnpSX",
        "outputId": "28c3bc19-d8c2-42f3-e2db-4ccb7ec44a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集大小: 2672, 测试集大小: 168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Model Trainning"
      ],
      "metadata": {
        "id": "QMAxND_E7Rb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization\n",
        "model     = conf.model\n",
        "criterion = conf.criterion\n",
        "device    = conf.device\n",
        "\n",
        "train_loss_history  = []\n",
        "val_loss_history    = []\n",
        "torch.cuda.empty_cache() # 清理缓存"
      ],
      "metadata": {
        "id": "BJAUxE_xlVeH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train_simple(num_epochs:int, learning_rate:float):\n",
        "  # 预设\n",
        "  conf.model.train() # 训练模式\n",
        "  opt = conf.optimizer(conf.model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # 训练\n",
        "  for epoch in range(num_epochs):\n",
        "      running_loss = 0.0\n",
        "      for label, grid, items in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "          label, grid, items = label.to(device), grid.to(device), items.to(device)\n",
        "          grid  = grid.unsqueeze(1)  # 添加 channel 维度 (channels, depth, height, width) for 3D CNN\n",
        "          opt.zero_grad()      # 清零 梯度\n",
        "          output = model(grid, items[:, 0])  # TODO: 确保 输入grid + temp_i, 前向传播\n",
        "          loss   = conf.criterion(output, items[:, 1:]) # TODO: 确保 输入 temp_i 以后的\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      # record\n",
        "      train_loss_history.append(running_loss / len(train_loader))\n",
        "      print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.6f}\")\n",
        "\n",
        "def model_train_KFold(num_epochs: int, learning_rate: float, num_fold: int):\n",
        "    # 预设\n",
        "    conf.model.train()  # 训练模式\n",
        "    opt = optim.Adam(conf.model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # KFold 交叉验证\n",
        "    kf = KFold(n_splits=num_fold, shuffle=True)  # 创建 KFold 对象\n",
        "\n",
        "    # 用于存储每个 fold 的验证损失\n",
        "    fold_val_losses = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset_augmented)):\n",
        "        # print\n",
        "        print(f\"\\nFold {fold + 1}\")\n",
        "\n",
        "        # 使用 Subset 创建训练集和验证集\n",
        "        train_subset = Subset(train_dataset_augmented, train_idx)\n",
        "        val_subset   = Subset(train_dataset_augmented, val_idx)\n",
        "\n",
        "        # 创建 DataLoader\n",
        "        train_loader_fold = DataLoader(train_subset, batch_size=32, shuffle=True,  num_workers=psutil.cpu_count(logical=True))\n",
        "        val_loader_fold   = DataLoader(val_subset,   batch_size=32, shuffle=False, num_workers=psutil.cpu_count(logical=True))\n",
        "\n",
        "        # 初始化 Early stopping 相关变量\n",
        "        best_val_loss               = float('inf')\n",
        "        patience                    = 5  # 设置 patience 值\n",
        "        epochs_without_improvement  = 0\n",
        "\n",
        "        # 训练模型\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for label, grid, items in tqdm(train_loader_fold, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "                label, grid, items  = label.to(device), grid.to(device), items.to(device)\n",
        "                grid                = grid.unsqueeze(1)  # 添加 channel 维度\n",
        "                opt.zero_grad()\n",
        "\n",
        "                output  = conf.model(grid, items[:, 0])\n",
        "                loss    = conf.criterion(output, items[:, 1:])\n",
        "\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # 计算验证集损失\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for label, grid, items in val_loader_fold:\n",
        "                    label, grid, items = label.to(device), grid.to(device), items.to(device)\n",
        "                    grid = grid.unsqueeze(1)  # 添加 channel 维度\n",
        "                    output    = conf.model(grid, items[:, 0])\n",
        "                    loss      = conf.criterion(output, items[:, 1:])\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            val_loss /= len(val_loader_fold)\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {running_loss / len(train_loader_fold):.7f} - Val Loss: {val_loss:.7f}\")\n",
        "\n",
        "            # Record\n",
        "            val_loss_history.append(val_loss)\n",
        "            train_loss_history.append(running_loss/len(train_loader_fold))\n",
        "\n",
        "            # ⚠️ Early Stopping 检查\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_without_improvement = 0  # reset\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                if epochs_without_improvement >= patience:\n",
        "                    print(f\"⚠️ Early stopping triggered at epoch {epoch + 1}\")\n",
        "                    break\n",
        "\n",
        "        # print\n",
        "        fold_val_losses.append(best_val_loss)\n",
        "        print(f\"Fold {fold + 1} Finished - Best Validation Loss: {best_val_loss:.7f}\")\n",
        "\n",
        "    # 计算平均验证损失\n",
        "    avg_val_loss = np.mean(fold_val_losses)\n",
        "    print(f\"✅ \\nAverage Validation Loss across all folds: {avg_val_loss:.7f}\")\n",
        "\n",
        "def plot_loss(plot_val_loss=False):\n",
        "  '''\n",
        "  绘制训练和验证损失曲线\n",
        "  '''\n",
        "  plt.figure(figsize=(10, 5))  # 设置图形大小\n",
        "  plt.plot(train_loss_history, label='Training Loss')\n",
        "  if plot_val_loss:\n",
        "    plt.plot(val_loss_history, label='Validation Loss', color='orange')\n",
        "  plt.xlabel('Iterations')  # 或 'Batches'，更准确\n",
        "  plt.ylabel('Loss (Log Scale)')  # 更新 y 轴标签\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.legend()  # 显示图例\n",
        "  plt.grid(True)  # 显示网格\n",
        "  plt.yscale('log')  # 设置 y 轴为对数刻度\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rzlNyHN-mS4B"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. OPTION: K-Fold Cross validation"
      ],
      "metadata": {
        "id": "kZBzgaWLuVBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.train_mode == \"k-fold\":\n",
        "  ## Trainning\n",
        "  train_start_time = time.time() # 记录时间\n",
        "  model_train_KFold(num_epochs=40,\n",
        "                    learning_rate=0.0001,\n",
        "                    num_fold=10)\n",
        "\n",
        "  ## Save and Record\n",
        "  torch.save(model.state_dict(), \"3d_cnn_model_v2.pth\") # 再次保存模型\n",
        "  train_end_time = time.time() # 记录时间\n",
        "  trainning_time = train_end_time - train_start_time # 计算训练时间\n",
        "\n",
        "  ## Print\n",
        "  print(\"✅ 继续训练完成，已保存为 `3d_cnn_model_v2.pth`\")\n",
        "  print(f\"🕛 训练耗时: {trainning_time:.2f} 秒\")\n",
        "\n",
        "  ## Plot: loss\n",
        "  plot_loss(plot_val_loss=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "hVCpsuZWuMa6",
        "outputId": "f7033487-9e04-4efa-fc17-f5f6354da459"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/40:   0%|          | 0/76 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: torch.Size([32, 1, 128, 128, 128])\n",
            "tensor_i shape: torch.Size([32])\n",
            "x1 shape after conv1 and pool: torch.Size([32, 16, 64, 64, 64])\n",
            "x2 shape after conv2 and pool: torch.Size([32, 32, 32, 32, 32])\n",
            "x3 shape after conv3: torch.Size([32, 64, 32, 32, 32])\n",
            "x1 shape after subregion_fc1: torch.Size([32, 31, 16, 16, 16])\n",
            "x2 shape after subregion_fc2: torch.Size([32, 31, 8, 8, 8])\n",
            "temp_1 shape: torch.Size([32, 1, 4096])\n",
            "temp_2 shape: torch.Size([32, 1, 512])\n",
            "x1 shape after view: torch.Size([32, 31, 4096])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/40:   0%|          | 0/76 [00:13<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (32) must match the size of tensor b (4096) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-55aed92d8acb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m## Trainning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtrain_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 记录时间\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   model_train_KFold(num_epochs=40,\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     num_fold=10)\n",
            "\u001b[0;32m<ipython-input-41-9a949db9a0be>\u001b[0m in \u001b[0;36mmodel_train_KFold\u001b[0;34m(num_epochs, learning_rate, num_fold)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutput\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mloss\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4aecbdf19f54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, tensor_i)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (4096) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. OPTION: Simple 2-stage trainning"
      ],
      "metadata": {
        "id": "4MBXgERwtWqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if conf.train_mode == \"simple\":\n",
        "  ## Trainning: stage-1\n",
        "  train_start_time = time.time() # 记录时间\n",
        "  model_train_simple(num_epochs=2,learning_rate=0.0002) # 训练\n",
        "  torch.save(model.state_dict(), \"3d_cnn_model.pth\") # save\n",
        "  print(\"✅ 模型训练完成，已保存！\")\n",
        "\n",
        "\n",
        "  ## Trainning: stage-2\n",
        "  model.load_state_dict(torch.load(\"3d_cnn_model.pth\")) # load model\n",
        "  model_train_simple(num_epochs=2,learning_rate=0.0001) # train\n",
        "\n",
        "  ## Save and Record\n",
        "  torch.save(model.state_dict(), \"3d_cnn_model_v2.pth\") # 再次保存模型\n",
        "  train_end_time = time.time() # 记录时间\n",
        "  trainning_time = train_end_time - train_start_time # 计算训练时间\n",
        "\n",
        "  ## Print\n",
        "  print(\"✅ 继续训练完成，已保存为 `3d_cnn_model_v2.pth`\")\n",
        "  print(f\"🕛 训练耗时: {trainning_time:.3f} 秒\")\n",
        "\n",
        "  ## Plot: loss\n",
        "  plot_loss()"
      ],
      "metadata": {
        "id": "cEgeh2w4cCEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\\\n",
        "As TC added, 4$e^{-4}$ shows gradient disapper (loss:Nan)\\\n",
        "$\\therefore$ 4$e^{-4}$ $\\rightarrow$ 2$e^{-4}$\\\n",
        "\n",
        "When reducing the amount of predicting parameters, loss drops faster at the first step (0.007 -> 0.004)"
      ],
      "metadata": {
        "id": "feyZshXnJPb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "when loss = 0.000139, seems to have good prediction capability"
      ],
      "metadata": {
        "id": "QLeQSbuSVaH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Model Testing"
      ],
      "metadata": {
        "id": "YvNzmgG3nLFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载模型\n",
        "model = conf.model                        # Create an instance of your model\n",
        "model.load_state_dict(torch.load('3d_cnn_model_v2.pth'))  # Load the saved state_dict\n",
        "model.eval()                                              # Set the model to evaluation mode\n",
        "\n",
        "# 用于存储预测值和真实值\n",
        "predictions_scaled   = []\n",
        "actual_values_scaled = []\n",
        "\n",
        "\n",
        "# 用于存储总的损失\n",
        "total_loss    = 0.0\n",
        "criterion     = conf.criterion\n",
        "test_loss_history    = []\n",
        "\n",
        "# 推理\n",
        "reasoning_start_time = time.time() # 记录时间\n",
        "with torch.no_grad():\n",
        "    for label, grid, items in tqdm(test_loader, desc=f\"Evaluating...\"): # Changed description\n",
        "        label, grid, items = label.to(device), grid.to(device), items.to(device)\n",
        "        grid    = grid.unsqueeze(1)               # 添加 channel 维度 (channels, depth, height, width) for 3D CNN\n",
        "        output  = model(grid, items[:, 0])        # TODO: 确保 输入grid + temp_i, 前向传播\n",
        "        loss    = criterion(output, items[:, 1:]) # TODO: 确保 输入 temp_i 以后的\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Append predictions and actual values to the lists\n",
        "        predictions_scaled.extend(output.cpu())         # Move predictions to CPU\n",
        "        actual_values_scaled.extend(items[:, 1:].cpu()) # Move actual values to CPU\n",
        "\n",
        "        # record\n",
        "        test_loss_history.append(total_loss / len(test_loader))\n",
        "\n",
        "reasoning_end_time = time.time() # 记录时间\n",
        "reasoning_time = reasoning_end_time - reasoning_start_time # 计算推理时间\n",
        "print(f\"🕛 推理耗时: {reasoning_time:.3f} 秒\")\n",
        "print(f\"Test Loss: {total_loss / len(test_loader):.6f}\")"
      ],
      "metadata": {
        "id": "5zn0vY-6nIZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Model Evaluation"
      ],
      "metadata": {
        "id": "iBFgqcg7i5No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.1 绝对误差 (MAE - Mean Absolute Error)**\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum |y_{true} - y_{pred}|\n",
        "$$\n",
        "\n",
        "- 衡量模型预测误差的平均绝对值。\n",
        "- 与 MSE 相比，MAE 不会放大较大的误差，对异常值（outliers）更鲁棒。"
      ],
      "metadata": {
        "id": "CkP2Oi2b2gse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAE = mean_absolute_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"📉 MAE: {MAE:.4f}\")"
      ],
      "metadata": {
        "id": "hcv9NmmB2blf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.2 均方误差(MSE - Mean Squared Error)**\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum \\left(y_{true} - y_{pred}\\right)^2\n",
        "$$\n",
        "\n",
        "- 衡量模型预测误差的平方平均值，放大较大误差，对异常值敏感。\n"
      ],
      "metadata": {
        "id": "ibbHFlLA3V4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"📉 MSE: {MSE:.4f}\")"
      ],
      "metadata": {
        "id": "6nF8q3oyjqGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.3. 均方根误差(RMSE - Root Mean Squared Error)**\n",
        "$$\n",
        "RMSE = \\sqrt{MSE}\n",
        "$$\n",
        "\n",
        "- RMSE = MSE 开平方，单位与原变量一致，更易解释。"
      ],
      "metadata": {
        "id": "6WnY_at14kFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = np.sqrt(MSE)\n",
        "print(f\"📉 RMSE: {RMSE:.4f}\")"
      ],
      "metadata": {
        "id": "9wk9Yb6_4p1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.4 平均百分比误差(MAPE - Mean Absolute Percentage Error)**\n",
        "$$\n",
        "MAPE = \\frac{1}{n} \\sum \\left| \\frac{y_{true} - y_{pred}}{y_{true}} \\right| x 100 \\text{%}\n",
        "$$\n",
        "\n",
        "- 衡量预测误差相对于真实值的百分比，适用于不同尺度的数据。\n",
        "- ⚠️ 适用于非负数据，否则可能会报错。\n"
      ],
      "metadata": {
        "id": "qSsg02pc5E-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "MAPE = mean_absolute_percentage_error(np.array(actual_values_scaled), np.array(predictions_scaled))\n",
        "print(f\"📉 MAPE: {MAPE:.2f}%\")"
      ],
      "metadata": {
        "id": "y6-OoYho5EUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.5 解释方差得分(Explained Variance Score)**\n",
        "$$\n",
        "EVS = 1 - \\frac{Var(y_{true}-y_{pred})}{Var(y_{true})}\n",
        "$$\n",
        "\n",
        "- 衡量模型对数据方差的解释能力，类似 R²，但不受数据缩放影响。"
      ],
      "metadata": {
        "id": "3QvF_FdO6v1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVS = explained_variance_score(actual_values_scaled, predictions_scaled)\n",
        "print(f\"📊 Explained Variance Score: {EVS:.4f}\")"
      ],
      "metadata": {
        "id": "nB_Dn7jM6jih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.6 平均对数误差(MSLE - Mean Squared Logarithmic Error)**\n",
        "$$\n",
        "\\text{MSLE} = \\frac{1}{n} \\sum \\left( \\log(1 + y_{\\text{true}}) - \\log(1 + y_{\\text{pred}}) \\right)^2\n",
        "$$\n",
        "\n",
        "- 适用于预测值变化范围较大的情况（如预测收入、人口等）。\n",
        "- 避免极端值影响，更关注相对误差。\n",
        "- ⚠️ 适用于非负数据，否则可能会报错。\n"
      ],
      "metadata": {
        "id": "6bY0onRP8PXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSLE = mean_squared_log_error(actual_values_scaled, predictions_scaled)\n",
        "print(f\"📉 MSLE: {MSLE:.4f}\")"
      ],
      "metadata": {
        "id": "Y-EDhu8O8Zn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.7 皮尔逊相关系数（Pearson Correlation Coefficient）**\n",
        "$$\n",
        "\\text{PCC} = \\frac{\\text{Cov}(y_{\\text{true}}, y_{\\text{pred}})}{\\sigma_{y_{\\text{true}}} \\cdot \\sigma_{y_{\\text{pred}}}}\n",
        "$$\n",
        "\n",
        "- 衡量预测值和真实值之间的线性相关性，范围在 \\([-1,1]\\) 之间：\n",
        "  - **+1** 表示完全正相关\n",
        "  - **0** 表示无相关性\n",
        "  - **-1** 表示完全负相关\n"
      ],
      "metadata": {
        "id": "4-hhL7qJ994k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCC, _ = scipy.stats.pearsonr(np.array(actual_values_scaled).flatten(), np.array(predictions_scaled).flatten())\n",
        "print(f\"📈 Pearson Correlation Coefficient: {PCC:.4f}\")"
      ],
      "metadata": {
        "id": "S81plJVH-BPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.7 方误差($R^{2}$ )**\n",
        "**R² 公式**\n",
        "\n",
        "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
        "\n",
        "其中：\n",
        "\n",
        "- $SS$ = Sum of Squares\n",
        "- $SS_{res} = \\sum (y_{\\text{true}} - y_{\\text{pred}})^2$ —— 残差平方和\n",
        "- $SS_{tot} = \\sum (y_{\\text{true}} - \\bar{y}_{\\text{true}})^2$ —— 总平方和\n",
        "- $\\bar{y}_{\\text{true}}$ 是真实值的均值。\n",
        "\n",
        "**R² 的解释**\n",
        "\n",
        "- **\\( R^2 = 1.0 \\)**：模型完美拟合数据。\n",
        "- **\\( 0 < R^2 < 1 \\)**：模型有一定的解释能力，但仍有误差。\n",
        "- **\\( R^2 $\\leq$ 0 \\)**：模型甚至比简单的均值预测还差。\n",
        "\n"
      ],
      "metadata": {
        "id": "0WesUyXZ4P-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算 R² Score\n",
        "R2 = r2_score(actual_values_scaled, predictions_scaled)\n",
        "print(f\"📊 R² Score: {R2:.4f}\")"
      ],
      "metadata": {
        "id": "nUt_1bVX4Mq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📌 8.8 训练与推理耗时 (Time consumed)**"
      ],
      "metadata": {
        "id": "6CBb7mdJ_qd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出评估结果\n",
        "print(f\"🕛 训练耗时：{trainning_time:3f} 秒\")\n",
        "print(f\"🕛 推理耗时: {reasoning_time:.3f} 秒\")"
      ],
      "metadata": {
        "id": "1i-7pkgonrqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 8.9 对比总结\n",
        "\n",
        "| 指标  | 作用 | 适用场景 |\n",
        "|------|------|--------|\n",
        "| **MSE**  | 误差平方平均，放大异常值 | 关注大误差 |\n",
        "| **RMSE** | MSE 开平方，与真实值单位一致 | 直观解释误差 |\n",
        "| **MAE**  | 误差绝对值平均 | 抗异常值能力强 |\n",
        "| **MAPE** | 误差的相对百分比 | 适用于不同尺度数据 |\n",
        "| **R²**   | 解释模型的拟合程度 | 衡量整体效果 |\n",
        "| **EVS**  | 解释数据方差的能力 | 类似 R²，但不受缩放影响 |\n",
        "| **MSLE** | 计算对数误差 | 适用于指数增长问题 |\n",
        "| **PCC**  | 相关性分析 | 判断预测值与真实值的线性相关性 |\n",
        "\n",
        "---\n",
        "\n",
        "📌 8.10 结论\n",
        "\n",
        "✅ **如果你关注误差的实际大小**：使用 **MSE / RMSE / MAE**  \n",
        "✅ **如果你需要标准化的模型评价**：使用 **R² / EVS**  \n",
        "✅ **如果数据具有不同尺度**：使用 **MAPE**  \n",
        "✅ **如果数据分布有长尾效应（指数增长）**：使用 **MSLE**  \n",
        "✅ **如果你想分析预测值和真实值的相关性**：使用 **PCC**  \n"
      ],
      "metadata": {
        "id": "yvf9QztH_5BY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 7.Vissualization"
      ],
      "metadata": {
        "id": "d3Rnv159jSWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check 归一化参数\n",
        "def scaler_checker(idx,scaler=joblib.load(\"scaler.pkl\")):\n",
        "  print(f'item:\\t{conf.DB_items[idx]}\\n'\n",
        "      f'min:\\t{scaler.data_min_[idx]:.2f} \\t\\t# 训练数据的最小值 (每列) \\n'\n",
        "      f'max:\\t{scaler.data_max_[idx]:.2f} \\t\\t# 训练数据的最大值 (每列) \\n'\n",
        "      f'range:\\t{scaler.data_range_[idx]:.2f} \\t\\t# max - min (每列) \\n'\n",
        "      f'offset:\\t{scaler.min_[idx]:.2f} \\t\\t# 归一化偏移量，计算公式：`-min * scaler`\\n'\n",
        "      f'scaler:\\t{scaler.scale_[idx]:.2f} \\t\\t# 归一化比例因子，计算公式：`1 / data_range_`\\n')"
      ],
      "metadata": {
        "id": "JtNyOwjYA_Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(1,len(conf.DB_items)):\n",
        "  scaler_checker(idx)"
      ],
      "metadata": {
        "id": "ODRWZcZ7eJvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "归一化公式：\n",
        "$$\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "$$\n",
        "\n",
        "反归一化：\n",
        "$$\n",
        "X_{\\text{original}} = X_{\\text{scaled}} \\times (X_{\\max} - X_{\\min}) + X_{\\min}\n",
        "$$\n",
        "\n",
        "Range:\n",
        "$$\n",
        "X_{\\text{range}} = X_{\\max} - X_{\\min}\n",
        "$$\n",
        "\n",
        "Offset:\n",
        "$$\n",
        "X_{\\text{offset}} = - X_{\\min} \\times X_{\\text{scaler}}\n",
        "$$\n",
        "\n",
        "Scaler:\n",
        "$$\n",
        "X_{\\text{scaler}} = \\frac{1}{X_{\\text{range}}}\n",
        "$$"
      ],
      "metadata": {
        "id": "OYP4L4qVgzur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 转换为 numpy 数组\n",
        "predictions_scaled   = np.array(predictions_scaled).copy()\n",
        "actual_values_scaled = np.array(actual_values_scaled).copy()\n",
        "\n",
        "# 反归一化\n",
        "reverse_nromalization = True\n",
        "if reverse_nromalization:\n",
        "  scaler          = joblib.load(\"scaler.pkl\")\n",
        "  scaler.min_     = scaler.min_[1:] # 只选择 scaler 的前 2 维\n",
        "  scaler.scale_   = scaler.scale_[1:]\n",
        "  predictions     = scaler.inverse_transform(predictions_scaled)\n",
        "  actual_values   = scaler.inverse_transform(actual_values_scaled)\n",
        "# 不做反归一化\n",
        "else:\n",
        "  predictions     = predictions_scaled\n",
        "  actual_values   = actual_values_scaled"
      ],
      "metadata": {
        "id": "bTXW7cwKjzGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义绘图参数\n",
        "def plot_prediction_results(actual_values, predictions, target_idx):\n",
        "    \"\"\"\n",
        "    绘制预测结果分析图（预测 vs 真实值、误差分布、误差 vs 真实值）\n",
        "\n",
        "    参数：\n",
        "    - actual_values: 真实值的 NumPy 数组 (N, 2)\n",
        "    - predictions: 预测值的 NumPy 数组 (N, 2)\n",
        "    - target_idx: 目标变量索引（0 或 1）\n",
        "    - target_name: 目标变量的名称 (str)\n",
        "    \"\"\"\n",
        "    target_name = conf.DB_items[target_idx + 1].replace('\\n', ' ')  # skip temp\n",
        "    errors = predictions[:, target_idx] - actual_values[:, target_idx]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # 1️⃣ 预测值 vs 真实值\n",
        "    axes[0].scatter(actual_values[:, target_idx], predictions[:, target_idx], c='blue', label='Predictions')\n",
        "    axes[0].plot([min(actual_values[:, target_idx]), max(actual_values[:, target_idx])],\n",
        "                 [min(actual_values[:, target_idx]), max(actual_values[:, target_idx])], 'r--', label='Ideal fit')\n",
        "    axes[0].set_xlabel(f'Actual Values ({target_name})')\n",
        "    axes[0].set_ylabel(f'Predicted Values ({target_name})')\n",
        "    axes[0].set_title(f'Predicted vs Actual Values ({target_name})')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # 2️⃣ 误差分布直方图\n",
        "    axes[1].hist(errors, bins=25, color='purple', edgecolor='black')\n",
        "    axes[1].set_xlabel(f'Prediction Error ({target_name})')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Distribution of Prediction Errors ({target_name})')\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # 3️⃣ 误差 vs 真实值\n",
        "    axes[2].scatter(actual_values[:, target_idx], errors, c='green')\n",
        "    axes[2].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[2].set_xlabel(f'Actual Values ({target_name})')\n",
        "    axes[2].set_ylabel(f'Prediction Error ({target_name})')\n",
        "    axes[2].set_title(f'Prediction Error vs Actual Values ({target_name})')\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'Prediction_Results.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Tnu_agM1nu8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 调用函数绘制目标值 1 (Target 1)\n",
        "for idx in range(0,predictions.shape[1]):\n",
        "  plot_prediction_results(actual_values, predictions, target_idx=idx)"
      ],
      "metadata": {
        "id": "zyXRYNe6eQCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Save"
      ],
      "metadata": {
        "id": "jyC7ZYN4F2ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save file ##\n",
        "\n",
        "# 获取当前时间戳（格式：YYYYMMDD_HHMMSS）\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 获取模型类型\n",
        "model_type = type(model).__name__  # 提取模型类名，如 \"Improved3DCNN\"\n",
        "\n",
        "# 构造文件名\n",
        "file_name = f\"model_results_{model_type}_{timestamp}.pkl\"\n",
        "file_path = f\"/content/{file_name}\"  # Colab 默认存储路径\n",
        "\n",
        "# 创建保存字典\n",
        "results_dict = {\n",
        "    \"predictions\": predictions,\n",
        "    \"actual_values\": actual_values,\n",
        "    \"scaler\": scaler,\n",
        "    \"train_loss_history\": train_loss_history,\n",
        "    \"test_loss_history\": test_loss_history,\n",
        "    \"trainning_time\": trainning_time,\n",
        "    \"reasoning_time\": reasoning_time,\n",
        "    \"R2\": R2,\n",
        "    \"MSE\": MSE,\n",
        "    \"RMSE\": RMSE,\n",
        "    \"MAE\": MAE,\n",
        "    \"EVS\": EVS,\n",
        "    \"MSLE\": MSLE,\n",
        "    \"PCC\": PCC\n",
        "}\n",
        "\n",
        "# 记录开始时间\n",
        "save_start_time = time.time()\n",
        "\n",
        "# 保存到文件\n",
        "with open(file_path, \"wb\") as f:\n",
        "    pickle.dump(results_dict, f)\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# 记录结束时间\n",
        "save_end_time = time.time()\n",
        "save_time = save_end_time - save_start_time\n",
        "\n",
        "# 获取文件大小\n",
        "file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "\n",
        "# 打印文件信息\n",
        "print(f\"✅ 结果已保存到:  {file_path}\")\n",
        "print(f\"💾 文件大小:     {file_size_mb:.3f} MB\")\n",
        "print(f\"⏳ 保存耗时       {save_time:.4f} 秒\")\n",
        "print(f\"📂 下载文件名:    {file_name}\")"
      ],
      "metadata": {
        "id": "30vMXExaF7E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Post Check ##\n",
        "if conf.dev_mode:\n",
        "  # 1️⃣ 读取保存的文件\n",
        "  if os.path.exists(file_path):\n",
        "      with open(file_path, \"rb\") as f:\n",
        "          loaded_results = pickle.load(f)\n",
        "      print(\"✅ 成功加载 model_results.pkl\")\n",
        "  else:\n",
        "      raise FileNotFoundError(f\"❌ 未找到文件: {file_path}\")\n",
        "\n",
        "  # 2️⃣ 读取变量\n",
        "  expected_keys = [\n",
        "      \"predictions\",\n",
        "      \"actual_values\",\n",
        "      \"scaler\",\n",
        "      \"train_loss_history\",\n",
        "      \"test_loss_history\",\n",
        "      \"test_loss_history\",\n",
        "      \"trainning_time\",\n",
        "      \"reasoning_time\",\n",
        "      \"R2\",\n",
        "      \"MSE\",\n",
        "      \"RMSE\",\n",
        "      \"MAE\",\n",
        "      \"EVS\",\n",
        "      \"MSLE\",\n",
        "      \"PCC\"\n",
        "  ]\n",
        "\n",
        "  # 检查是否所有变量都正确存储\n",
        "  missing_keys = [key for key in expected_keys if key not in loaded_results]\n",
        "  if missing_keys:\n",
        "      print(f\"⚠️ 缺少变量: {missing_keys}\")\n",
        "  else:\n",
        "      print(\"✅ 所有变量均已正确存储！\")\n",
        "\n",
        "  # 3️⃣ 测试变量的长度和内容\n",
        "  print(\"\\n📊 变量检查报告\")\n",
        "\n",
        "  for key, value in loaded_results.items():\n",
        "      if isinstance(value, (list, tuple, set)):\n",
        "          print(f\"🔹 {key}: 长度 {len(value)}\")\n",
        "      elif isinstance(value, dict):\n",
        "          print(f\"🔹 {key}: 包含 {len(value.keys())} 个键\")\n",
        "      elif isinstance(value, (int, float, str)):\n",
        "          print(f\"🔹 {key}: 值 {value}\")\n",
        "      else:\n",
        "          print(f\"🔹 {key}: 类型 {type(value)}\")\n",
        "\n",
        "  print(\"\\n✅ 核验完成！\")"
      ],
      "metadata": {
        "id": "6iQ7Jj64KOBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载文件（仅限 Google Colab）\n",
        "from google.colab import files  # 仅在 Google Colab 运行\n",
        "files.download(file_path)"
      ],
      "metadata": {
        "id": "H1c-M0W5LUIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 终止会话\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "YogN458yHH2Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}