{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Lmp_ML/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Enviornment Prepare"
      ],
      "metadata": {
        "id": "5LSSxtHN4uqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.0.Download \\& Import"
      ],
      "metadata": {
        "id": "BgcK3K-J_MpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "# install package\n",
        "pip install opencv-python pillow\n",
        "pip install segmentation_models_pytorch\n",
        "pip install kaggle\n",
        "pip install dropbox\n",
        "pip install scikit-image\n",
        "\n",
        "# remove unnecessary\n",
        "cd /content\n",
        "rm -rf *\n",
        "\n",
        "# from Git-Hub reciprotory\n",
        "git clone https://github.com/mbc2009/Lmp_ML\n",
        "rm -rf /content/Lmp_ML/main.ipynb"
      ],
      "metadata": {
        "id": "744mQNJ56I_2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "FZg3wlTp1HaJ"
      },
      "outputs": [],
      "source": [
        "# basic import\n",
        "import  os, sys, time, math, random, math, shutil, pickle\n",
        "from    datetime                import  datetime\n",
        "from    typing                  import  List, Tuple\n",
        "from    dropbox                 import  Dropbox\n",
        "from    tqdm                    import  tqdm\n",
        "from    matplotlib              import  pyplot      as plt\n",
        "import  numpy                                       as np\n",
        "import  pandas                                      as pd\n",
        "import  zipfile\n",
        "import  warnings\n",
        "import  shutil\n",
        "\n",
        "# advance import\n",
        "from    skimage                 import  io\n",
        "from    scipy                   import  interpolate\n",
        "from    scipy.interpolate       import  RegularGridInterpolator\n",
        "from    scipy.ndimage           import  generic_filter\n",
        "from    PIL                     import  Image\n",
        "\n",
        "# torch import\n",
        "import  torch\n",
        "from    torch                   import  nn          as      nn\n",
        "from    torch.nn                import  functional  as F\n",
        "import  torch.optim                                 as optim\n",
        "from    torch.utils.data        import  Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "from    torchvision             import  transforms, models\n",
        "from    torchvision.transforms  import  *\n",
        "import  torchvision.transforms.functional as TF\n",
        "import  kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1.Functions define"
      ],
      "metadata": {
        "id": "jqRmtqnK-s3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1.1.File Reader"
      ],
      "metadata": {
        "id": "AY_on1y6-4uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch data from EXCEL\n",
        "def fetch(len_i=10,\n",
        "          sigma_i=14,\n",
        "          temp_i=373,\n",
        "          item_i='flux\\n(L/m^2/h)',\n",
        "          original_file_path='../../Database/LmpGP.xlsx'\n",
        "          ):\n",
        "    # 加载Excel文件\n",
        "    df = pd.read_excel(original_file_path)\n",
        "    # 查找行号\n",
        "    row_number = df[\n",
        "        (df['len\\n(A)'] == len_i) &\n",
        "        (df['sigma\\n(A)'] == sigma_i) &\n",
        "        (df['temp\\n(k)'] == temp_i)\n",
        "    ].index\n",
        "    # 读取文件\n",
        "    return df.at[row_number[0], item_i]\n",
        "    # 项目名称对照表\n",
        "    '''\n",
        "        def __init__(self,):\n",
        "            self.label_dict = {\n",
        "                            \"pore_radius\\n(A)\":                                                                                                            (r\"Pore diameter ${D}_{\\mathrm{GA}}$\", r\" $\\mathrm{(nm)}$\"),\n",
        "                            \"pore_radius\\n(A)(smoothed_by_Sigma)\":                                                                                         (r\"Pore diameter ${D}_{\\mathrm{GA}}$\", r\" $\\mathrm{(nm)}$\"),\n",
        "                            \"thickness_of_box\\n(A)\":                                                                                                       (r\"GP thickness $t_{\\mathrm{GA}}$\", r\" $\\mathrm{ (\\AA)}$\"),\n",
        "                            \"bond_density\\n(unitless)\":                                                                                                    (r\"$C-C$ bond density ${\\rho}_{C-C}$\", r\" (unitless)\"),\n",
        "                            \"specific_surface_area\\n(m^2/g)\":                                                                                              (r\"Specific surface area $S_{\\mathrm{GA}}$\", r\" $\\mathrm{(m^{2}/g)}$\"),\n",
        "                            \"specific_surface_area\\n(m^2/g)(smoothed_by_Pore radius)\":                                                                     (r\"Specific surface area $S_{\\mathrm{GA}}$\", r\" $\\mathrm{(m^{2}/g)}$\"),\n",
        "                            \"surface_area\\n(A^2)\":                                                                                                         (r\"Surface area $A_{\\mathrm{GA}}$\", r\" $\\mathrm{({\\AA}^{2})}$\"),\n",
        "                            \"porosity\\n(unitless)\":                                                                                                        (r\"Porosity $\\phi_{\\mathrm{GA}}$\", r\" ($\\%$)\"),\n",
        "                            \"porosity\\n(unitless)(smoothed_by_Pore radius)\":                                                                               (r\"Porosity $\\phi_{\\mathrm{GA}}$\", r\" ($\\%$)\"),\n",
        "                            \"density\\n(g/cm^3)\":                                                                                                           (r\"Density ${\\rho}_{\\mathrm{GA}}$\",r\" $\\mathrm{ (mg/{cm}^{3})}$\"),\n",
        "                            'sigma\\n(A)':                                                                                                                  (r\"Sigma $\\sigma$\",r\" $\\mathrm{ (\\AA)}$\"),\n",
        "                            'Diffusivity_EA_filtrates\\n_in_membrane\\n(m^2/s)':                                                                             (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)':                                                                             (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)':                                                                   (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)(smoothed_by_Surface area)':                                         (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)(smoothed_by_Surface area)(smoothed_by_Pore radius)':                (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_Knudsen_PoreRadius\\n(m^2/s)':                                                                                     (r\"Knudsen diffusivity $\\mathcalD}_{Kn}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_Knudsen_PoreRadius\\n(m^2/s)Diffusivity_Knudsen_PoreRadius\\n(m^2/s)(smoothed_by_Pore radius)':                     (r\"Knudsen diffusivity $\\mathcalD}_{Kn}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'flux\\n(L/m^2/h)':                                                                                                             (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Sigma)':                                                                                          (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)':                                                                                    (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)(smoothed_by_Tortuosity)':                                                            (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{ (L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)(smoothed_by_Tortuosity)(smoothed_by_Porosity)':                                      (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{ (L/m^{2}/h)}$\"),\n",
        "                            'tortuosity\\n(unitless)':                                                                                                      (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'tortuosity\\n(unitless)(smoothed_by_Pore radius)':                                                                             (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'tortuosity\\n(unitless)(smoothed_by_Surface area)':                                                                            (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_cutoff':                                                                                                                 (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_1/2_peak':                                                                                                               (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_cutoff(smoothed_by_Pore radius)':                                                                                        (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            }\n",
        "            self.color_lsit()\n",
        "    '''\n",
        "\n",
        "\n",
        "# read the data fule\n",
        "class LmpGP_file_reader():\n",
        "    def __init__(self, len: int, sigma: int, temp: int, path=\"../../Database\"):\n",
        "        ## terminology of parameters\n",
        "        self.sigma = sigma\n",
        "        self.len   = len\n",
        "        self.temp  = temp\n",
        "        ## working path\n",
        "        self.path           =  path\n",
        "        self.As_path        =  f\"{self.path}/assemblys_pretreated\"\n",
        "        self.Di_path        =  f\"{self.path}/assemblys_distillated/len_{self.len}\"\n",
        "        self.folder         =  f\"len_{self.len}_sigma_{self.sigma}_{self.temp}\"\n",
        "        self.dump_file      =  f\"dump_di\"\n",
        "        self.data_file      =  f\"data.3_len_{self.len}_sigma_{self.sigma}_pretreated\"\n",
        "        self.temp_dump_file =  f\"./merged_dump_file.txt\"\n",
        "        ## auto excecuting\n",
        "        #self.process_AGMD_dump_file()\n",
        "        self.read_GP_data_file()\n",
        "    def read_GP_data_file(self):\n",
        "        ### initialize\n",
        "        self.GP_atom_count, self.GP_box_bounds_info, self.GP_atom_data = None, {}, []\n",
        "        file_path = os.path.join(f\"{self.As_path}/{self.data_file}\")\n",
        "        atom_section = False\n",
        "\n",
        "        ### extract info\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "\n",
        "                ## Get the number of atoms\n",
        "                if line.strip().endswith(\"atoms\"):\n",
        "                    self.GP_atom_count = int(line.split()[0])\n",
        "\n",
        "                ## Get the box info\n",
        "                # boudnds\n",
        "                if 'xlo xhi' in line:\n",
        "                    self.GP_box_bounds_info['x'] = list(map(float, line.strip().split()[:2]))\n",
        "                if 'ylo yhi' in line:\n",
        "                    self.GP_box_bounds_info['y'] = list(map(float, line.strip().split()[:2]))\n",
        "                if 'zlo zhi' in line:\n",
        "                    self.GP_box_bounds_info['z'] = list(map(float, line.strip().split()[:2]))\n",
        "\n",
        "                ## Read atom data\n",
        "                # Check title\n",
        "                if 'Atoms' in line and '#' in line:\n",
        "                    atom_section = True\n",
        "                    continue\n",
        "                # read\n",
        "                if atom_section and line.strip():\n",
        "                    parts = line.split()\n",
        "                    if len(parts) >= 6:  # valid line Ensured\n",
        "                        atom_id, atom_type, x, y, z = int(parts[0]), int(parts[1]), float(parts[4]), float(parts[5]), float(parts[6])\n",
        "                        self.GP_atom_data.append([atom_id, atom_type, x, y, z])\n",
        "\n",
        "            # calculate side length\n",
        "            x_length = float(self.GP_box_bounds_info['x'][1]) - float(self.GP_box_bounds_info['x'][0])\n",
        "            y_length = float(self.GP_box_bounds_info['y'][1]) - float(self.GP_box_bounds_info['y'][0])\n",
        "            z_length = float(self.GP_box_bounds_info['z'][1]) - float(self.GP_box_bounds_info['z'][0])\n",
        "            self.GP_box_lengths_info = np.array([x_length, y_length, z_length])\n",
        "\n",
        "        ### post-treatment\n",
        "        ## convert\n",
        "        self.GP_atom_data = np.array(self.GP_atom_data)\n",
        "        ## sorting\n",
        "        # 首先按照 type 排序，然后在 type 相同的情况下按 ID 排序\n",
        "        if hasattr(self, 'GP_atom_data') and self.GP_atom_data.size > 0:\n",
        "            # 获取排序后的索引\n",
        "            sorted_indices = np.lexsort((self.GP_atom_data[:, 0], self.GP_atom_data[:, 1]))  # 先按 ID，再按 type 排序\n",
        "            # 应用排序\n",
        "            self.GP_atom_data = self.GP_atom_data[sorted_indices]\n",
        "\n",
        "    def process_AGMD_dump_file(self):\n",
        "        ## Find dump file\n",
        "        path = f\"{self.Di_path}/{self.folder}\"\n",
        "        # list all files\n",
        "        file_list = os.listdir(path)\n",
        "        # open folders\n",
        "        file_list = [f for f in file_list if os.path.isfile(os.path.join(path, f))]\n",
        "        # select dump file only\n",
        "        file_list = [f for f in file_list if self.dump_file in f]\n",
        "        # sort by time label\n",
        "        file_list = [f.replace(f\"{self.dump_file}_\", \"\") for f in file_list]\n",
        "        file_list.sort()\n",
        "        file_list = [f\"{path}/{self.dump_file}_{f}\" for f in file_list]\n",
        "\n",
        "        ## Combine dump files into one\n",
        "        output_file_path = os.path.join(\"./merged_dump_file.txt\")\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            for file_name in file_list:\n",
        "                with open(os.path.join(file_name), 'r') as input_file:\n",
        "                    output_file.write(input_file.read())\n",
        "\n",
        "        ### Collect basic info\n",
        "        ## Comput TIMESTEP positions list\n",
        "        with open(self.temp_dump_file, 'r') as file:\n",
        "            for line in file:\n",
        "                ## box info\n",
        "                if 'ITEM: BOX BOUNDS' in line:\n",
        "                    self.box_bounds_info = {}\n",
        "                    self.box_bounds_info[\"x\"] = list(map(float, next(file).strip().split()))\n",
        "                    self.box_bounds_info[\"y\"] = list(map(float, next(file).strip().split()))\n",
        "                    self.box_bounds_info[\"z\"] = list(map(float, next(file).strip().split()))\n",
        "                    x_length = float(self.box_bounds_info['x'][1]) - float(self.box_bounds_info['x'][0])\n",
        "                    y_length = float(self.box_bounds_info['y'][1]) - float(self.box_bounds_info['y'][0])\n",
        "                    z_length = float(self.box_bounds_info['z'][1]) - float(self.box_bounds_info['z'][0])\n",
        "                    self.box_lengths_info = np.array([x_length, y_length, z_length])\n",
        "                    break\n",
        "        ## build timesteps list\n",
        "        self.timestep_positions, self.timestep_list = {}, []\n",
        "        timestep_checker = 200000\n",
        "        with open(self.temp_dump_file, 'r') as file:\n",
        "            line_index = -1\n",
        "            while True:\n",
        "                line = file.readline()\n",
        "                line_index += 1\n",
        "                if not line:\n",
        "                    break  # 到达文件末尾\n",
        "                if 'ITEM: TIMESTEP' in line:\n",
        "                    # 读取时间步的值\n",
        "                    timestep = int(file.readline().strip())\n",
        "                    line_index += 1\n",
        "                    if timestep != timestep_checker:\n",
        "                        # 记录当前时间步的起始行号\n",
        "                        self.timestep_positions[timestep] = line_index\n",
        "                        # 将步数记入列表\n",
        "                        self.timestep_list.append(timestep)\n",
        "                        timestep_checker = timestep\n",
        "    def read_single_timestep_for__AGMD_dump_file(self, timestep):\n",
        "        #### 打开文件并开始读取\n",
        "        file_path = os.path.join(self.temp_dump_file)\n",
        "        with open(file_path, 'r') as file:\n",
        "            ### Jump to timestep\n",
        "            file.seek(0)  # 开始时先回到文件开头\n",
        "            for _ in range(self.timestep_positions[timestep]):  # 跳过line_number - 1行\n",
        "                file.readline()\n",
        "            ## Current number of atoms\n",
        "            while True:\n",
        "                line = next(file)\n",
        "                if 'ITEM: NUMBER OF ATOMS' in line:\n",
        "                    self.atom_count = int(next(file).strip())\n",
        "                    break\n",
        "            ## Skip\n",
        "            while True:\n",
        "                line = next(file)\n",
        "                if 'ITEM: ATOMS' in line:\n",
        "                    break\n",
        "\n",
        "            ### Extract atoms info\n",
        "            atom_data = []\n",
        "            for _ in range(self.atom_count):\n",
        "                # 读取并分割行\n",
        "                atom_info_str = next(file).strip().split()\n",
        "                # 分别处理不同类型的数据\n",
        "                # ITEM: ATOMS id type xs ys zs\n",
        "                atom_info = [int(atom_info_str[0]), int(atom_info_str[1])] + [float(x) for x in atom_info_str[2:]]\n",
        "                atom_data.append(atom_info)\n",
        "\n",
        "        ### Post treatment\n",
        "        # convert to NumPy matrix\n",
        "        self.atom_data = np.array(atom_data)\n",
        "        ## de-normalize\n",
        "        if hasattr(self, 'box_lengths_info') and self.atom_data.size > 0:\n",
        "            for i in range(self.atom_data.shape[0]):\n",
        "                # xs\n",
        "                self.atom_data[i, 2] = self.atom_data[i, 2] * self.box_lengths_info[0] + float(self.box_bounds_info[\"x\"][0])\n",
        "                # ys\n",
        "                self.atom_data[i, 3] = self.atom_data[i, 3] * self.box_lengths_info[1] + float(self.box_bounds_info[\"y\"][0])\n",
        "                # zs\n",
        "                self.atom_data[i, 4] = self.atom_data[i, 4] * self.box_lengths_info[2] + float(self.box_bounds_info[\"z\"][0])\n",
        "        ## sorting\n",
        "        # 首先按照 type 排序，然后在 type 相同的情况下按 ID 排序\n",
        "        if hasattr(self, 'atom_data') and self.atom_data.size > 0:\n",
        "            # 获取排序后的索引\n",
        "            sorted_indices = np.lexsort((self.atom_data[:, 0], self.atom_data[:, 1]))  # 先按 ID，再按 type 排序\n",
        "            # 应用排序\n",
        "            self.atom_data = self.atom_data[sorted_indices]\n",
        "    def build_dump_atoms_info_list__for__AGMD_dump_file(self):\n",
        "        # 初始化一个列表来存储每个时间步的原子信息矩阵\n",
        "        self.atom_data_list = []\n",
        "        # 遍历每个时间步\n",
        "        for timestep in self.timestep_positions.keys():\n",
        "            # 读取该时间步的原子信息\n",
        "            self.read_single_timestep_for__AGMD_dump_file(timestep)\n",
        "            # 将原子信息矩阵添加到列表中\n",
        "            self.atom_data_list.append(self.atom_data)\n",
        "            print(timestep)\n",
        "    #def check_atom_data_list(self):\n",
        "        #self.timestep_list\n",
        "        #self.box_bounds_info\n",
        "    def check_atom_data_list(self, output_filename=\"check_atom_data_list.txt\"):\n",
        "        \"\"\"\n",
        "        将atom_data_list的内容输出为LAMMPS dump文件格式\n",
        "        :param output_filename: 输出文件名\n",
        "        \"\"\"\n",
        "        with open(output_filename, 'w') as dump_file:\n",
        "            for i, atom_data in enumerate(self.atom_data_list):\n",
        "                # 写入时间步\n",
        "                dump_file.write(\"ITEM: TIMESTEP\\n\")\n",
        "                dump_file.write(f\"{self.timestep_list[i]}\\n\")\n",
        "                # 写入原子数量\n",
        "                dump_file.write(\"ITEM: NUMBER OF ATOMS\\n\")\n",
        "                dump_file.write(f\"{len(atom_data)}\\n\")\n",
        "                # 写入盒子边界\n",
        "                dump_file.write(\"ITEM: BOX BOUNDS pp pp ff\\n\")\n",
        "                for bound in ['x', 'y', 'z']:\n",
        "                    dump_file.write(f\"{self.box_bounds_info[bound][0]} {self.box_bounds_info[bound][1]}\\n\")\n",
        "                # 写入原子信息\n",
        "                dump_file.write(\"ITEM: ATOMS id type x y z\\n\")\n",
        "                for atom in atom_data:\n",
        "                    dump_file.write(\" \".join(map(str, atom)) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "OinYQCkb46xc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1.2.Data Pretreeatment"
      ],
      "metadata": {
        "id": "EgpnnqGj_ctr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Datafile_to_grids():\n",
        "    def __init__(self,\n",
        "                 GP_data,      # data file 原始数据\n",
        "                 grid_size,    # 网格划分的尺寸\n",
        "                 limits,       # 实际的 x/y/z 边界上下限\n",
        "                 limits_new,  # 需要补全到的 x/y/z 边界上下限\n",
        "                 output_filename = \"CHECK_DataFile\"\n",
        "                 ):\n",
        "        self.output_filename = output_filename\n",
        "        self.GP_data = GP_data\n",
        "        self.grid_size = grid_size\n",
        "        self.xmin, self.xmax = limits[\"x\"]\n",
        "        self.ymin, self.ymax = limits[\"y\"]\n",
        "        self.zmin, self.zmax = limits[\"z\"]\n",
        "        self.xmin_new, self.xmax_new = limits_new[\"x\"]\n",
        "        self.ymin_new, self.ymax_new = limits_new[\"y\"]\n",
        "        self.zmin_new, self.zmax_new = limits_new[\"z\"]\n",
        "\n",
        "        # 计算原始边界和新边界的长度\n",
        "        self.xlength = self.xmax - self.xmin\n",
        "        self.ylength = self.ymax - self.ymin\n",
        "        self.zlength = self.zmax - self.zmin\n",
        "        self.xlength_new = self.xmax_new - self.xmin_new\n",
        "        self.ylength_new = self.ymax_new - self.ymin_new\n",
        "        self.zlength_new = self.zmax_new - self.zmin_new\n",
        "\n",
        "        # 自动执行\n",
        "        self.GP_data_pretreat()\n",
        "        self.GP_data_analysis()\n",
        "        self.GP_data_aligning()\n",
        "        self.GP_data_to_grid()\n",
        "        self.check_atom_data_list()\n",
        "\n",
        "    def GP_data_pretreat(self):\n",
        "        self.GP_data = np.delete(self.GP_data, 1, axis=1)  # 删除 atom type 列\n",
        "\n",
        "    def GP_data_analysis(self):\n",
        "        self.GP_atom_count = self.GP_data.shape[0]\n",
        "\n",
        "    def GP_data_aligning(self):\n",
        "        '''\n",
        "        对data file 进行补全\n",
        "        '''\n",
        "        # 计算需要复制的倍数，包括正方向和负方向\n",
        "        x_repeats_pos = int(np.ceil((self.xmax_new - self.xmax) / self.xlength))\n",
        "        x_repeats_neg = int(np.ceil((self.xmin - self.xmin_new) / self.xlength))\n",
        "        y_repeats_pos = int(np.ceil((self.ymax_new - self.ymax) / self.ylength))\n",
        "        y_repeats_neg = int(np.ceil((self.ymin - self.ymin_new) / self.ylength))\n",
        "        z_repeats_pos = int(np.ceil((self.zmax_new - self.zmax) / self.zlength))\n",
        "        z_repeats_neg = int(np.ceil((self.zmin - self.zmin_new) / self.zlength))\n",
        "\n",
        "        # 扩展原子坐标，包括正方向和负方向\n",
        "        extended_atoms = []\n",
        "        new_atom_id = 1\n",
        "        for i in range(-x_repeats_neg, x_repeats_pos + 1):\n",
        "            for j in range(-y_repeats_neg, y_repeats_pos + 1):\n",
        "                for k in range(-z_repeats_neg, z_repeats_pos + 1):\n",
        "                    for atom in self.GP_data:\n",
        "                        new_atom = atom.copy()\n",
        "                        new_atom[0] = new_atom_id  # 更新原子ID\n",
        "                        new_atom[1] = atom[1] + i * self.xlength\n",
        "                        new_atom[2] = atom[2] + j * self.ylength\n",
        "                        new_atom[3] = atom[3] + k * self.zlength\n",
        "                        extended_atoms.append(new_atom)\n",
        "                        new_atom_id += 1\n",
        "\n",
        "        # 转换为numpy数组\n",
        "        extended_atoms = np.array(extended_atoms)\n",
        "\n",
        "        # 过滤掉超出新边界的原子\n",
        "        extended_atoms = extended_atoms[\n",
        "            (extended_atoms[:, 1] >= self.xmin_new) & (extended_atoms[:, 1] < self.xmax_new) &\n",
        "            (extended_atoms[:, 2] >= self.ymin_new) & (extended_atoms[:, 2] < self.ymax_new) &\n",
        "            (extended_atoms[:, 3] >= self.zmin_new) & (extended_atoms[:, 3] < self.zmax_new)\n",
        "        ]\n",
        "\n",
        "        # 更新GP_data，并重新分配ID以确保连续\n",
        "        self.GP_data = extended_atoms\n",
        "        self.GP_data[:, 0] = np.arange(1, self.GP_data.shape[0] + 1)\n",
        "        self.GP_atom_count_extended = self.GP_data.shape[0]\n",
        "\n",
        "    def GP_data_to_grid(self):\n",
        "        '''\n",
        "        将原子数据转换为网格数据\n",
        "        '''\n",
        "        # N 格子, N+1 个点\n",
        "        x_bins = np.linspace(self.xmin_new, self.xmax_new, self.grid_size[0] + 1)\n",
        "        y_bins = np.linspace(self.ymin_new, self.ymax_new, self.grid_size[1] + 1)\n",
        "        z_bins = np.linspace(self.zmin_new, self.zmax_new, self.grid_size[2] + 1)\n",
        "\n",
        "        # 生成 X * Y * Z 形状的矩阵 (grid_size=(XY,Z))\n",
        "        grid_counts = np.zeros(self.grid_size)\n",
        "\n",
        "        for atom in self.GP_data:\n",
        "            x_idx = np.digitize(atom[1], x_bins) - 1    # digitize() 从1开始\n",
        "            y_idx = np.digitize(atom[2], y_bins) - 1\n",
        "            z_idx = np.digitize(atom[3], z_bins) - 1\n",
        "            if x_idx < self.grid_size[0] and y_idx < self.grid_size[1] and z_idx < self.grid_size[2]:\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "\n",
        "        self.grid_counts = grid_counts\n",
        "\n",
        "        # 打印结果以检查\n",
        "        #print(f\"Grid counts shape: {self.grid_counts.shape}\")\n",
        "        #print(self.grid_counts)\n",
        "\n",
        "    def check_atom_data_list(self):\n",
        "        \"\"\"\n",
        "        将atom_data_list的内容输出为LAMMPS dump文件格式\n",
        "        :param output_filename: 输出文件名\n",
        "        \"\"\"\n",
        "\n",
        "        with open(self.output_filename, 'w') as data_file:\n",
        "            # 写入基本信息\n",
        "            data_file.write(\"LAMMPS data file via write_data, version 2 Aug 2023, timestep = 50000, units = metal\\n\\n\")\n",
        "            data_file.write(f\"{self.GP_atom_count_extended} atoms\\n\")\n",
        "            data_file.write(\"1 atom types\\n\\n\")\n",
        "            data_file.write(f\"{self.xmin_new} {self.xmax_new} xlo xhi\\n\")\n",
        "            data_file.write(f\"{self.ymin_new} {self.ymax_new} ylo yhi\\n\")\n",
        "            data_file.write(f\"{self.zmin_new} {self.zmax_new} zlo zhi\\n\\n\")\n",
        "            data_file.write(\"Masses\\n\\n\")\n",
        "            data_file.write(\"1 12.0107\\n\\n\")\n",
        "            data_file.write(\"Atoms # full\\n\\n\")\n",
        "            for atom in self.GP_data:\n",
        "                atom_inserted = np.insert(atom, 1, 1)                            # 在第二项位置插入 atom type   为 1\n",
        "                atom_inserted = np.insert(atom_inserted, 1, 1)                   # 在第二项位置插入 molecule ID 为 1\n",
        "                atom_inserted = np.insert(atom_inserted, 3, 0)                   # 在第二项位置插入 charge      为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity X  为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity Y  为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity Z  为 0\n",
        "\n",
        "                # 将 atom ID 和 molecule ID 转换为整数, 其他保持原有精度\n",
        "                formatted_line = f\"{int(atom_inserted[0])} {int(atom_inserted[1])} {int(atom_inserted[2])} {int(atom_inserted[3])} \" \\\n",
        "                                 f\"{atom_inserted[4]} {atom_inserted[5]} {atom_inserted[6]} \" \\\n",
        "                                 f\"{int(atom_inserted[7])} {int(atom_inserted[8])} {int(atom_inserted[9])}\"\n",
        "\n",
        "                data_file.write(formatted_line + \"\\n\")\n",
        "\n",
        "\n",
        "class GridDataset(Dataset):\n",
        "    def __init__(self, grid_counts_dict):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, grid in grid_counts_dict.items():\n",
        "            self.data.append(grid)\n",
        "            self.labels.append(self.extract_label_from_key(label))\n",
        "\n",
        "        # 转换为 torch.Tensor\n",
        "        self.data   = torch.tensor(self.data, dtype=torch.float32).unsqueeze(1)  # 添加 channel 维度\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.float32)\n",
        "\n",
        "    def extract_label_from_key(self, key):\n",
        "        # 解析出 len_i, sigma_i, temp_i\n",
        "        parts = key.split('_')\n",
        "        len_i = int(parts[1])\n",
        "        sigma_i = int(parts[3])\n",
        "        temp_i = int(parts[-1])\n",
        "\n",
        "        # 返回两个标签\n",
        "        label1 = len_i\n",
        "        label2 = sigma_i\n",
        "        return label1, label2\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        label1, label2 = self.labels[idx]\n",
        "        labels = torch.tensor([label1, label2], dtype=torch.float32)  # 将两个标签组合成一个张量\n",
        "        # 打印调试信息\n",
        "        print(f\"Loading data at index {idx}: data shape = {data.shape}, labels = {labels}\")\n",
        "        return data, labels"
      ],
      "metadata": {
        "id": "emmK7HK4_26U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Prepare"
      ],
      "metadata": {
        "id": "zMZMBg2nAu1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Configuration"
      ],
      "metadata": {
        "id": "Bb4oybLgB3X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conf:\n",
        "    '''\n",
        "    Pre-defined parameters for ML tranning\n",
        "    '''\n",
        "    #\n",
        "    path = '/content/Lmp_ML'\n",
        "    path_to_Database = os.path.join(path, 'DataBase')\n",
        "    path_to_GA_data_file = os.path.join(path_to_Database, 'assemblys_pretreated')\n",
        "\n",
        "    # Data parameters\n",
        "    image_folder        = os.path.join(path, 'images')\n",
        "    mask_folder         = os.path.join(path, 'segmaps')\n",
        "    resize              = (512, 512) # it will be very painful if the image height and width are not the same!!!!\n",
        "\n",
        "    # Determine data cleaning directory\n",
        "    patched_image_folder = os.path.join(path, 'images_with_patch')\n",
        "    patched_mask_folder  = os.path.join(path, 'segmaps_with_patch')\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs      = 20\n",
        "    batch_size      = 16\n",
        "    learning_rate   = 2e-4\n",
        "    momentum        = 0.9\n",
        "    weight_decay    = 1.5e-4\n",
        "\n",
        "    # Model parameters\n",
        "    num_classes = 1  # For binary segmentation\n",
        "    kernel_size = 3\n",
        "    stride      = 1\n",
        "    padding     = 1\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "lNDI7V46AsdM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.Data Pre-treatment"
      ],
      "metadata": {
        "id": "TG1qEgNoCEZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 1D performance data"
      ],
      "metadata": {
        "id": "C1IUf2ICCO-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 3D GA structure"
      ],
      "metadata": {
        "id": "VTI9IUqJHY7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.1 Collect data file info"
      ],
      "metadata": {
        "id": "WQM5Qi3BJK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initial a main list to store all info\n",
        "info_list = np.empty((0, 15))\n",
        "\n",
        "# traverse though all data file\n",
        "for len_i in range(2, 21, 1):\n",
        "    for sigma_i in range(8, 19, 1):\n",
        "        for temp_i in range(373, 332, -10):\n",
        "\n",
        "            # read data file info\n",
        "            GP = LmpGP_file_reader(len=len_i, sigma=sigma_i, temp=temp_i, path=Conf.path_to_Database)\n",
        "            GP_atom_count = GP.GP_atom_count\n",
        "            GP_box_bounds_info = GP.GP_box_bounds_info\n",
        "            GP_atom_data = GP.GP_atom_data\n",
        "\n",
        "            # cal. max boundary len\n",
        "            x_length = GP_box_bounds_info['x'][1] - GP_box_bounds_info['x'][0]\n",
        "            y_length = GP_box_bounds_info['y'][1] - GP_box_bounds_info['y'][0]\n",
        "            z_length = GP_box_bounds_info['z'][1] - GP_box_bounds_info['z'][0]\n",
        "            max_BoundLength_i = np.amax([x_length, y_length, z_length])\n",
        "\n",
        "            # cal. min boundary len\n",
        "            min_Bound_i = np.amin([\n",
        "                                    GP_box_bounds_info['x'][0],\n",
        "                                    GP_box_bounds_info['x'][1],\n",
        "                                    GP_box_bounds_info['y'][0],\n",
        "                                    GP_box_bounds_info['y'][1],\n",
        "                                    GP_box_bounds_info['z'][0],\n",
        "                                    GP_box_bounds_info['z'][1]\n",
        "                                ])\n",
        "\n",
        "            # temp list sotring all info\n",
        "            info_i = [int(len_i),                      # 0\n",
        "                      int(sigma_i),                    # 1\n",
        "                      int(temp_i),                     # 2\n",
        "                      GP_atom_count,                   # 3\n",
        "                      GP_box_bounds_info['x'][0],      # 4\n",
        "                      GP_box_bounds_info['x'][1],      # 5\n",
        "                      GP_box_bounds_info['y'][0],      # 6\n",
        "                      GP_box_bounds_info['y'][1],      # 7\n",
        "                      GP_box_bounds_info['z'][0],      # 8\n",
        "                      GP_box_bounds_info['z'][1],      # 9\n",
        "                      min_Bound_i,                     # 10\n",
        "                      x_length,                        # 11\n",
        "                      y_length,                        # 12\n",
        "                      z_length,                        # 13\n",
        "                      max_BoundLength_i                # 14\n",
        "                    ]\n",
        "\n",
        "            info_list = np.vstack((info_list, info_i))"
      ],
      "metadata": {
        "id": "wcYORrTjBO_4"
      },
      "execution_count": 46,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}