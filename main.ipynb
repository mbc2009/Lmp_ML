{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbc2009/Lmp_ML/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.Enviornment Prepare"
      ],
      "metadata": {
        "id": "5LSSxtHN4uqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.0.Download \\& Import"
      ],
      "metadata": {
        "id": "BgcK3K-J_MpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "# install package\n",
        "pip install opencv-python pillow\n",
        "pip install segmentation_models_pytorch\n",
        "pip install kaggle\n",
        "pip install dropbox\n",
        "pip install scikit-image\n",
        "\n",
        "# remove unnecessary\n",
        "cd /content\n",
        "rm -rf *\n",
        "\n",
        "# from Git-Hub reciprotory\n",
        "git clone https://github.com/mbc2009/Lmp_ML\n",
        "rm -rf /content/Lmp_ML/main.ipynb"
      ],
      "metadata": {
        "id": "744mQNJ56I_2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "FZg3wlTp1HaJ"
      },
      "outputs": [],
      "source": [
        "# basic import\n",
        "import  os, sys, time, math, random, math, shutil, pickle\n",
        "import  threading\n",
        "import  concurrent.futures\n",
        "from    datetime                import  datetime\n",
        "from    typing                  import  List, Tuple\n",
        "from    dropbox                 import  Dropbox\n",
        "from    tqdm                    import  tqdm\n",
        "from    matplotlib              import  pyplot      as plt\n",
        "import  numpy                                       as np\n",
        "import  pandas                                      as pd\n",
        "import  zipfile\n",
        "import  warnings\n",
        "import  shutil\n",
        "import  psutil\n",
        "\n",
        "# advance import\n",
        "from    skimage                 import  io\n",
        "from    scipy                   import  interpolate\n",
        "from    scipy.interpolate       import  RegularGridInterpolator\n",
        "from    scipy.ndimage           import  generic_filter\n",
        "from    PIL                     import  Image\n",
        "\n",
        "# torch import\n",
        "import  torch\n",
        "from    torch                   import  nn          as      nn\n",
        "from    torch.nn                import  functional  as F\n",
        "import  torch.optim                                 as optim\n",
        "from    torch.utils.data        import  Dataset, DataLoader, TensorDataset, random_split, Subset\n",
        "from    torchvision             import  transforms, models\n",
        "from    torchvision.transforms  import  *\n",
        "import  torchvision.transforms.functional as TF\n",
        "import  kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看 CPU 核心数\n",
        "cpu_count = os.cpu_count()\n",
        "print(f\"CPU 核心数:\\t{cpu_count}\")\n",
        "\n",
        "# 查看内存信息\n",
        "virtual_memory = psutil.virtual_memory()\n",
        "print(f\"总内存:\\t\\t{virtual_memory.total / (1024**3):.2f} GB\")\n",
        "\n",
        "# 查看 GPU 信息\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"可用 GPU 数量:\\t{gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        gpu_name = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i+1}:\\t\\t{gpu_name}\")\n",
        "else:\n",
        "    print(\"没有可用的 GPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWi3Q0T6PkC",
        "outputId": "9e3d9be5-9a74-4a41-bdf7-a1cbc1c8846d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU 核心数:\t12\n",
            "总内存:\t\t83.48 GB\n",
            "可用 GPU 数量:\t1\n",
            "GPU 1:\t\tNVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1.Functions define"
      ],
      "metadata": {
        "id": "jqRmtqnK-s3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1.1.File Reader"
      ],
      "metadata": {
        "id": "AY_on1y6-4uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch data from EXCEL\n",
        "def fetch(len_i=10,\n",
        "          sigma_i=14,\n",
        "          temp_i=373,\n",
        "          item_i='flux\\n(L/m^2/h)',\n",
        "          original_file_path='../../Database/LmpGP.xlsx'\n",
        "          ):\n",
        "    # 加载Excel文件\n",
        "    df = pd.read_excel(original_file_path)\n",
        "    # 查找行号\n",
        "    row_number = df[\n",
        "        (df['len\\n(A)'] == len_i) &\n",
        "        (df['sigma\\n(A)'] == sigma_i) &\n",
        "        (df['temp\\n(k)'] == temp_i)\n",
        "    ].index\n",
        "    # 读取文件\n",
        "    return df.at[row_number[0], item_i]\n",
        "    # 项目名称对照表\n",
        "    '''\n",
        "        def __init__(self,):\n",
        "            self.label_dict = {\n",
        "                            \"pore_radius\\n(A)\":                                                                                                            (r\"Pore diameter ${D}_{\\mathrm{GA}}$\", r\" $\\mathrm{(nm)}$\"),\n",
        "                            \"pore_radius\\n(A)(smoothed_by_Sigma)\":                                                                                         (r\"Pore diameter ${D}_{\\mathrm{GA}}$\", r\" $\\mathrm{(nm)}$\"),\n",
        "                            \"thickness_of_box\\n(A)\":                                                                                                       (r\"GP thickness $t_{\\mathrm{GA}}$\", r\" $\\mathrm{ (\\AA)}$\"),\n",
        "                            \"bond_density\\n(unitless)\":                                                                                                    (r\"$C-C$ bond density ${\\rho}_{C-C}$\", r\" (unitless)\"),\n",
        "                            \"specific_surface_area\\n(m^2/g)\":                                                                                              (r\"Specific surface area $S_{\\mathrm{GA}}$\", r\" $\\mathrm{(m^{2}/g)}$\"),\n",
        "                            \"specific_surface_area\\n(m^2/g)(smoothed_by_Pore radius)\":                                                                     (r\"Specific surface area $S_{\\mathrm{GA}}$\", r\" $\\mathrm{(m^{2}/g)}$\"),\n",
        "                            \"surface_area\\n(A^2)\":                                                                                                         (r\"Surface area $A_{\\mathrm{GA}}$\", r\" $\\mathrm{({\\AA}^{2})}$\"),\n",
        "                            \"porosity\\n(unitless)\":                                                                                                        (r\"Porosity $\\phi_{\\mathrm{GA}}$\", r\" ($\\%$)\"),\n",
        "                            \"porosity\\n(unitless)(smoothed_by_Pore radius)\":                                                                               (r\"Porosity $\\phi_{\\mathrm{GA}}$\", r\" ($\\%$)\"),\n",
        "                            \"density\\n(g/cm^3)\":                                                                                                           (r\"Density ${\\rho}_{\\mathrm{GA}}$\",r\" $\\mathrm{ (mg/{cm}^{3})}$\"),\n",
        "                            'sigma\\n(A)':                                                                                                                  (r\"Sigma $\\sigma$\",r\" $\\mathrm{ (\\AA)}$\"),\n",
        "                            'Diffusivity_EA_filtrates\\n_in_membrane\\n(m^2/s)':                                                                             (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)':                                                                             (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)':                                                                   (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)(smoothed_by_Surface area)':                                         (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_TA_filtrates\\n_in_membrane\\n(m^2/s)(averaged)(smoothed_by_Surface area)(smoothed_by_Pore radius)':                (r\"Diffusivity $\\mathcal{D}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_Knudsen_PoreRadius\\n(m^2/s)':                                                                                     (r\"Knudsen diffusivity $\\mathcalD}_{Kn}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'Diffusivity_Knudsen_PoreRadius\\n(m^2/s)Diffusivity_Knudsen_PoreRadius\\n(m^2/s)(smoothed_by_Pore radius)':                     (r\"Knudsen diffusivity $\\mathcalD}_{Kn}$\",r\" $\\mathrm{ (m^{2}/s)}$\"),\n",
        "                            'flux\\n(L/m^2/h)':                                                                                                             (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Sigma)':                                                                                          (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)':                                                                                    (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{(L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)(smoothed_by_Tortuosity)':                                                            (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{ (L/m^{2}/h)}$\"),\n",
        "                            'flux\\n(L/m^2/h)(smoothed_by_Pore radius)(smoothed_by_Tortuosity)(smoothed_by_Porosity)':                                      (r\"Flux $\\mathcal{J}$\",r\" $\\mathrm{ (L/m^{2}/h)}$\"),\n",
        "                            'tortuosity\\n(unitless)':                                                                                                      (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'tortuosity\\n(unitless)(smoothed_by_Pore radius)':                                                                             (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'tortuosity\\n(unitless)(smoothed_by_Surface area)':                                                                            (r\"Tortuosity ${\\tau}_{\\mathrm{GA}}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_cutoff':                                                                                                                 (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_1/2_peak':                                                                                                               (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            'CN\\n_cutoff(smoothed_by_Pore radius)':                                                                                        (r\"$\\#$ of $C$ atoms nearby $n_{C}$\", r\" (unitless)\"),\n",
        "                            }\n",
        "            self.color_lsit()\n",
        "    '''\n",
        "\n",
        "\n",
        "# read lammps data file\n",
        "class LmpGP_file_reader():\n",
        "    def __init__(self, len: int, sigma: int, temp: int, path=\"../../Database\"):\n",
        "        ## terminology of parameters\n",
        "        self.sigma          = sigma\n",
        "        self.len            = len\n",
        "        self.temp           = temp\n",
        "        ## working path\n",
        "        self.path           =  path\n",
        "        self.As_path        =  f\"{self.path}/assemblys_pretreated\"\n",
        "        self.Di_path        =  f\"{self.path}/assemblys_distillated/len_{self.len}\"\n",
        "        self.folder         =  f\"len_{self.len}_sigma_{self.sigma}_{self.temp}\"\n",
        "        self.dump_file      =  f\"dump_di\"\n",
        "        self.data_file      =  f\"data.3_len_{self.len}_sigma_{self.sigma}_pretreated\"\n",
        "        self.temp_dump_file =  f\"./merged_dump_file.txt\"\n",
        "        ## auto excecuting\n",
        "        #self.process_AGMD_dump_file()\n",
        "        self.read_GP_data_file()\n",
        "    def read_GP_data_file(self):\n",
        "        ### initialize\n",
        "        self.GP_atom_count, self.GP_box_bounds_info, self.GP_atom_data = None, {}, []\n",
        "        file_path = os.path.join(f\"{self.As_path}/{self.data_file}\")\n",
        "        atom_section = False\n",
        "\n",
        "        ### extract info\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "\n",
        "                ## Get the number of atoms\n",
        "                if line.strip().endswith(\"atoms\"):\n",
        "                    self.GP_atom_count = int(line.split()[0])\n",
        "\n",
        "                ## Get the box info\n",
        "                # boudnds\n",
        "                if 'xlo xhi' in line:\n",
        "                    self.GP_box_bounds_info['x'] = list(map(float, line.strip().split()[:2]))\n",
        "                if 'ylo yhi' in line:\n",
        "                    self.GP_box_bounds_info['y'] = list(map(float, line.strip().split()[:2]))\n",
        "                if 'zlo zhi' in line:\n",
        "                    self.GP_box_bounds_info['z'] = list(map(float, line.strip().split()[:2]))\n",
        "\n",
        "                ## Read atom data\n",
        "                # Check title\n",
        "                if 'Atoms' in line and '#' in line:\n",
        "                    atom_section = True\n",
        "                    continue\n",
        "                # read\n",
        "                if atom_section and line.strip():\n",
        "                    parts = line.split()\n",
        "                    if len(parts) >= 6:  # valid line Ensured\n",
        "                        atom_id, atom_type, x, y, z = int(parts[0]), int(parts[1]), float(parts[4]), float(parts[5]), float(parts[6])\n",
        "                        self.GP_atom_data.append([atom_id, atom_type, x, y, z])\n",
        "\n",
        "            # calculate side length\n",
        "            x_length = float(self.GP_box_bounds_info['x'][1]) - float(self.GP_box_bounds_info['x'][0])\n",
        "            y_length = float(self.GP_box_bounds_info['y'][1]) - float(self.GP_box_bounds_info['y'][0])\n",
        "            z_length = float(self.GP_box_bounds_info['z'][1]) - float(self.GP_box_bounds_info['z'][0])\n",
        "            self.GP_box_lengths_info = np.array([x_length, y_length, z_length])\n",
        "\n",
        "        ### post-treatment\n",
        "        ## convert\n",
        "        self.GP_atom_data = np.array(self.GP_atom_data)\n",
        "        ## sorting\n",
        "        # 首先按照 type 排序，然后在 type 相同的情况下按 ID 排序\n",
        "        if hasattr(self, 'GP_atom_data') and self.GP_atom_data.size > 0:\n",
        "            # 获取排序后的索引\n",
        "            sorted_indices = np.lexsort((self.GP_atom_data[:, 0], self.GP_atom_data[:, 1]))  # 先按 ID，再按 type 排序\n",
        "            # 应用排序\n",
        "            self.GP_atom_data = self.GP_atom_data[sorted_indices]\n",
        "    def process_AGMD_dump_file(self):\n",
        "        ## Find dump file\n",
        "        path = f\"{self.Di_path}/{self.folder}\"\n",
        "        # list all files\n",
        "        file_list = os.listdir(path)\n",
        "        # open folders\n",
        "        file_list = [f for f in file_list if os.path.isfile(os.path.join(path, f))]\n",
        "        # select dump file only\n",
        "        file_list = [f for f in file_list if self.dump_file in f]\n",
        "        # sort by time label\n",
        "        file_list = [f.replace(f\"{self.dump_file}_\", \"\") for f in file_list]\n",
        "        file_list.sort()\n",
        "        file_list = [f\"{path}/{self.dump_file}_{f}\" for f in file_list]\n",
        "\n",
        "        ## Combine dump files into one\n",
        "        output_file_path = os.path.join(\"./merged_dump_file.txt\")\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            for file_name in file_list:\n",
        "                with open(os.path.join(file_name), 'r') as input_file:\n",
        "                    output_file.write(input_file.read())\n",
        "\n",
        "        ### Collect basic info\n",
        "        ## Comput TIMESTEP positions list\n",
        "        with open(self.temp_dump_file, 'r') as file:\n",
        "            for line in file:\n",
        "                ## box info\n",
        "                if 'ITEM: BOX BOUNDS' in line:\n",
        "                    self.box_bounds_info = {}\n",
        "                    self.box_bounds_info[\"x\"] = list(map(float, next(file).strip().split()))\n",
        "                    self.box_bounds_info[\"y\"] = list(map(float, next(file).strip().split()))\n",
        "                    self.box_bounds_info[\"z\"] = list(map(float, next(file).strip().split()))\n",
        "                    x_length = float(self.box_bounds_info['x'][1]) - float(self.box_bounds_info['x'][0])\n",
        "                    y_length = float(self.box_bounds_info['y'][1]) - float(self.box_bounds_info['y'][0])\n",
        "                    z_length = float(self.box_bounds_info['z'][1]) - float(self.box_bounds_info['z'][0])\n",
        "                    self.box_lengths_info = np.array([x_length, y_length, z_length])\n",
        "                    break\n",
        "        ## build timesteps list\n",
        "        self.timestep_positions, self.timestep_list = {}, []\n",
        "        timestep_checker = 200000\n",
        "        with open(self.temp_dump_file, 'r') as file:\n",
        "            line_index = -1\n",
        "            while True:\n",
        "                line = file.readline()\n",
        "                line_index += 1\n",
        "                if not line:\n",
        "                    break  # 到达文件末尾\n",
        "                if 'ITEM: TIMESTEP' in line:\n",
        "                    # 读取时间步的值\n",
        "                    timestep = int(file.readline().strip())\n",
        "                    line_index += 1\n",
        "                    if timestep != timestep_checker:\n",
        "                        # 记录当前时间步的起始行号\n",
        "                        self.timestep_positions[timestep] = line_index\n",
        "                        # 将步数记入列表\n",
        "                        self.timestep_list.append(timestep)\n",
        "                        timestep_checker = timestep\n",
        "    def read_single_timestep_for__AGMD_dump_file(self, timestep):\n",
        "        #### 打开文件并开始读取\n",
        "        file_path = os.path.join(self.temp_dump_file)\n",
        "        with open(file_path, 'r') as file:\n",
        "            ### Jump to timestep\n",
        "            file.seek(0)  # 开始时先回到文件开头\n",
        "            for _ in range(self.timestep_positions[timestep]):  # 跳过line_number - 1行\n",
        "                file.readline()\n",
        "            ## Current number of atoms\n",
        "            while True:\n",
        "                line = next(file)\n",
        "                if 'ITEM: NUMBER OF ATOMS' in line:\n",
        "                    self.atom_count = int(next(file).strip())\n",
        "                    break\n",
        "            ## Skip\n",
        "            while True:\n",
        "                line = next(file)\n",
        "                if 'ITEM: ATOMS' in line:\n",
        "                    break\n",
        "\n",
        "            ### Extract atoms info\n",
        "            atom_data = []\n",
        "            for _ in range(self.atom_count):\n",
        "                # 读取并分割行\n",
        "                atom_info_str = next(file).strip().split()\n",
        "                # 分别处理不同类型的数据\n",
        "                # ITEM: ATOMS id type xs ys zs\n",
        "                atom_info = [int(atom_info_str[0]), int(atom_info_str[1])] + [float(x) for x in atom_info_str[2:]]\n",
        "                atom_data.append(atom_info)\n",
        "\n",
        "        ### Post treatment\n",
        "        # convert to NumPy matrix\n",
        "        self.atom_data = np.array(atom_data)\n",
        "        ## de-normalize\n",
        "        if hasattr(self, 'box_lengths_info') and self.atom_data.size > 0:\n",
        "            for i in range(self.atom_data.shape[0]):\n",
        "                # xs\n",
        "                self.atom_data[i, 2] = self.atom_data[i, 2] * self.box_lengths_info[0] + float(self.box_bounds_info[\"x\"][0])\n",
        "                # ys\n",
        "                self.atom_data[i, 3] = self.atom_data[i, 3] * self.box_lengths_info[1] + float(self.box_bounds_info[\"y\"][0])\n",
        "                # zs\n",
        "                self.atom_data[i, 4] = self.atom_data[i, 4] * self.box_lengths_info[2] + float(self.box_bounds_info[\"z\"][0])\n",
        "        ## sorting\n",
        "        # 首先按照 type 排序，然后在 type 相同的情况下按 ID 排序\n",
        "        if hasattr(self, 'atom_data') and self.atom_data.size > 0:\n",
        "            # 获取排序后的索引\n",
        "            sorted_indices = np.lexsort((self.atom_data[:, 0], self.atom_data[:, 1]))  # 先按 ID，再按 type 排序\n",
        "            # 应用排序\n",
        "            self.atom_data = self.atom_data[sorted_indices]\n",
        "    def build_dump_atoms_info_list__for__AGMD_dump_file(self):\n",
        "        # 初始化一个列表来存储每个时间步的原子信息矩阵\n",
        "        self.atom_data_list = []\n",
        "        # 遍历每个时间步\n",
        "        for timestep in self.timestep_positions.keys():\n",
        "            # 读取该时间步的原子信息\n",
        "            self.read_single_timestep_for__AGMD_dump_file(timestep)\n",
        "            # 将原子信息矩阵添加到列表中\n",
        "            self.atom_data_list.append(self.atom_data)\n",
        "            print(timestep)\n",
        "    #def check_atom_data_list(self):\n",
        "        #self.timestep_list\n",
        "        #self.box_bounds_info\n",
        "    def check_atom_data_list(self, output_filename=\"check_atom_data_list.txt\"):\n",
        "        \"\"\"\n",
        "        将atom_data_list的内容输出为LAMMPS dump文件格式\n",
        "        :param output_filename: 输出文件名\n",
        "        \"\"\"\n",
        "        with open(output_filename, 'w') as dump_file:\n",
        "            for i, atom_data in enumerate(self.atom_data_list):\n",
        "                # 写入时间步\n",
        "                dump_file.write(\"ITEM: TIMESTEP\\n\")\n",
        "                dump_file.write(f\"{self.timestep_list[i]}\\n\")\n",
        "                # 写入原子数量\n",
        "                dump_file.write(\"ITEM: NUMBER OF ATOMS\\n\")\n",
        "                dump_file.write(f\"{len(atom_data)}\\n\")\n",
        "                # 写入盒子边界\n",
        "                dump_file.write(\"ITEM: BOX BOUNDS pp pp ff\\n\")\n",
        "                for bound in ['x', 'y', 'z']:\n",
        "                    dump_file.write(f\"{self.box_bounds_info[bound][0]} {self.box_bounds_info[bound][1]}\\n\")\n",
        "                # 写入原子信息\n",
        "                dump_file.write(\"ITEM: ATOMS id type x y z\\n\")\n",
        "                for atom in atom_data:\n",
        "                    dump_file.write(\" \".join(map(str, atom)) + \"\\n\")\n",
        "\n",
        "\n",
        "# extract info from lammps data file\n",
        "def extract_info(len_i:int,sigma_i:int,temp_i:int)->list:\n",
        "    # 读取数据文件信息\n",
        "    GP                  = LmpGP_file_reader(len=len_i, sigma=sigma_i, temp=temp_i, path=Conf.path_to_Database)\n",
        "    GP_atom_count       = GP.GP_atom_count\n",
        "    GP_box_bounds_info  = GP.GP_box_bounds_info\n",
        "\n",
        "    # 计算最大边界长度\n",
        "    x_length          = GP_box_bounds_info['x'][1] - GP_box_bounds_info['x'][0]\n",
        "    y_length          = GP_box_bounds_info['y'][1] - GP_box_bounds_info['y'][0]\n",
        "    z_length          = GP_box_bounds_info['z'][1] - GP_box_bounds_info['z'][0]\n",
        "    max_BoundLength_i = np.amax([x_length, y_length, z_length])\n",
        "\n",
        "    # 计算最小边界长度\n",
        "    min_Bound_i  = np.amin([GP_box_bounds_info['x'][0],\n",
        "                            GP_box_bounds_info['x'][1],\n",
        "                            GP_box_bounds_info['y'][0],\n",
        "                            GP_box_bounds_info['y'][1],\n",
        "                            GP_box_bounds_info['z'][0],\n",
        "                            GP_box_bounds_info['z'][1]\n",
        "                            ])\n",
        "\n",
        "    # 存储信息\n",
        "    return [int(len_i),                   # 0\n",
        "            int(sigma_i),                 # 1\n",
        "            int(temp_i),                  # 2\n",
        "            GP_atom_count,                # 3\n",
        "            GP_box_bounds_info['x'][0],   # 4\n",
        "            GP_box_bounds_info['x'][1],   # 5\n",
        "            GP_box_bounds_info['y'][0],   # 6\n",
        "            GP_box_bounds_info['y'][1],   # 7\n",
        "            GP_box_bounds_info['z'][0],   # 8\n",
        "            GP_box_bounds_info['z'][1],   # 9\n",
        "            min_Bound_i,                  # 10\n",
        "            x_length,                     # 11\n",
        "            y_length,                     # 12\n",
        "            z_length,                     # 13\n",
        "            max_BoundLength_i]            # 14\n",
        "\n",
        "\n",
        "# 找到拥有最大/最小 max_BoundLength_i 的那一行\n",
        "def find_info(mode: str, info_matrix:np.ndarray):\n",
        "  row             = []\n",
        "  bounds_info_i   = None\n",
        "  direction       = None\n",
        "\n",
        "  if mode == 'min':\n",
        "    row = info_matrix[np.argmin(info_matrix[:, 14])]\n",
        "  if mode == 'max':\n",
        "    row = info_matrix[np.argmax(info_matrix[:, 14])]\n",
        "  else:\n",
        "    print('select mode')\n",
        "\n",
        "  # 判断最{mode}的 'max_BoundLength_i' 所在方向\n",
        "  if row[14] == abs(row[5]-row[4]):\n",
        "      direction = 'x'\n",
        "      bounds_info_i = (row[4], row[5])\n",
        "  elif row[14] == abs(row[7]-row[6]):\n",
        "      direction = 'y'\n",
        "      bounds_info_i = (row[6], row[7])\n",
        "  elif row[14] == abs(row[9]-row[8]):\n",
        "      direction = 'z'\n",
        "      bounds_info_i = (row[8], row[9])\n",
        "\n",
        "  print(f\"拥有最{mode} 'x/y/z 最大边长' 的那一行是: {row}\")\n",
        "  print(f'起止坐标； {bounds_info_i}')\n",
        "  print(f\"最{mode}的方向是: {direction}，长度是: {row[14]}\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OinYQCkb46xc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1.2.Data Pretreeatment"
      ],
      "metadata": {
        "id": "EgpnnqGj_ctr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Datafile_to_grids():\n",
        "    def __init__(self,\n",
        "                 GP_data,      # data file 原始数据\n",
        "                 grid_size,    # 网格划分的尺寸\n",
        "                 limits,       # 实际的 x/y/z 边界上下限\n",
        "                 limits_new,  # 需要补全到的 x/y/z 边界上下限\n",
        "                 output_filename = \"CHECK_DataFile\"\n",
        "                 ):\n",
        "        self.output_filename = output_filename\n",
        "        self.GP_data = GP_data\n",
        "        self.grid_size = grid_size\n",
        "        self.xmin, self.xmax = limits[\"x\"]\n",
        "        self.ymin, self.ymax = limits[\"y\"]\n",
        "        self.zmin, self.zmax = limits[\"z\"]\n",
        "        self.xmin_new, self.xmax_new = limits_new[\"x\"]\n",
        "        self.ymin_new, self.ymax_new = limits_new[\"y\"]\n",
        "        self.zmin_new, self.zmax_new = limits_new[\"z\"]\n",
        "\n",
        "        # 计算原始边界和新边界的长度\n",
        "        self.xlength = self.xmax - self.xmin\n",
        "        self.ylength = self.ymax - self.ymin\n",
        "        self.zlength = self.zmax - self.zmin\n",
        "        self.xlength_new = self.xmax_new - self.xmin_new\n",
        "        self.ylength_new = self.ymax_new - self.ymin_new\n",
        "        self.zlength_new = self.zmax_new - self.zmin_new\n",
        "\n",
        "        # 自动执行\n",
        "        self.GP_data_pretreat()\n",
        "        self.GP_data_analysis()\n",
        "        self.GP_data_aligning()\n",
        "        self.GP_data_to_grid()\n",
        "        self.check_atom_data_list()\n",
        "\n",
        "    def GP_data_pretreat(self):\n",
        "        self.GP_data = np.delete(self.GP_data, 1, axis=1)  # 删除 atom type 列\n",
        "\n",
        "    def GP_data_analysis(self):\n",
        "        self.GP_atom_count = self.GP_data.shape[0]\n",
        "\n",
        "    def GP_data_aligning(self):\n",
        "        '''\n",
        "        对data file 进行补全\n",
        "        '''\n",
        "        # 计算需要复制的倍数，包括正方向和负方向\n",
        "        x_repeats_pos = int(np.ceil((self.xmax_new - self.xmax) / self.xlength))\n",
        "        x_repeats_neg = int(np.ceil((self.xmin - self.xmin_new) / self.xlength))\n",
        "        y_repeats_pos = int(np.ceil((self.ymax_new - self.ymax) / self.ylength))\n",
        "        y_repeats_neg = int(np.ceil((self.ymin - self.ymin_new) / self.ylength))\n",
        "        z_repeats_pos = int(np.ceil((self.zmax_new - self.zmax) / self.zlength))\n",
        "        z_repeats_neg = int(np.ceil((self.zmin - self.zmin_new) / self.zlength))\n",
        "\n",
        "        # 扩展原子坐标，包括正方向和负方向\n",
        "        extended_atoms = []\n",
        "        new_atom_id = 1\n",
        "        for i in range(-x_repeats_neg, x_repeats_pos + 1):\n",
        "            for j in range(-y_repeats_neg, y_repeats_pos + 1):\n",
        "                for k in range(-z_repeats_neg, z_repeats_pos + 1):\n",
        "                    for atom in self.GP_data:\n",
        "                        new_atom = atom.copy()\n",
        "                        new_atom[0] = new_atom_id  # 更新原子ID\n",
        "                        new_atom[1] = atom[1] + i * self.xlength\n",
        "                        new_atom[2] = atom[2] + j * self.ylength\n",
        "                        new_atom[3] = atom[3] + k * self.zlength\n",
        "                        extended_atoms.append(new_atom)\n",
        "                        new_atom_id += 1\n",
        "\n",
        "        # 转换为numpy数组\n",
        "        extended_atoms = np.array(extended_atoms)\n",
        "\n",
        "        # 过滤掉超出新边界的原子\n",
        "        extended_atoms = extended_atoms[\n",
        "            (extended_atoms[:, 1] >= self.xmin_new) & (extended_atoms[:, 1] < self.xmax_new) &\n",
        "            (extended_atoms[:, 2] >= self.ymin_new) & (extended_atoms[:, 2] < self.ymax_new) &\n",
        "            (extended_atoms[:, 3] >= self.zmin_new) & (extended_atoms[:, 3] < self.zmax_new)\n",
        "        ]\n",
        "\n",
        "        # 更新GP_data，并重新分配ID以确保连续\n",
        "        self.GP_data = extended_atoms\n",
        "        self.GP_data[:, 0] = np.arange(1, self.GP_data.shape[0] + 1)\n",
        "        self.GP_atom_count_extended = self.GP_data.shape[0]\n",
        "\n",
        "    def GP_data_to_grid(self):\n",
        "        '''\n",
        "        将原子数据转换为网格数据\n",
        "        '''\n",
        "        # N 格子, N+1 个点\n",
        "        x_bins = np.linspace(self.xmin_new, self.xmax_new, self.grid_size[0] + 1)\n",
        "        y_bins = np.linspace(self.ymin_new, self.ymax_new, self.grid_size[1] + 1)\n",
        "        z_bins = np.linspace(self.zmin_new, self.zmax_new, self.grid_size[2] + 1)\n",
        "\n",
        "        # 生成 X * Y * Z 形状的矩阵 (grid_size=(XY,Z))\n",
        "        grid_counts = np.zeros(self.grid_size)\n",
        "\n",
        "        for atom in self.GP_data:\n",
        "            x_idx = np.digitize(atom[1], x_bins) - 1    # digitize() 从1开始\n",
        "            y_idx = np.digitize(atom[2], y_bins) - 1\n",
        "            z_idx = np.digitize(atom[3], z_bins) - 1\n",
        "            if x_idx < self.grid_size[0] and y_idx < self.grid_size[1] and z_idx < self.grid_size[2]:\n",
        "                grid_counts[x_idx, y_idx, z_idx] += 1\n",
        "\n",
        "        self.grid_counts = grid_counts\n",
        "\n",
        "        # 打印结果以检查\n",
        "        #print(f\"Grid counts shape: {self.grid_counts.shape}\")\n",
        "        #print(self.grid_counts)\n",
        "\n",
        "    def check_atom_data_list(self):\n",
        "        \"\"\"\n",
        "        将atom_data_list的内容输出为LAMMPS dump文件格式\n",
        "        :param output_filename: 输出文件名\n",
        "        \"\"\"\n",
        "\n",
        "        with open(self.output_filename, 'w') as data_file:\n",
        "            # 写入基本信息\n",
        "            data_file.write(\"LAMMPS data file via write_data, version 2 Aug 2023, timestep = 50000, units = metal\\n\\n\")\n",
        "            data_file.write(f\"{self.GP_atom_count_extended} atoms\\n\")\n",
        "            data_file.write(\"1 atom types\\n\\n\")\n",
        "            data_file.write(f\"{self.xmin_new} {self.xmax_new} xlo xhi\\n\")\n",
        "            data_file.write(f\"{self.ymin_new} {self.ymax_new} ylo yhi\\n\")\n",
        "            data_file.write(f\"{self.zmin_new} {self.zmax_new} zlo zhi\\n\\n\")\n",
        "            data_file.write(\"Masses\\n\\n\")\n",
        "            data_file.write(\"1 12.0107\\n\\n\")\n",
        "            data_file.write(\"Atoms # full\\n\\n\")\n",
        "            for atom in self.GP_data:\n",
        "                atom_inserted = np.insert(atom, 1, 1)                            # 在第二项位置插入 atom type   为 1\n",
        "                atom_inserted = np.insert(atom_inserted, 1, 1)                   # 在第二项位置插入 molecule ID 为 1\n",
        "                atom_inserted = np.insert(atom_inserted, 3, 0)                   # 在第二项位置插入 charge      为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity X  为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity Y  为 0\n",
        "                atom_inserted = np.insert(atom_inserted, len(atom_inserted), 0)  # 在第二项位置插入 velocity Z  为 0\n",
        "\n",
        "                # 将 atom ID 和 molecule ID 转换为整数, 其他保持原有精度\n",
        "                formatted_line = f\"{int(atom_inserted[0])} {int(atom_inserted[1])} {int(atom_inserted[2])} {int(atom_inserted[3])} \" \\\n",
        "                                 f\"{atom_inserted[4]} {atom_inserted[5]} {atom_inserted[6]} \" \\\n",
        "                                 f\"{int(atom_inserted[7])} {int(atom_inserted[8])} {int(atom_inserted[9])}\"\n",
        "\n",
        "                data_file.write(formatted_line + \"\\n\")\n",
        "\n",
        "\n",
        "class GridDataset(Dataset):\n",
        "    def __init__(self, grid_counts_dict):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, grid in grid_counts_dict.items():\n",
        "            self.data.append(grid)\n",
        "            self.labels.append(self.extract_label_from_key(label))\n",
        "\n",
        "        # 转换为 torch.Tensor\n",
        "        self.data   = torch.tensor(self.data, dtype=torch.float32).unsqueeze(1)  # 添加 channel 维度\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.float32)\n",
        "\n",
        "    def extract_label_from_key(self, key):\n",
        "        # 解析出 len_i, sigma_i, temp_i\n",
        "        parts = key.split('_')\n",
        "        len_i = int(parts[1])\n",
        "        sigma_i = int(parts[3])\n",
        "        temp_i = int(parts[-1])\n",
        "\n",
        "        # 返回两个标签\n",
        "        label1 = len_i\n",
        "        label2 = sigma_i\n",
        "        return label1, label2\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        label1, label2 = self.labels[idx]\n",
        "        labels = torch.tensor([label1, label2], dtype=torch.float32)  # 将两个标签组合成一个张量\n",
        "        # 打印调试信息\n",
        "        print(f\"Loading data at index {idx}: data shape = {data.shape}, labels = {labels}\")\n",
        "        return data, labels"
      ],
      "metadata": {
        "id": "emmK7HK4_26U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Prepare"
      ],
      "metadata": {
        "id": "zMZMBg2nAu1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Configuration"
      ],
      "metadata": {
        "id": "Bb4oybLgB3X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conf:\n",
        "    '''\n",
        "    Pre-defined parameters for ML tranning\n",
        "    '''\n",
        "    # Paths\n",
        "    path = '/content/Lmp_ML'\n",
        "    path_to_Database          = os.path.join(path, 'DataBase')\n",
        "    path_to_GA_data_file      = os.path.join(path_to_Database, 'assemblys_pretreated')\n",
        "    path_to_OUTPUTS           = os.path.join(path, 'OUTPUTS')\n",
        "    path_to_aligned_DataFile  = os.path.join(path_to_OUTPUTS, 'aligned_DataFile')\n",
        "\n",
        "    # Supercelling: new boundary in X/Y/Z directions\n",
        "    GP_box_bounds_info_new = {'x': [600, 800],\n",
        "                              'y': [600, 800],\n",
        "                              'z': [600, 800]}\n",
        "\n",
        "    # 3D grid: number of grides in X/Y/Z directions\n",
        "    grid_size = [200, 200, 200]\n",
        "\n",
        "    # Data\n",
        "    image_folder        = os.path.join(path, 'images')\n",
        "    mask_folder         = os.path.join(path, 'segmaps')\n",
        "    resize              = (512, 512) # it will be very painful if the image height and width are not the same!!!!\n",
        "\n",
        "    # Determine data cleaning directory\n",
        "    patched_image_folder = os.path.join(path, 'images_with_patch')\n",
        "    patched_mask_folder  = os.path.join(path, 'segmaps_with_patch')\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs      = 20\n",
        "    batch_size      = 16\n",
        "    learning_rate   = 2e-4\n",
        "    momentum        = 0.9\n",
        "    weight_decay    = 1.5e-4\n",
        "\n",
        "    # Model parameters\n",
        "    num_classes = 1  # For binary segmentation\n",
        "    kernel_size = 3\n",
        "    stride      = 1\n",
        "    padding     = 1\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "lNDI7V46AsdM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create folders for OUTPUT\n",
        "os.makedirs(Conf.path_to_OUTPUTS,          exist_ok=True)\n",
        "os.makedirs(Conf.path_to_aligned_DataFile, exist_ok=True)"
      ],
      "metadata": {
        "id": "arvNBawbcgho"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.Data Pre-treatment"
      ],
      "metadata": {
        "id": "TG1qEgNoCEZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1.1D performance data"
      ],
      "metadata": {
        "id": "C1IUf2ICCO-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2.3D GA structure"
      ],
      "metadata": {
        "id": "VTI9IUqJHY7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Collect data files info ##\n",
        "\n",
        "# 初始化一个主列表用于存储所有信息\n",
        "info_mtx = np.empty((0, 15))\n",
        "\n",
        "# 创建一个线程池\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # 生成所有任务\n",
        "    future_to_params = {executor.submit(extract_info, len_i, sigma_i, temp_i):\n",
        "                        (len_i, sigma_i, temp_i)\n",
        "                        for len_i in range(2, 21, 1)\n",
        "                        for sigma_i in range(8, 19, 1)\n",
        "                        for temp_i in range(373, 333, -10)}\n",
        "\n",
        "    # 收集所有返回的数据\n",
        "    results = [future.result() for future in concurrent.futures.as_completed(future_to_params)]\n",
        "\n",
        "# 转换为 NumPy 数组\n",
        "info_mtx = np.array(results)\n",
        "\n",
        "# 打印结果以检查\n",
        "print(f'info list shape:\\t{info_mtx.shape}')\n",
        "print(f'reference length:\\t{(18-8+1)*(20-2+1)*4},{15}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcYORrTjBO_4",
        "outputId": "80f46063-6b3e-4b5c-fe55-29aff9cae31a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info list shape:\t(836, 15)\n",
            "reference length:\t836,15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 寻找最大和最短的 box 长度 ##\n",
        "find_info('min', info_mtx)\n",
        "find_info('max', info_mtx)"
      ],
      "metadata": {
        "id": "liHhNvLECFt8",
        "outputId": "591aa4b3-6da4-4c9f-9c23-0d7302885272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select mode\n",
            "拥有最min 'x/y/z 最大边长' 的那一行是: [3.00000000e+00 8.00000000e+00 3.53000000e+02 3.83600000e+03\n",
            " 6.51675695e+02 7.08250305e+02 6.52167825e+02 7.08467175e+02\n",
            " 6.51941735e+02 7.08693265e+02 6.51675695e+02 5.65746094e+01\n",
            " 5.62993504e+01 5.67515307e+01 5.67515307e+01]\n",
            "起止坐标； (651.9417346706942, 708.6932653293076)\n",
            "最min的方向是: z，长度是: 56.75153065861332\n",
            "\n",
            "\n",
            "拥有最max 'x/y/z 最大边长' 的那一行是: [4.00000000e+00 1.80000000e+01 3.53000000e+02 4.27600000e+03\n",
            " 6.00449820e+02 7.59476180e+02 6.00133917e+02 7.60501083e+02\n",
            " 6.00572822e+02 7.60062178e+02 6.00133917e+02 1.59026359e+02\n",
            " 1.60367165e+02 1.59489356e+02 1.60367165e+02]\n",
            "起止坐标； (600.1339173401502, 760.5010826598495)\n",
            "最max的方向是: y，长度是: 160.36716531969932\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 存储所有 GP.grid_counts 矩阵的字典\n",
        "grid_counts_dict = {}\n",
        "\n",
        "# 设定温度列表\n",
        "temp_list = [373, 363, 353, 343]\n",
        "\n",
        "# 处理单个任务的函数\n",
        "def process_grid(len_i:int, sigma_i:int):\n",
        "    \"\"\" 处理单个 len_i 和 sigma_i 组合的数据，返回 grid_counts_dict 需要的键值对 \"\"\"\n",
        "\n",
        "    # 读取第一个温度的数据\n",
        "    GP = LmpGP_file_reader(len=len_i, sigma=sigma_i, temp=temp_list[0], path=Conf.path_to_Database)\n",
        "    GP_atom_count = GP.GP_atom_count\n",
        "    GP_box_bounds_info = GP.GP_box_bounds_info\n",
        "    GP_atom_data = GP.GP_atom_data\n",
        "\n",
        "    # Supercelling 处理\n",
        "    GP = Datafile_to_grids(GP_data          = GP_atom_data,                 # 原始的 data file info\n",
        "                           grid_size        = Conf.grid_size,               # 网格划分的尺寸\n",
        "                           limits           = GP_box_bounds_info,           # 实际的 x/y/z 边界上下限\n",
        "                           limits_new       = Conf.GP_box_bounds_info_new,  # 需要补全到的 x/y/z 边界上下限\n",
        "                           output_filename  = os.path.join(Conf.path_to_aligned_DataFile,\n",
        "                                                           f'CHECK_data_4_len_{len_i}_sigma_{sigma_i}_{temp_list[0]}.txt')\n",
        "                           )\n",
        "\n",
        "    # 生成 3D grid\n",
        "    GP_grid = GP.grid_counts\n",
        "\n",
        "    # 存储不同温度的结果\n",
        "    grid_counts_local = {}\n",
        "\n",
        "    for temp_i in temp_list:\n",
        "        if temp_i != temp_list[0]:  # 复制文件\n",
        "            src_file = os.path.join(\n",
        "                Conf.path_to_aligned_DataFile, f\"CHECK_data_4_len_{len_i}_sigma_{sigma_i}_{temp_list[0]}.txt\"\n",
        "            )\n",
        "            dst_file = os.path.join(\n",
        "                Conf.path_to_aligned_DataFile, f\"CHECK_data_4_len_{len_i}_sigma_{sigma_i}_{temp_i}.txt\"\n",
        "            )\n",
        "            shutil.copy(src_file, dst_file)\n",
        "\n",
        "        # 生成 grid_counts 字典的 key\n",
        "        grid_name_i                    = f\"len_{len_i}_sigma_{sigma_i}_{temp_i}\"\n",
        "        grid_counts_local[grid_name_i] = GP_grid   # 存储 3D grid 矩阵\n",
        "\n",
        "    return grid_counts_local\n",
        "\n",
        "\n",
        "# 多线程处理\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # 生成所有任务\n",
        "    future_to_params = {\n",
        "        executor.submit(process_grid, len_i, sigma_i):\n",
        "        (len_i, sigma_i)\n",
        "        for len_i in range(2, 21, 1)\n",
        "        for sigma_i in range(8, 19, 1)\n",
        "    }\n",
        "\n",
        "    # 收集所有返回的结果\n",
        "    for future in concurrent.futures.as_completed(future_to_params):\n",
        "        grid_counts_dict.update(future.result())  # 合并字典\n"
      ],
      "metadata": {
        "id": "TRSHMasY_dbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 保存 grid_counts_dict 字典到 pickle 文件 ##\n",
        "# grid_counts_dict: 记录了所有supercelling后的3D矩阵\n",
        "\n",
        "output_filename = os.path.join(Conf.path_to_OUTPUTS, \"grid_counts_dict.pkl\")\n",
        "with open(output_filename, 'wb') as f:\n",
        "    pickle.dump(grid_counts_dict, f)\n",
        "\n",
        "print(f\"所有的 grid_counts 已经保存到 {output_filename}\")"
      ],
      "metadata": {
        "id": "6BuubdJWeOhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}